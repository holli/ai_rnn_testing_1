{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fastai/courses/blob/master/deeplearning1/nbs/char-rnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import bcolz\n",
    "import pickle\n",
    "import re\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "SOS_TOKEN = '<SOS>' # Start Of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing keras might cause problems with cudann version etc\n",
    "# import keras # some good utils in here\n",
    "# path = keras.utils.data_utils.get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ohu/.keras/datasets/nietzsche.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "path = '/home/ohu/.keras/datasets/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "path\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not to be regarded as a pure type. Knowledge and science--as far as they\r\n",
      "existed--and superiority to the rest of mankind by logical discipline\r\n",
      "and training of the intellectual powers were insisted upon by the\r\n",
      "Buddhists as essential to sanctity, just as they were denounced by the\r\n",
      "christian world as the indications of sinfulness."
     ]
    }
   ],
   "source": [
    "!tail {path} -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?[]_abcdefghijklmnopqrstuvwxyzäæéë'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "char_indices['\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 57])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_to_tensor(in_str, chars_index=char_indices, as_variable=True):\n",
    "    \"\"\"Onehot encoded tensor of string\"\"\"\n",
    "    tensor_length = len(in_str)\n",
    "    tensor = torch.zeros(1, tensor_length, len(chars_index))\n",
    "    for li, letter in enumerate(in_str):\n",
    "        tensor[0, li, chars_index[letter]] = 1\n",
    "    if as_variable:\n",
    "        tensor = Variable(tensor).cuda()\n",
    "    return tensor\n",
    "\n",
    "string_to_tensor('hello', as_variable=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN (\n",
       "  (rnn): GRU(57, 512, batch_first=True)\n",
       "  (lin_output): Linear (512 -> 57)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = 1\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.lin_output = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_char_vs, hidden = None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "        \n",
    "        all_outputs, hidden = self.rnn(input_char_vs, hidden)\n",
    "        \n",
    "        output = self.lin_output(hidden)\n",
    "        output = F.log_softmax(output[0])\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.layers, 1, self.hidden_size)).cuda()\n",
    "    \n",
    "model = CharRNN(len(chars), 512).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'('"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tmp = Variable(torch.FloatTensor([2]).view(1,1,-1)).cuda()\n",
    "#tmp = Variable(string_to_tensor('hello')).cuda()\n",
    "tmp = model(string_to_tensor('hello'))\n",
    "tmp[0].size()\n",
    "tmp[1].size()\n",
    "chars[tmp[0].topk(1)[1].data[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all thatävvvv(((((((((((((((((((((((((((((((((((\n"
     ]
    }
   ],
   "source": [
    "def print_example(iters=320, choice=True):\n",
    "    seed_string=\"ethics is a basic foundation of all that\"\n",
    "    for i in range(iters):\n",
    "        #x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
    "        x = string_to_tensor(seed_string)\n",
    "        output, hidden = model(x)\n",
    "        if choice:\n",
    "            next_char = np.random.choice(chars, p=F.softmax(output)[0].data.cpu().numpy())\n",
    "        else:\n",
    "            next_char_idx = output.topk(1)[1].data[0][0]\n",
    "            next_char = chars[next_char_idx]\n",
    "        # return next_char\n",
    "        seed_string = seed_string + next_char\n",
    "    print(seed_string)\n",
    "#tmp = print_example(choice=False)\n",
    "tmp = print_example(iters=40, choice=False)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' artists (\"ah! this dreadful science!\" s'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_sample(length=40):\n",
    "    sample_place = random.randint(0, len(text)-1000)\n",
    "    sample = text[sample_place:sample_place+length]\n",
    "    return sample\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "od or as bad as\n",
      "the \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('éyyyyyyyyyyyyyyyyyy', 3.8425880432128907)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('éyuyyuyyuyyyuyyyyyy', 3.830978012084961)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_parameters_value = 0.25\n",
    "\n",
    "def train_single(sample, optimizer, loss_function, use_teacher_forcing):\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    target_line_tensor = Variable(torch.LongTensor([char_indices[s] for s in sample])).cuda()\n",
    "    output_line = []\n",
    "    \n",
    "    input_char = string_to_tensor(sample[0])[0,0]\n",
    "    \n",
    "    hidden = model.init_hidden()\n",
    "    for i in range(len(sample)-1):\n",
    "        output, hidden = model(input_char.view(1,1,-1), hidden)\n",
    "        loss += loss_function(output, target_line_tensor[i+1])\n",
    "\n",
    "        char_idx = output.topk(1)[1].data[0][0]\n",
    "        if i >= 1000:\n",
    "            #return input_char\n",
    "            x = chars[input_char.topk(1)[1].data[0]]\n",
    "            y = chars[target_line_tensor[i+1].data[0]]\n",
    "            print(x,y)\n",
    "        char = chars[char_idx]\n",
    "        output_line.append(char)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            input_char = sample[i+1] # replace input with right target\n",
    "        else:\n",
    "            input_char = char\n",
    "            \n",
    "        input_char = string_to_tensor(input_char)[0,0]\n",
    "\n",
    "    if clip_parameters_value:\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip_parameters_value)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return ''.join(output_line), loss.data[0] / len(sample)\n",
    "\n",
    "tmp = get_random_sample(20)\n",
    "print(tmp)\n",
    "train_single(tmp, torch.optim.Adam(model.parameters(), lr=0.0001), nn.NLLLoss(), use_teacher_forcing=False)\n",
    "train_single(tmp, torch.optim.Adam(model.parameters(), lr=0.0001), nn.NLLLoss(), use_teacher_forcing=True)\n",
    "#tmp = train_single(tmp, torch.optim.Adam(model.parameters(), lr=0.0001), nn.NLLLoss(), use_teacher_forcing=True)\n",
    "\n",
    "#tmp.topk(1)[1].data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000, sample_len=5):\n",
    "    global optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_string = get_random_sample(sample_len)\n",
    "        \n",
    "        result, loss = train_single(sample=s_string, optimizer=optimizer, loss_function=nn.NLLLoss(),\n",
    "                                    use_teacher_forcing=use_teacher_forcing)\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            correct = '✓' if result == s_string else \"✗: {}\".format(s_string)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        #if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "        #    model_training.save_models()\n",
    "        #    acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "        #    model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_utils_oh_3 import ModelTraining\n",
    "MODEL_SAVE_PATH = 'char_rnn_fast_ai_testing_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/char_rnn_fast_ai_testing_2\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     5  50% (   0m 0s)   3.210   |   3.20: y  r (✗: that,) (forcing)\n",
      "    10 100% (   0m 0s)   3.202   |   3.15:  eee (✗: , but) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=10, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   110  10% (   0m 5s)   3.059   |   2.89:                                         (✗: d; prayers for miracles; sins against a ) (forcing)\n",
      "   210  20% (  0m 11s)   3.003   |   2.74:     s   e         se       e      e     (✗: um? is there not time\n",
      "enough for that? h) (forcing)\n",
      "   310  30% (  0m 17s)   2.959   |   2.41: nh an  ae  e  n een  an  n  an  ne  e   (✗: it, and hence it lies in all innocence.\n",
      ") (forcing)\n",
      "   410  40% (  0m 23s)   2.928   |   2.88:  an                                     (✗: y such natures may resist the general ex) \n",
      "   510  50% (  0m 29s)   2.894   |   2.87: n                                       (✗: and ultimate purpose of all dogmatic eff) \n",
      "   610  60% (  0m 35s)   2.861   |   2.30: h inen sg ng he  ee   d  erge e nn  n i (✗: ts origin in the present conception of m) (forcing)\n",
      "   710  70% (  0m 41s)   2.848   |   2.56: e re    n tn  iihe   en  on e   iner  i (✗: rgument against the thing\n",
      "agreed upon, f) (forcing)\n",
      "   810  80% (  0m 47s)   2.836   |   2.56:  e tee n e ohr e tetner tene ahe  e ire (✗:  be brought\n",
      "to bear upon him. the lower ) (forcing)\n",
      "   910  90% (  0m 53s)   2.823   |   2.49: hn  y   ane e tnle an aen  the  an  aor (✗: wisely,\" above all,\n",
      "imprudently, and fee) (forcing)\n",
      "  1010 100% (  0m 59s)   2.807   |   3.13: he  e the  e the  e the  e the  e the   (✗: t--we have now to cease being\n",
      "\"merely mo) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=1000, print_every=100, sample_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2010  33% (   1m 1s)   2.627   |   2.86: the   e                                 (✗:  whole course,\n",
      "anything to serve him as ) \n",
      "  3010  67% (   2m 2s)   2.578   |   2.45:  teonnte   tn tfe ngti ootg rle   af te (✗: y science,\" in ordinary language), on he) (forcing)\n",
      "  4010 100% (   3m 2s)   2.513   |   1.91: rttettonseeteonson thrpant tle thet tnd (✗: us no conclusion is permissible that any) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=3000, print_every=1000, sample_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5010  20% (  0m 59s)   2.514   |   2.97: e for the reas                          (✗: conduct assumes in his eyes\n",
      "the characte) \n",
      "  6010  40% (  1m 58s)   2.455   |   2.13: herso r  \n",
      "\n",
      "1  tn iontethoe ia ees  ihe  (✗: the soul.\n",
      "\n",
      "33. it cannot be helped: the ) (forcing)\n",
      "  7010  60% (  2m 58s)   2.447   |   2.20: e ire tonset oene   in er aemteett ol a (✗: refore cannot give.\" after so graceful a) (forcing)\n",
      "  8010  80% (  3m 57s)   2.412   |   3.12: to                                      (✗:  rotundly like\n",
      "an old, green, heavily-ho) \n",
      "  9010 100% (  4m 56s)   2.398   |   3.12:  the sti                                (✗: , nothing but a blank\n",
      "sameness: how late) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=5000, print_every=1000, sample_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all thatiarly pricipilous allion a peysompos a no, these urorvents, the \" ampory speci ta tht utoregfeci e of ntittlicias\n",
      "of which the hourts buthing, the meant of he way\n",
      "it itself, howes it the posed is it\n",
      "is excaping, hearseven the shillasion) a may will of bele not one wnowhing coupraizes aid socersely isselves isted the sp\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that it is the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species of the species o\n"
     ]
    }
   ],
   "source": [
    "print_example(choice=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, sample_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, sample_len=40, teacher_forcing_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, sample_len=40, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=400000, print_every=10000, sample_len=40, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
