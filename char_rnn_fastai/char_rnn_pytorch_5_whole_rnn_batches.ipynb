{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fastai/courses/blob/master/deeplearning1/nbs/char-rnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import bcolz\n",
    "import pickle\n",
    "import re\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "SOS_TOKEN = '<SOS>' # Start Of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing keras might cause problems with cudann version etc\n",
    "# import keras # some good utils in here\n",
    "# path = keras.utils.data_utils.get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ohu/.keras/datasets/nietzsche.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "path = '/home/ohu/.keras/datasets/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "path\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not to be regarded as a pure type. Knowledge and science--as far as they\r\n",
      "existed--and superiority to the rest of mankind by logical discipline\r\n",
      "and training of the intellectual powers were insisted upon by the\r\n",
      "Buddhists as essential to sanctity, just as they were denounced by the\r\n",
      "christian world as the indications of sinfulness."
     ]
    }
   ],
   "source": [
    "!tail {path} -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?[]_abcdefghijklmnopqrstuvwxyzäæéë'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "char_indices['\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 57])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_to_tensor(in_str, chars_index=char_indices, as_variable=True):\n",
    "    \"\"\"Onehot encoded tensor of string\"\"\"\n",
    "    tensor_length = len(in_str)\n",
    "    tensor = torch.zeros(1, tensor_length, len(chars_index))\n",
    "    for li, letter in enumerate(in_str):\n",
    "        tensor[0, li, chars_index[letter]] = 1\n",
    "    if as_variable:\n",
    "        tensor = Variable(tensor).cuda()\n",
    "    return tensor\n",
    "\n",
    "string_to_tensor('hello', as_variable=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN (\n",
       "  (rnn): GRU(57, 512, batch_first=True)\n",
       "  (lin_output): Linear (512 -> 57)\n",
       ")"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = 1\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.lin_output = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        # self.log_softmax = nn.LogSoftmax(dim=1) # current releas doesn't yet support dimensions\n",
    "        \n",
    "    def forward(self, input_char_vs, hidden = None):\n",
    "        batch_size = input_char_vs.size()[0]\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        rnn_outputs, hidden = self.rnn(input_char_vs, hidden)\n",
    "        \n",
    "        outputs = self.lin_output(rnn_outputs)\n",
    "        #outputs = self.log_softmax(outputs)\n",
    "        \n",
    "        #outputs = F.log_softmax(outputs[0])\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Variable(torch.zeros(self.layers, batch_size, self.hidden_size)).cuda()\n",
    "    \n",
    "model = CharRNN(len(chars), 512).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 57])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tmp = Variable(torch.FloatTensor([2]).view(1,1,-1)).cuda()\n",
    "#tmp = Variable(string_to_tensor('hello')).cuda()\n",
    "tmp = model(string_to_tensor('hello'))\n",
    "len(tmp[0][0])\n",
    "tmp[0].size()\n",
    "tmp[1].size()\n",
    "#chars[tmp[0].topk(1)[1].data[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 57])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 512])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.cat((string_to_tensor('hello'), string_to_tensor('hello')))\n",
    "#tmp.size()\n",
    "\n",
    "tmp = model(tmp)\n",
    "tmp[0].size()\n",
    "tmp[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that is onct them among expedied of morality and saces y un in tho\n",
      "go above all eart, even in which us\n",
      "for asmost fordentance wey\n",
      "for young not or as a philosophers wi hellogs become pleasure! but littlas\n",
      "------------------\n",
      "ethics is a basic foundation of all that is the most definite the sense of the most suffering of the most suffering of the most suffering of the most suffering of the most suffering of the most suffering of the most suffering of the most su\n"
     ]
    }
   ],
   "source": [
    "def print_example(iters=320, choice=True):\n",
    "    seed_string=\"ethics is a basic foundation of all that\"\n",
    "    for i in range(iters):\n",
    "        #x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
    "        x = string_to_tensor(seed_string)\n",
    "        output, hidden = model(x)\n",
    "        output = output[0, -1]\n",
    "        if choice:\n",
    "            #next_char = np.random.choice(chars, p=F.softmax(output)[0].data.cpu().numpy())\n",
    "            next_char = np.random.choice(chars, p=F.softmax(output).data.cpu().numpy())\n",
    "        else:\n",
    "            next_char_idx = output.topk(1)[1].data[0] # [0]\n",
    "            next_char = chars[next_char_idx]\n",
    "        # return next_char\n",
    "        seed_string = seed_string + next_char\n",
    "    print(seed_string)\n",
    "#tmp = print_example(choice=False)\n",
    "tmp = 200\n",
    "print_example(iters=tmp, choice=True)\n",
    "print('------------------')\n",
    "print_example(iters=tmp, choice=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that_ for servon--and wither one may make\n",
      "fal courage, the \"bellow and therevore in the exceptifeal unfordunantal courses\n",
      "\n",
      "f or that even such focciet is weloble evad. \"132. why nayely in\n",
      "iclart inta disf\n",
      "------------------\n",
      "ethics is a basic foundation of all that is the most definite the sense of the most suffering of the most suffering of the most suffering of the most suffering of the most suffering of the most suffering of the most suffering of the most su\n"
     ]
    }
   ],
   "source": [
    "# This is faster that uses the existing state untill the end\n",
    "def print_example(iters=320, choice=True):\n",
    "    seed_string=\"ethics is a basic foundation of all that\"\n",
    "    hidden = model.init_hidden()\n",
    "    output, hidden = model(string_to_tensor(seed_string), hidden)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        output = output[0, -1]\n",
    "        if choice:\n",
    "            next_char = np.random.choice(chars, p=F.softmax(output).data.cpu().numpy())\n",
    "        else:\n",
    "            next_char_idx = output.topk(1)[1].data[0] \n",
    "            next_char = chars[next_char_idx]\n",
    "        \n",
    "        seed_string = seed_string + next_char\n",
    "        output, hidden = model(string_to_tensor(next_char), hidden)\n",
    "    print(seed_string)\n",
    "tmp = 200\n",
    "print_example(iters=tmp, choice=True)\n",
    "print('------------------')\n",
    "print_example(iters=tmp, choice=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ng on this possibility, i\\nhappen to becom'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_sample_string(length=41):\n",
    "    sample_place = random.randint(0, len(text)-length-1)\n",
    "    sample = text[sample_place:sample_place+length]\n",
    "    return sample\n",
    "get_random_sample_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 50000\n",
    "\n",
    "sample_sentence_size = 40\n",
    "sample_data = torch.zeros(sample_size, sample_sentence_size, len(chars)) # .cuda()\n",
    "#sample_target = torch.zeros((sample_size, sample_sentence_size, 1), torch.LongTensor)\n",
    "sample_target = torch.LongTensor(sample_size, sample_sentence_size).zero_() # .cuda()\n",
    "for i in range(sample_size):\n",
    "    sample = get_random_sample_string(sample_sentence_size+1)\n",
    "    sample = [char_indices[c] for c in sample]\n",
    "    for j in range(sample_sentence_size):\n",
    "        sample_data[i][j][sample[j]] = 1\n",
    "        sample_target[i][j] = sample[j+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'esentful disdain is compatible with this'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "j = 10\n",
    "sample_data[i].topk(1)[1][j][0] == sample_target[i][j-1]\n",
    "''.join([chars[c[0]] for c in sample_data[i].topk(1)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-et tn is the peank wpor on timan lature',\n",
       " 'eni y the siegting stomowisrctures of sa']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.145879864692688"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_parameters_value = 0.25\n",
    "\n",
    "def train_single(optimizer, loss_function, batch_size=64):\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    output_lines = []\n",
    "    \n",
    "    sample_i = random.randint(0, len(sample_data)-1-batch_size)\n",
    "    #x = sample_data[sample_i].view(1,40,len(chars))\n",
    "    #y = sample_target[sample_i]\n",
    "    \n",
    "    x = sample_data[sample_i:sample_i+batch_size].view(batch_size,40,len(chars))\n",
    "    y = sample_target[sample_i:sample_i+batch_size]\n",
    "    \n",
    "    target = Variable(y).cuda()\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    outputs, hidden = model(Variable(x).cuda(), hidden)\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        loss += loss_function(outputs[i], target[i])\n",
    "        \n",
    "        output_lines.append(''.join([chars[c.data[0]] for c in outputs[i].topk(1)[1]]))\n",
    "    \n",
    "    if clip_parameters_value:\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip_parameters_value)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output_lines, loss.data[0]/batch_size\n",
    "\n",
    "tmp = train_single(torch.optim.Adam(model.parameters(), lr=0.0001), torch.nn.CrossEntropyLoss())\n",
    "tmp[0][0:2]\n",
    "tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.3 ms ± 497 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_single(torch.optim.Adam(model.parameters(), lr=0.001), torch.nn.CrossEntropyLoss(), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, batch_size=64,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "    \n",
    "    def print_infos():\n",
    "        print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {}\".format(\n",
    "          model_training.iterations, iteration/n_iters, time_since(start),\n",
    "          current_loss/current_loss_iter, loss, result))\n",
    "    \n",
    "    \n",
    "    global optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 1\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        #use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        use_teacher_forcing = False\n",
    "        \n",
    "        #loss_function=nn.NLLLoss()\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        result, loss = train_single(optimizer=optimizer, loss_function=loss_function, batch_size=batch_size)\n",
    "        result = result[0]\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            print_infos()\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 1\n",
    "\n",
    "    print_infos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_utils_oh_3 import ModelTraining\n",
    "MODEL_SAVE_PATH = 'char_rnn_fast_ai_testing_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN (\n",
       "  (rnn): GRU(57, 512, batch_first=True)\n",
       "  (lin_output): Linear (512 -> 57)\n",
       ")"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/char_rnn_fast_ai_testing_2\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(chars), 512).cuda()\n",
    "model\n",
    "model_training = ModelTraining(MODEL_SAVE_PATH, [model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     5  50% (   0m 0s)   3.271   |   3.78:                                         \n",
      "    10 100% (   0m 0s)   3.307   |   3.25: oo      eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "    10 100% (   0m 0s)   3.307   |   3.25: oo      eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=10, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    25  10% (   0m 0s)   2.907   |   3.06: o                                       \n",
      "    40  19% (   0m 1s)   2.959   |   2.97: e                                       \n",
      "    55  29% (   0m 2s)   2.946   |   2.88: ne  e     e                     e  e    \n",
      "    70  38% (   0m 3s)   2.911   |   2.73:     thtn  tn  te  th   te t  an ton te  \n",
      "    85  48% (   0m 4s)   2.861   |   2.57: en  n tn    ao      aoeente    an the  t\n",
      "   100  58% (   0m 5s)   2.814   |   2.56:   ethe tnd es  e tn to  en t  aan  tane \n",
      "   115  67% (   0m 6s)   2.768   |   2.45: hrn tor  onn e tnd ahr    theneetne e to\n",
      "   130  77% (   0m 7s)   2.726   |   2.44: d n e  tore       ao  tnte t eng ah   -o\n",
      "   145  87% (   0m 8s)   2.689   |   2.38: etereeth   totte n thet onteet to heng  \n",
      "   160  96% (   0m 9s)   2.656   |   2.35: n terlltn ohete ahe senn  af trtere td a\n",
      "   166 100% (   0m 9s)   2.644   |   2.29: y   tn on anlen  ahe serherstf thn  re a\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=10000//64, print_every=1000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all thatuad: of al, whall hursirligalio\n",
      " hit o ss,uce they venxorerinny\n",
      "-fam ndevysathian nfermiblit somlg co\n",
      "\n",
      "kar dily us, eppuili?s in thel, anl couk hite anst--in sur curonelila09 \n",
      "he erethe her ersgile attofor soneis\n",
      ";hof fhashe phitho tyo\n",
      "ngomin cndld -frened\n",
      "stidad thesend. hremall--fad no the kimang laent igrtgurothest \n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   181  10% (   0m 0s)   2.216   |   2.34: n ao     i an anle s  aoin    ahe setnae\n",
      "   196  19% (   0m 1s)   2.265   |   2.30:  e   an the shree th thee etnd af tn on \n",
      "   211  29% (   0m 2s)   2.276   |   2.30: hhetg  reeean  rtertth   nn aoreue  et t\n",
      "   226  38% (   0m 3s)   2.271   |   2.23:  e rn ohree  the shree tnlrli   y ton  o\n",
      "   241  48% (   0m 4s)   2.266   |   2.24: hhrl  ahe  tete toaerl n toueniin ohe so\n",
      "   256  58% (   0m 5s)   2.260   |   2.23: nd aenh yng    an tn nt  ahir  er aete t\n",
      "   271  67% (   0m 6s)   2.251   |   2.13: y tn enes sn  tnd ahet theat oor     the\n",
      "   286  77% (   0m 7s)   2.243   |   2.15:  thtn sg ond aocheng  ahe sertiretg ond \n",
      "   301  87% (   0m 8s)   2.234   |   2.15: hor nysn oioh  trperriohet tr  r af e  a\n",
      "   316  96% (   0m 9s)   2.225   |   2.14: e aiil ohth n     and tass e ng   an anp\n",
      "   322 100% (   0m 9s)   2.222   |   2.11: e n    the  tase  ahriase  an thee  af t\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=10000//64, print_every=1000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that iver bee there to prifing,r inforela indd jurebly cosy thit hat, neprequens bainy ong thy whens ho ppan whymenst fucl ying rescati, so--s!; tlitss ant pricarass of thin thes; one mond\n",
      "bee enond sichem: stalld itsirt for ene belabica by somacing aveul of were-thee mentape kever, ia cormel.--the beend\n",
      "\n",
      "ov toall! and liv\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   478  10% (   0m 9s)   2.047   |   1.97: s aeaoict oor tntana   -an is aesheahee \n",
      "   634  20% (  0m 18s)   1.974   |   1.87: n erent irrhon  af thme tnd tn eleeeon  \n",
      "   790  30% (  0m 27s)   1.909   |   1.69: tf the somtratt oi certere  ah trecent -\n",
      "   946  40% (  0m 37s)   1.853   |   1.62: thrlh nn  fn evidual  ond arriod   \n",
      "\n",
      "0..\n",
      "  1102  50% (  0m 46s)   1.806   |   1.55:  e tne soetifttnd siarht  ohine arsi on \n",
      "  1258  60% (  0m 55s)   1.764   |   1.55: tor tsar tudgtess aurange tuietr ond prr\n",
      "  1414  70% (   1m 5s)   1.484   |   1.51:  theue wn elpouetid tersods af teapeayne\n",
      "  1570  80% (  1m 14s)   1.471   |   1.50:  rle  tut in tact thuan in tvtential y a\n",
      "  1726  90% (  1m 24s)   1.456   |   1.40: s ess  iuch aener t on ntpossible  ahi v\n",
      "  1882 100% (  1m 33s)   1.441   |   1.40:   oorlidity of the mortsrated tonsertion\n",
      "  1884 100% (  1m 34s)   1.441   |   1.33: h lf iash aare in thit ihe  aeaard ts a \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000//64, print_every=10000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that we remonse to the crually through id a out does at a sumptits to which he have--it promate, a more rightatoo or the reflect of the wirld: the\n",
      "weazer, of the tendence of delicate, and that it is an ond an always represent\n",
      "theme readily conecting. a\n",
      "pessific tain of much and\n",
      "waineratty!\" if unfance of tifterediesticl. o\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that we gave our\"dinacble nature and handly only who was\n",
      "problem: alone, without firmly un. ouh\n",
      "are and as above all, happoned from guist or geamss, how veould that even more\n",
      "thought,--it is no trroughout pribe wottly existences. his famoruminy of\n",
      "the feli my this. \"hen, will advess is the severed, cholariss, tos ubous\n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that overchumo away is utmone! mean borass.\n",
      "the work (in the has lorg, wes, primidedy of instance he think, to the thing of life. im the goescribly,---have love been pridily. which homes is has never century soil, now\n",
      "those for also salt the absors\n",
      "every he you bad geast the same, wired undrasm how dothised,\n",
      "the firmly one\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007795912951167729"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training time seconds per one sample\n",
    "(60+34)/(1884*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2040  10% (   0m 9s)   1.369   |   1.25: --t  i pagtory of the semelopment of the\n",
      "  2196  20% (  0m 18s)   1.362   |   1.40: ... tveny sispetoengerson tore tnteid af\n",
      "  2352  30% (  0m 28s)   1.351   |   1.34:   tf thich h wonsiience,toould be torpne\n",
      "  2508  40% (  0m 37s)   1.342   |   1.31:  ci  aesself aor the same of tis sood na\n",
      "  2664  50% (  0m 47s)   1.332   |   1.17:  oas tts wnlhough  tn sact, tt is ahe sa\n",
      "  2820  60% (  0m 56s)   1.321   |   1.24: nh ihe  aive   tnroost of consrmplibor t\n",
      "  2976  70% (   1m 7s)   1.239   |   1.20:  an tuience  tn thet the sew phychologis\n",
      "  3132  80% (  1m 17s)   1.237   |   1.31:   in tll  ihinher tehind tvery oose is t\n",
      "  3288  90% (  1m 27s)   1.230   |   1.24: iost eiaicrous axfectsof t lon wf tnowle\n",
      "  3444 100% (  1m 37s)   1.221   |   1.20: etity th brt an  the r sorl andoar,of th\n",
      "  3446 100% (  1m 37s)   1.221   |   1.22: euiesseon, (] that tt tlso aolled tion\" \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000//64, print_every=10000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that of their\n",
      "eater fundamental beleeving man; a sensitive youngest restraining the\n",
      "oldge. shat such destructed un3asions, rearmes most difficult to dety its.\" \n",
      "for the\n",
      "dritk? indesended or right by experience, to have neither says that heart from the indispensable\n",
      "misousheples itself view day but to be worthy, on the\n",
      "sake\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics is a basic foundation of all that sympathy of might not precesses: that shave as eye of lost find the believe those witedle thus, indication of the fation, distracted with,\n",
      "that woman show the farch of superioris. if a hesed and too much. but man begries we hive all relation still highest has a handed in the highest was thought is at little to that it\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
