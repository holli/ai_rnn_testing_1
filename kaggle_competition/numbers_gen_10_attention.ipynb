{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'numbers_gen_10_attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data = pickle.load(open(\"data/en_train_fixed_4_sentences.pkl\", \"rb\" ))\n",
    "all_data = pickle.load(open(\"data/en_train_fixed_5_manual.pkl\", \"rb\" ))\n",
    "# all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 448170,  (dropped rows: 9470022)\n"
     ]
    }
   ],
   "source": [
    "sample_data = all_data.copy()\n",
    "#sample_data = sample_data[sample_data['class'] != 'NOT_CHANGED']\n",
    "#sample_data = sample_data[sample_data['class'] != 'MANUAL']\n",
    "\n",
    "sample_data =  sample_data[sample_data['class'] == 'NUMBERS']\n",
    "\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(sample_data), len(all_data)-len(sample_data)))\n",
    "sample_data = sample_data.reset_index(drop=True)\n",
    "del(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NUMBERS']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "categories_all = sorted(sample_data[\"class\"].unique())\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', 'two', 'twenty']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_after_common = pickle.load(open(\"data/en_features/words_after_ext.pkl\", 'rb'))\n",
    "words_after_index = dict((c, i) for i, c in enumerate(words_after_common))\n",
    "words_after_common[0:7]\n",
    "len(words_after_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#onehot_sos = torch.zeros(1, 1, len(words_after_index))\n",
    "#onehot_sos[0, 0, words_after_index[SOS_TOKEN]] = 1\n",
    "#onehot_sos.size()\n",
    "#del(onehot_sos)\n",
    "\n",
    "sos_tensor = torch.LongTensor([words_after_index[SOS_TOKEN]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data_classes_select = list(sample_data.groupby('class'))\n",
    "\n",
    "balanced_data_accessed_counter = 0 \n",
    "balanced_data_length = 0\n",
    "def balanced_data_randomize(max_len=20000):\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter\n",
    "    balanced_data = pd.concat([v.sample(min(max_len, len(v))) for k, v in balanced_data_classes_select])\n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_accessed_counter = 0\n",
    "\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_accessed_counter\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_accessed_counter += 1\n",
    "    if balanced_data_accessed_counter/balanced_data_length > 0.2:\n",
    "        balanced_data_randomize()\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample\n",
    "    \n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "NUMBERS    20000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data.groupby('class')['class'].count()\n",
    "#sample_data.groupby('class')['class'].count()\n",
    "balanced_data.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id                                               688488\n",
       "token_id                                                       2\n",
       "class                                                    NUMBERS\n",
       "before                                                     1950s\n",
       "after                                           nineteen fifties\n",
       "class_org                                                   DATE\n",
       "a_word_ind                                           [7, 136, 0]\n",
       "sentence       since the <SAMPLE> , scientists have built sel...\n",
       "Name: 412676, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBERS : 1978 -> nineteen seventy eight <EOS> [7, 33, 16, 0]\n",
      "\" iv copa brasil - <SAMPLE> \" .\n",
      "torch.Size([1, 5, 104])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sentence_id = sample_row['class']\n",
    "    \n",
    "    return sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence']\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    s_aft_str = ' '.join([words_after_common[i] for i in s_aft])\n",
    "    print(s_class, ':', s_bef, '->', s_aft_str, s_aft)\n",
    "    print(s_sentence)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 µs ± 2.68 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ATTENTION_LENGTH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(50, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 128, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "        self.hidden_size = words_hidden_size + chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        hidden_states_cat = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.hidden_size)).cuda()\n",
    "        for ei in range(min(MAX_ATTENTION_LENGTH, len(string_tensor[0]))):\n",
    "            hidden_states_cat[ei] = torch.cat((output_words, all_outputs_chars[0, ei].view(1,-1)), 1)\n",
    "                \n",
    "        output = torch.cat((output_words[0], output_chars[0]), 0)\n",
    "        #output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #return output, all_outputs_chars\n",
    "        return output, hidden_states_cat\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))\n",
    "    \n",
    "    \n",
    "encoder_rnn = EncoderRNN(words_input_size=wv_vecs.shape[-1], chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=256,\n",
    "                         words_layers=1, chars_layers=1).cuda()\n",
    "encoder_rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 384])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t), s_bef\n",
    "    \n",
    "(encoder_output, encoder_outputs), tmp = test_encoder_single_sample()\n",
    "tmp\n",
    "encoder_output.size()\n",
    "encoder_outputs.size()\n",
    "list(encoder_output.data.cpu().numpy()) == list(encoder_outputs[len(tmp)].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): GRU(384, 384, batch_first=True)\n",
       "  (lin_out): Linear (384 -> 1351)\n",
       "  (embedding): Embedding(1351, 384)\n",
       "  (attn): Linear (768 -> 20)\n",
       "  (attn_combine): Linear (768 -> 384)\n",
       "  (dropout): Dropout (p = 0.1)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 1, 384]), torch.Size([1, 20])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p=0.1, max_length=MAX_ATTENTION_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.max_length = max_length\n",
    "        self.attn = nn.Linear(hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    #def forward(self, char, hidden):\n",
    "    def forward(self, last_input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(last_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #IPython.core.debugger.Pdb().set_trace()\n",
    "        #attn_weights = F.softmax(self.attn(torch.cat((char[0], hidden[0]), 1)))\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.lin_out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        if self.n_layers > 1:\n",
    "            hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "            res = torch.cat((input_var, hid_var), 0)\n",
    "            return res\n",
    "        else:\n",
    "            return input_var\n",
    "        \n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(words_after_common), hidden_size=encoder_output.size()[0], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn\n",
    "\n",
    "tmp_outputs = Variable(torch.zeros(MAX_ATTENTION_LENGTH, encoder_output.size()[0])).cuda()\n",
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_hiddens, tmp_outputs)\n",
    "[v.size() for v in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  1.2000  2.4000  3.6000  4.8000\n",
       "[torch.FloatTensor of size 1x1x4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(torch.FloatTensor([1,0.1,0.1]).view(1,1,-1), torch.arange(0, 12).view(1,3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 1, 384]), torch.Size([1, 20])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 128\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "seventies\n"
     ]
    }
   ],
   "source": [
    "tmp_outputs = Variable(torch.zeros(MAX_ATTENTION_LENGTH, encoder_output.size()[0])).cuda()\n",
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_hiddens, tmp_outputs)\n",
    "[v.size() for v in tmp]\n",
    "print(tmp[0].topk(1)[1])\n",
    "print(words_after_common[tmp[0].topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('seventies elevenths twentieths amperes unfavorably symbolizing unfavorably unfavorably symbolizing unfavorably industrialized unfavorably symbolizing unfavorably symbolizing unfavorably unfavorably symbolizing unfavorably industrialized',\n",
       " 'seventies elevenths twentieths amperes unfavorably symbolizing unfavorably unfavorably symbolizing unfavorably industrialized unfavorably symbolizing unfavorably symbolizing unfavorably unfavorably symbolizing unfavorably industrialized',\n",
       " 'nineteen ninety seven',\n",
       " ('1997', [7, 23, 18, 0], 'NUMBERS', 'franks et al <SAMPLE> , pp .'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None, return_more=False):\n",
    "    decoder_rnn.eval()\n",
    "    encoder_rnn.eval()\n",
    "    \n",
    "    s_bef, s_aft, s_class, s_sentence = sample = get_random_sample()\n",
    "        \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    #encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_outputs[0].view(1,1,-1))\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    decoder_attns_arr = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attns_arr.append(decoder_attns)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([word_index])).cuda()\n",
    "        #decoder_input = torch.zeros(1, 1, len(words_after_index))\n",
    "        #decoder_input[0, 0, word_index] = 1\n",
    "        #decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    sample_target = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "    \n",
    "    if return_more:\n",
    "        return output, decoded_output, decoder_attns_arr, sample\n",
    "    \n",
    "    return output, output, sample_target, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1772           => hybridize milli megapascals unfavorably hybridize unfavorably hybridize unfavorably hybridize unfavorably hybridize unfavorably hybridize unfavorably hybridize unfavorably hybridize unfavorably hybridize unfavorably || [81, 33, 5, 0] \n",
      "                  wiedewelt was chosen for eight annual periods as director of the academy between <SAMPLE> and 1794 .\n",
      "2:44           => seventies yards rijksstraat unfavorably symbolizing unfavorably symbolizing unfavorably unfavorably symbolizing unfavorably industrialized unfavorably symbolizing unfavorably symbolizing unfavorably unfavorably symbolizing unfavorably || [5, 41, 19, 0] \n",
      "                  the writing credits on the 45 are ( l . green , y . nomura , b . hoffert ) , and the song 's length is <SAMPLE> .\n"
     ]
    }
   ],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', s_sentence, ))\n",
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    #encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_outputs[0].view(1,1,-1))\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "    \n",
    "    ###\n",
    "    target_arr = s_aft\n",
    "    #target_arr = after_sentence_to_word_indexes(s_aft, include_eos=True)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        #decoder_target_i = chars_after_index[target_arr[i]]\n",
    "        decoder_target_i = target_arr[i]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word_index = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([word_index])).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            s_aft_sentence = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "            correct = '✓' if result == s_aft_sentence else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0: # or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/numbers_gen_10_attention\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   7.151   |   7.02: 2000 -> everybodyisflawless ninth (✗: [5, 8, 0]) (forcing)\n",
      "    18  36% (   0m 0s)   6.615   |   7.09: 17 November 2014 -> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> (✗: [11, 99, 12, 69, 6, 50, 0]) (forcing)\n",
      "    27  54% (   0m 0s)   5.867   |   1.41: June 1952 ->  (✗: [68, 7, 38, 5, 0]) \n",
      "    36  72% (   0m 0s)   5.681   |   6.97: 1830 -> two <EOS> (✗: [40, 34, 0]) (forcing)\n",
      "    45  90% (   0m 0s)   5.664   |   6.62: 1915 -> two <EOS> (✗: [7, 51, 0]) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (   0m 9s)   2.958   |   4.05: June 26, 1972 -> nineteen <EOS> <EOS> <EOS> <EOS> <EOS> (✗: [68, 6, 79, 7, 33, 5, 0]) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 20s)   2.441   |   4.01: .170 -> nineteen one <EOS> <EOS> (✗: [46, 9, 18, 25, 0]) (forcing)\n",
      "  3000  22% (  0m 40s)   2.243   |   4.14: $1,000,000 -> the twenty of (✗: [9, 90, 85, 0]) \n",
      "  4000  33% (   1m 2s)   2.127   |   3.31: October 10, 2012 -> the twenty of thousand (✗: [61, 93, 6, 47, 0]) \n",
      "  5000  44% (  1m 23s)   1.992   |   1.05: 1997 -> nineteen ninety eight (✗: [7, 23, 18, 0]) (forcing)\n",
      "  6000  56% (  1m 44s)   1.970   |   3.05: 978-0-7134-8943-9 -> the point point sil one one one <EOS> one one sil one <EOS> <EOS> sil one one (✗: [15, 18, 16, 58, 25, 58, 18, 9, 13, 19, 58, 16, 15, 19, 13, 58, 15, 0]) (forcing)\n",
      "  7000  67% (   2m 5s)   1.876   |   1.40: 1973 -> nineteen ninety <EOS> (✗: [7, 33, 13, 0]) (forcing)\n",
      "  8000  78% (  2m 25s)   1.820   |   1.04: 1978 -> nineteen ninety <EOS> (✗: [7, 33, 16, 0]) (forcing)\n",
      "  9000  89% (  2m 46s)   1.837   |   1.52: 552,064 -> one hundred thousand (✗: [14, 10, 38, 5, 8, 39, 19, 0]) \n",
      " 10000 100% (   3m 7s)   1.747   |   1.23: 1 -> two (✗: [9, 0]) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  25% (  3m 32s)   0.921   |   0.65: January 2016 -> january twenty twenty (✗: [63, 6, 75, 0]) (forcing)\n",
      " 30000  50% (  7m 12s)   0.680   |   0.18: 160 -> one hundred sixty (✓) \n",
      " 40000  75% ( 10m 59s)   0.575   |   0.47: 88.80 m -> eighty eight point (✗: [27, 16, 46, 16, 25, 108, 0]) \n",
      " 50000 100% ( 14m 39s)   0.487   |   0.58: May 17, 2008 -> may eleventh two thousand eight (✗: [66, 99, 5, 8, 16, 0]) \n",
      "Saved model to data/models/numbers_gen_10_attention/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 65.62% (    6562/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=40000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 April 1676  => the twenty second of february nineteen o six || [11, 6, 84, 12, 71, 75, 33, 20, 0] \n",
      "                  she committed suicide in prison , and the sisters zippel were executed together on hotorget on <SAMPLE> .\n",
      "May 25, 2006   => may third two thousand six || [66, 6, 78, 5, 8, 20, 0] \n",
      "                  lindsay , meggen ( <SAMPLE> ) .\n",
      "4 ft           => four over      || [19, 132, 0] \n",
      "                  they can reach nearly <SAMPLE> in length .\n",
      "2008-06-25     => the twenty second of september two thousand seven || [11, 6, 78, 12, 68, 5, 8, 16, 0] \n",
      "                  archived from the original ( pdf ) on <SAMPLE> .\n",
      "February 12, 2007 => february third two thousand seven || [72, 95, 5, 8, 18, 0] \n",
      "                  <SAMPLE> .\n",
      "$19,000        => one thousand dollars || [7, 8, 85, 0] \n",
      "                  \" email trips up nebraska attorney general , agrees to <SAMPLE> penalty \" .\n",
      "6.1 km2        => three point seven two point seven || [20, 46, 9, 106, 89, 0] \n",
      "                  the watershed of nuangola outlet has an area of 2 . 35 square miles ( <SAMPLE> ) .\n",
      "16,000         => fifteen thousand || [75, 8, 0] \n",
      "                  eight thousand fans were selected and allowed to purchase the <SAMPLE> tickets that were made available to the public .\n",
      "II             => two            || [11, 73, 0] \n",
      "                  the county of carcassonne was subsumed within barcelona thereafter , though a viscounty was created in 1082 by raymond berengar <SAMPLE> .\n",
      "57             => fifty seven    || [14, 18, 0] \n",
      "                  myelinogenesis in the optic nerve of ( c <SAMPLE> bl x cba ) f 1 hybrid mice : a morphometric analysis . european journal of morphology , 35 ( 1 ) , 3 - 18 .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60000  20% (  3m 49s)   0.485   |   0.02: 1997 -> nineteen ninety seven (✓) (forcing)\n",
      " 70000  40% (  7m 30s)   0.418   |   0.01: 264 -> two hundred sixty four (✓) (forcing)\n",
      " 80000  60% ( 11m 19s)   0.454   |   0.01: 2011 -> twenty eleven (✓) (forcing)\n",
      " 90000  80% ( 15m 14s)   0.434   |   0.00: 1969 -> nineteen sixty nine (✓) (forcing)\n",
      "100000 100% ( 19m 10s)   0.380   |   1.27: $2,000,000 -> two billion thousand thousand (✗: [5, 90, 85, 0]) (forcing)\n",
      "Saved model to data/models/numbers_gen_10_attention/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 77.45% (    7745/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0201-          => zero point two one || [25, 5, 25, 9, 0] \n",
      "                  the coding sequences of 80 new genes ( kiaa <SAMPLE> kiaa 0280 ) deduced by analysis of cdna clones from cell line kg 1 and brain . \"\n",
      "October 5, 2007 => october fifteenth two thousand seven || [61, 78, 5, 8, 18, 0] \n",
      "                  archived from the original on <SAMPLE> .\n",
      "III            => three          || [11, 76, 0] \n",
      "                  in his will , henry <SAMPLE> left his wife glogow as her oprawa wdowia , which she ruled until her own death .\n",
      "February 11, 1932 => february twenty third nineteen thirty two || [72, 92, 7, 34, 5, 0] \n",
      "                  yvonne fine sangiacomo ( born <SAMPLE> , san francisco ) .\n",
      "943.2          => three hundred point two three || [15, 10, 41, 13, 46, 5, 0] \n",
      "                  the population density was <SAMPLE> people per square mile ( 361 . 8 / km² ) .\n",
      "74 percent     => seventy million || [33, 19, 83, 0] \n",
      "                  the young conservatives of texas gave her a cumulative score in 2013 of <SAMPLE> .\n",
      "2.42           => two point two four || [5, 46, 19, 5, 0] \n",
      "                  the average household size was <SAMPLE> and the average family size was 3 . 02 .\n",
      "17 December 2007 => the seventeenth of november two thousand seven || [11, 99, 12, 65, 5, 8, 18, 0] \n",
      "                  archived <SAMPLE> at the wayback machine .\n",
      "362.8/km²      => three hundred eighty one eighty august || [13, 10, 39, 5, 46, 16, 112, 106, 89, 0] \n",
      "                  the population density was 940 . 4 people per square mile ( <SAMPLE> ) .\n",
      "113,832        => one hundred thirty one twelve point two three || [9, 10, 49, 8, 16, 10, 34, 5, 0] \n",
      "                  in the county wide contest , it was defeated with 102 , 281 people voting in its favor and <SAMPLE> voting against .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   29 November 2007\n",
      "output:  ['the', 'twentieth', 'of', 'november', 'two', 'thousand', 'seven']\n",
      "target:    the twenty ninth of november two thousand seven\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAFpCAYAAABOCL1KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2359956be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def debug_show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    #ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_xticklabels([''] + input_sentence + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def debug_eval_sample_show_attention():\n",
    "    output, decoded_output, decoder_attns_arr, sample = test_model_single_sample(None, return_more=True)\n",
    "    print('input:  ', sample[0])\n",
    "    print('output: ', decoded_output)\n",
    "    print('target:   ', ' '.join([words_after_common[w] for w in sample[1][:-1]]))\n",
    "\n",
    "    attns = np.array([arr.data[0].cpu().numpy() for arr in decoder_attns_arr])\n",
    "\n",
    "    debug_show_attention(list(sample[0]), decoded_output, attns)\n",
    "    #plt.matshow(attns)\n",
    "    \n",
    "debug_eval_sample_show_attention()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
