{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'whole_gen_12_1_simpler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data = pickle.load(open(\"data/en_train_fixed_4_sentences.pkl\", \"rb\" ))\n",
    "all_data = pickle.load(open(\"data/en_train_fixed_5_manual.pkl\", \"rb\" ))\n",
    "# all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 653598,  (dropped rows: 9264594)\n"
     ]
    }
   ],
   "source": [
    "sample_data = all_data.copy()\n",
    "sample_data = sample_data[sample_data['class'] != 'NOT_CHANGED']\n",
    "sample_data = sample_data[sample_data['class'] != 'MANUAL']\n",
    "sample_data = sample_data[sample_data['before'].str.len() > 0]\n",
    "sample_data = sample_data[sample_data['before'].str.len() <= 31]\n",
    "# sample_data = sample_data[sample_data['class'] != 'ELECTRONIC']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(sample_data), len(all_data)-len(sample_data)))\n",
    "sample_data = sample_data.reset_index(drop=True)\n",
    "del(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC    731\n",
       "LETTERS         1\n",
       "NUMBERS        14\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = sample_data[sample_data['before'].str.len() > 30]\n",
    "tmp.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ELECTRONIC', 'LETTERS', 'NUMBERS', 'PLAIN', 'VERBATIM']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "categories_all = sorted(sample_data[\"class\"].unique())\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', 'two', 'twenty']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_after_common = pickle.load(open(\"data/en_features/words_after_ext.pkl\", 'rb'))\n",
    "words_after_index = dict((c, i) for i, c in enumerate(words_after_common))\n",
    "words_after_common[0:7]\n",
    "len(words_after_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1351])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_tensor = torch.zeros(1, 1, len(words_after_index))\n",
    "sos_tensor[0, 0, words_after_index[SOS_TOKEN]] = 1\n",
    "sos_tensor.size()\n",
    "#del(onehot_sos)\n",
    "\n",
    "# sos_tensor = torch.LongTensor([words_after_index[SOS_TOKEN]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "balanced_data_classes_select = list(sample_data.groupby('class'))\n",
    "\n",
    "balanced_data_accessed_counter = 0 \n",
    "balanced_data_randomize_freq = False\n",
    "balanced_data_length = 0\n",
    "\n",
    "last_samples = collections.deque(maxlen=100)\n",
    "balanced_data_last_sample = last_samples\n",
    "\n",
    "def balanced_data_randomize_org(max_len=20000):\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter, balanced_data_randomize_freq\n",
    "    balanced_data = pd.concat([v.sample(min(max_len, len(v))) for k, v in balanced_data_classes_select])\n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_randomize_freq = balanced_data_length * 0.2\n",
    "    balanced_data_accessed_counter = 0\n",
    "balanced_data_randomize = balanced_data_randomize_org\n",
    "\n",
    "\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_accessed_counter\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_accessed_counter += 1\n",
    "    if balanced_data_randomize_freq and balanced_data_accessed_counter > balanced_data_randomize_freq:\n",
    "        balanced_data_randomize()\n",
    "        \n",
    "    sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    last_samples.append(sample)\n",
    "    balanced_data_last_sample = sample\n",
    "    return sample\n",
    "    \n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC     4964\n",
       "LETTERS       20000\n",
       "NUMBERS       20000\n",
       "PLAIN         20000\n",
       "VERBATIM      11741\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data.groupby('class')['class'].count()\n",
    "#sample_data.groupby('class')['class'].count()\n",
    "balanced_data.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id                                               586402\n",
       "token_id                                                      18\n",
       "class                                                      PLAIN\n",
       "before                                                        mt\n",
       "after                                                      mount\n",
       "class_org                                                  PLAIN\n",
       "a_word_ind                                              [169, 0]\n",
       "sentence       king sejo healed his own skin problems at bany...\n",
       "Name: 512491, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERBATIM : & -> and <EOS> [55, 0]\n",
      "['cum', 'gratia', '<SAMPLE>', 'priuilegio', 'regis', '.']\n",
      "torch.Size([1, 2, 104])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()   \n",
    "    return sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence'].split(' ')\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    s_aft_str = ' '.join([words_after_common[i] for i in s_aft])\n",
    "    print(s_class, ':', s_bef, '->', s_aft_str, s_aft)\n",
    "    print(s_sentence)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 µs ± 1.61 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ATTENTION_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>557156</th>\n",
       "      <td>636804</td>\n",
       "      <td>18</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.mcu.usmc.mil/historydivision/Pages/...</td>\n",
       "      <td>h t t p colon slash slash w w w dot m c u dot ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>marine corps history division http : / / www ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177662</th>\n",
       "      <td>204144</td>\n",
       "      <td>19</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.calabasashistoricalsociety.org/City...</td>\n",
       "      <td>h t t p colon slash slash w w w dot c a l a b ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>hogle , gene nac green book of pacific coast t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id       class  \\\n",
       "557156       636804        18  ELECTRONIC   \n",
       "177662       204144        19  ELECTRONIC   \n",
       "\n",
       "                                                   before  \\\n",
       "557156  http://www.mcu.usmc.mil/historydivision/Pages/...   \n",
       "177662  http://www.calabasashistoricalsociety.org/City...   \n",
       "\n",
       "                                                    after   class_org  \\\n",
       "557156  h t t p colon slash slash w w w dot m c u dot ...  ELECTRONIC   \n",
       "177662  h t t p colon slash slash w w w dot c a l a b ...  ELECTRONIC   \n",
       "\n",
       "                                               a_word_ind  \\\n",
       "557156  [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "177662  [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "\n",
       "                                                 sentence  \n",
       "557156  marine corps history division http : / / www ....  \n",
       "177662  hogle , gene nac green book of pacific coast t...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = sample_data[sample_data['before'].str.len()>MAX_ATTENTION_LENGTH]\n",
    "len(tmp)\n",
    "tmp.sample(2)\n",
    "# tmp[~tmp['before'].str.contains('/')].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): GRU(50, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): GRU(104, 128, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "        self.hidden_size = words_hidden_size + chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.GRU(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.GRU(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = hidden_words.view(1, -1)\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = hidden_chars.view(1, -1)\n",
    "        \n",
    "        #hidden_states_cat = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.hidden_size)).cuda()\n",
    "        #for ei in range(min(MAX_ATTENTION_LENGTH, len(string_tensor[0]))):\n",
    "        #    hidden_states_cat[ei] = torch.cat((output_words, all_outputs_chars[0, ei].view(1,-1)), 1)\n",
    "\n",
    "        all_outputs_chars_padded = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.chars_hidden_size)).cuda()\n",
    "        att_length = min(len(all_outputs_chars[0]), MAX_ATTENTION_LENGTH-1)\n",
    "        all_outputs_chars_padded[0:att_length] = all_outputs_chars[0][0:att_length]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "\n",
    "        #output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #return output, all_outputs_chars\n",
    "        return output, all_outputs_chars_padded\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1 = var1.cuda(); var2 = var2.cuda()\n",
    "        return (var1, var2)\n",
    "    \n",
    "    \n",
    "encoder_rnn = EncoderRNN(words_input_size=wv_vecs.shape[-1], chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=256,\n",
    "                         words_layers=1, chars_layers=1).cuda()\n",
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 256])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    #s_bef, s_aft, s_class, s_sentence = get_random_sample(True)\n",
    "    print(s_bef)\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(s_sentence, wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t), s_bef\n",
    "    \n",
    "(tmp_encoder_output, tmp_encoder_outputs), tmp = test_encoder_single_sample()\n",
    "tmp\n",
    "tmp_encoder_output.size()\n",
    "tmp_encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (emb_lin): Linear (1351 -> 384)\n",
       "  (dropout): Dropout (p = 0.1)\n",
       "  (attn): Linear (768 -> 30)\n",
       "  (attn_combine): Linear (640 -> 384)\n",
       "  (rnn): GRU(384, 384, batch_first=True)\n",
       "  (lin_out): Linear (384 -> 1351)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 384]), torch.Size([1, 30])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, chars_encoded_size,\n",
    "                 n_layers=1, dropout_p=0.1, max_length=MAX_ATTENTION_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.emb_lin = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = nn.Linear(self.hidden_size+self.hidden_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size+chars_encoded_size, self.hidden_size)\n",
    "        \n",
    "        #self.module_attn = torch.nn.ModuleList([self.emb_lin, self.dropout, self.attn, self.attn_combine])\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, bidirectional=False)\n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        #self.module_rnn = torch.nn.ModuleList([self.rnn, self.lin_out])\n",
    "\n",
    "    def forward(self, last_input, hidden, encoder_outputs):\n",
    "        embedded = self.emb_lin(last_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = embedded[0]\n",
    "                \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded, hidden), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        #return embedded, attn_applied\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, attn_applied[0]), 1)\n",
    "        rnn_input = self.attn_combine(rnn_input).unsqueeze(0)\n",
    "        rnn_input = F.relu(rnn_input)\n",
    "    \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        output = F.log_softmax(self.lin_out(output[0]))\n",
    "        \n",
    "        return output, hidden[0], attn_weights\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        if self.n_layers > 1:\n",
    "            hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "            res = torch.cat((input_var, hid_var), 0)\n",
    "            return res\n",
    "        else:\n",
    "            return input_var\n",
    "        \n",
    "    def mods_split(self):\n",
    "        mods = list(decoder_rnn.modules())[1:]\n",
    "        for gru_index, mod in enumerate(mods):\n",
    "            #print(mod)\n",
    "            if type(mod) == torch.nn.modules.rnn.GRU:\n",
    "                break\n",
    "        return mods[:gru_index], mods[gru_index:]\n",
    "        \n",
    "    def mods_attn(self):\n",
    "        return self.mods_split()[0]\n",
    "        \n",
    "    def mods_gru(self):\n",
    "        return self.mods_split()[1]\n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(words_after_common), hidden_size=tmp_encoder_output.size()[1],\n",
    "                         chars_encoded_size=tmp_encoder_outputs.size()[1], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn\n",
    "\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_encoder_output, tmp_encoder_outputs)\n",
    "#tmp\n",
    "[v.size() for v in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 3, 'params': <generator object Module.parameters at 0x7fdbea1dc360>},\n",
       " {'lr': 3, 'params': <generator object Module.parameters at 0x7fdbec7d13b8>},\n",
       " {'lr': 3, 'params': <generator object Module.parameters at 0x7fdbe2d45ba0>},\n",
       " {'lr': 3, 'params': <generator object Module.parameters at 0x7fdbed6ac888>},\n",
       " {'params': <generator object Module.parameters at 0x7fdbe321feb8>},\n",
       " {'params': <generator object Module.parameters at 0x7fdbe8ff4468>},\n",
       " {'params': <generator object Module.parameters at 0x7fdbe2e27728>}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [{'params': mod.parameters(), 'lr': 3} for mod in decoder_rnn.mods_attn()]\n",
    "tmp += [{'params': mod.parameters()} for mod in decoder_rnn.mods_gru()]\n",
    "tmp.append(\n",
    "    {'params': encoder_rnn.parameters()}\n",
    ")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('barbeque sim of of of of of of of of of of of of of of of of of of',\n",
       " 'barbeque sim of of of of of of of of of of of of of of of of of of',\n",
       " 'p',\n",
       " ('P.',\n",
       "  [24, 0],\n",
       "  'LETTERS',\n",
       "  ['571998',\n",
       "   'the',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'welfare',\n",
       "   'and',\n",
       "   'household',\n",
       "   'production',\n",
       "   ':',\n",
       "   'international',\n",
       "   'perspectives',\n",
       "   '(',\n",
       "   'stephen',\n",
       "   '<SAMPLE>',\n",
       "   'jenkins',\n",
       "   ',',\n",
       "   'arie',\n",
       "   'kapteyn',\n",
       "   ',',\n",
       "   'bernard',\n",
       "   'm',\n",
       "   '.',\n",
       "   's',\n",
       "   '.',\n",
       "   'van',\n",
       "   'praag',\n",
       "   ',',\n",
       "   'eds',\n",
       "   '.']))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None, return_more=False, sample=False):\n",
    "    decoder_rnn.eval()\n",
    "    encoder_rnn.eval()\n",
    "    \n",
    "    if not sample:\n",
    "        sample = get_random_sample()\n",
    "    s_bef, s_aft, s_class, s_sentence = sample\n",
    "        \n",
    "    words_t = Variable(words_to_word_vectors_tensor(s_sentence, wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "    decoder_hidden = encoder_output\n",
    "    \n",
    "    decoded_output = []\n",
    "    decoder_attns_arr = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attns_arr.append(decoder_attns)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "                \n",
    "        decoder_input = torch.zeros(1, 1, len(words_after_index))\n",
    "        decoder_input[0, 0, word_index] = 1\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    sample_target = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "    \n",
    "    if return_more:\n",
    "        return output, decoded_output, decoder_attns_arr, sample\n",
    "    \n",
    "    return output, output, sample_target, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.iisg.nl/archives/en/files/i/10886062.phpBaraheni => h w t t dot w w w dot i s n slash dot g s dash s s h || h t t p colon slash slash w w w dot i i s g dot n l slash a r c h i v e s slash e n slash f i l e s slash i slash o n e o e i g h t e i g h t s i x o s i x t w o dot p h p b a r a h e n i \n",
      "                  ['writers', 'association', 'of', 'iran', ':', '<SAMPLE>', ',', 'reza', '(', '2005', ')', '.']\n",
      "mobilised      => publicized     || mobilized \n",
      "                  ['since', 'the', 'murder', 'was', 'motivated', 'by', 'racism', ',', 'it', '<SAMPLE>', 'large', 'parts', 'of', 'the', 'norwegian', 'population', '.']\n"
     ]
    }
   ],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        after = ' '.join([words_after_common[i] for i in s_aft[0:-1]])\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, after, '', s_sentence, ))\n",
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/     100)\n",
      "CPU times: user 2.07 s, sys: 20 ms, total: 2.09 s\n",
      "Wall time: 2.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    global encoder_output, encoder_outputs\n",
    "    words_t = Variable(words_to_word_vectors_tensor(s_sentence, wv_vecs, wv_idx)).cuda() \n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    target_arr = s_aft\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        decoder_target_i = target_arr[i]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word_index = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_input = torch.zeros(1, 1, len(words_after_index))\n",
    "        decoder_input[0, 0, word_index] = 1\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    # not used yet\n",
    "    # https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n",
    "    clip_parameters_value = 0.25\n",
    "    if clip_parameters_value:\n",
    "        for m in [decoder_rnn, encoder_rnn]:\n",
    "            torch.nn.utils.clip_grad_norm(m.parameters(), clip_parameters_value)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = [op for op in decoder_rnn.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(op['params']) for op in optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "    global optimizer\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    tmp = [{'params': mod.parameters(), 'lr': (lr/10)} for mod in decoder_rnn.mods_attn()]\n",
    "    tmp += [{'params': mod.parameters()} for mod in decoder_rnn.mods_gru()]\n",
    "    tmp.append(\n",
    "        {'params': encoder_rnn.parameters()}\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(tmp, lr=lr)\n",
    "    \n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             optimizer=optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            s_aft_sentence = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "            correct = '✓' if result == s_aft_sentence else \"✗: {}\".format(s_aft_sentence)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/whole_gen_12_1_simpler\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    10  18% (   0m 0s)   7.205   |   7.17: st -> mcm mcm (✗: saint) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    19  36% (  3m 31s)   7.179   |   7.13: organised -> recolonization (✗: organized) \n",
      "    28  54% (  3m 31s)   6.785   |   1.79: 1968 ->  (✗: nineteen sixty eight) \n",
      "    37  72% (  3m 31s)   6.045   |   3.64: Centre ->  (✗: center) \n",
      "    46  90% (  3m 31s)   5.885   |   3.60: bros ->  (✗: brothers) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   551  53% (   0m 6s)   3.388   |   4.87: MTV -> and <EOS> <EOS> (✗: m t v) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 15s)   2.691   |   1.04: Goal.com -> nineteen (✗: g o a l dot c o m) \n",
      "  3000  22% (  0m 31s)   2.460   |   2.69: jr -> saint (✗: junior) (forcing)\n",
      "  4000  33% (  0m 47s)   2.367   |   2.72: 23 March 1939 -> the twenty of of twenty twenty (✗: the twenty third of march nineteen thirty nine) \n",
      "  5000  44% (   1m 3s)   2.252   |   4.58: December 12, 2012 -> the twenty of twenty twenty (✗: december twelfth twenty twelve) (forcing)\n",
      "  6000  56% (  1m 19s)   2.115   |   2.55: IA -> p (✗: i a) \n",
      "  7000  67% (  1m 34s)   1.968   |   4.70: 5.7% -> nineteen s <EOS> <EOS> (✗: five point seven percent) (forcing)\n",
      "  8000  78% (  1m 50s)   1.951   |   2.25: C.A. -> p s (✗: c a) (forcing)\n",
      "  9000  89% (   2m 7s)   1.897   |   3.39: 460 -> u hundred three (✗: four hundred sixty) (forcing)\n",
      " 10000 100% (  2m 25s)   1.872   |   1.69: colour -> center (✗: color) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (  2m 58s)   0.696   |   0.00: ISBN -> i s b n (✓) (forcing)\n",
      " 30000  22% (  5m 49s)   0.509   |   0.53: LAUSD -> l a u s <EOS> (✗: l a u s d) (forcing)\n",
      " 40000  33% (  8m 50s)   0.442   |   0.00: Theatre -> theater (✓) (forcing)\n",
      " 50000  44% ( 11m 47s)   0.325   |   0.00: Scoop.co.nz -> s c o o p dot c o dot n z (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 88.65% (    8865/   10000)\n",
      " 60000  56% ( 15m 39s)   0.356   |   0.00: FutureSource.com -> f u t u r e s o u r c e dot c o m (✓) \n",
      " 70000  67% ( 18m 43s)   0.213   |   1.13: IFLA's -> i f l l a's (✗: i f l a's) \n",
      " 80000  78% ( 21m 46s)   0.225   |   0.00: dr -> doctor (✓) \n",
      " 90000  89% ( 24m 50s)   0.217   |   0.00: Ibu -> i b u (✓) \n",
      "100000 100% ( 27m 42s)   0.209   |   4.62: modernisation -> saint (✗: modernization) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.54% (    9254/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000   5% (  2m 56s)   0.199   |   0.00: # -> number (✓) (forcing)\n",
      "120000  10% (  5m 47s)   0.213   |   0.00: & -> and (✓) \n",
      "130000  15% (  8m 46s)   0.252   |   0.00: sq -> square (✓) (forcing)\n",
      "140000  20% ( 11m 42s)   0.174   |   0.00: & -> and (✓) \n",
      "150000  25% ( 14m 34s)   0.222   |   0.01: CALISMALAR -> c a l i s m a l a r (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.09% (    9309/   10000)\n",
      "160000  30% ( 18m 13s)   0.252   |   0.00: M. -> m (✓) (forcing)\n",
      "170000  35% (  21m 4s)   0.176   |   0.00: Centre -> center (✓) \n",
      "180000  40% ( 23m 57s)   0.230   |   0.00: GVK -> g v k (✓) \n",
      "190000  45% ( 26m 48s)   0.243   |   0.00: jr -> junior (✓) \n",
      "200000  50% ( 29m 44s)   0.271   |   0.00: Scrum.com -> s c r u m dot c o m (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.49% (    9249/   10000)\n",
      "210000  55% ( 33m 29s)   0.197   |   0.00: CEV -> c e v (✓) \n",
      "220000  60% ( 36m 28s)   0.221   |   0.00: & -> and (✓) (forcing)\n",
      "230000  65% ( 39m 23s)   0.179   |   0.00: & -> and (✓) \n",
      "240000  70% ( 42m 20s)   0.168   |   0.00: LB -> pound (✓) \n",
      "250000  75% ( 45m 11s)   0.269   |   0.00: C.S. -> c s (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.26% (    9226/   10000)\n",
      "260000  80% ( 48m 53s)   0.198   |   0.00: - -> to (✓) \n",
      "270000  85% ( 51m 42s)   0.262   |   0.00: T. -> t (✓) (forcing)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-17229cbbb125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-714df54beb04>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-0764d8fe0758>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_after_common\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Use own prediction as next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdecoded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_samples[-2]['before'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dict_path = 'data/models/whole_gen_12_1_simpler/250000_'\n",
    "\n",
    "decoder_rnn.load_state_dict(torch.load(state_dict_path + 'DecoderRNN'))\n",
    "encoder_rnn.load_state_dict(torch.load(state_dict_path + 'EncoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample_data = sample_data[sample_data['before'].str.len() <= 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286765   5% (  2m 51s)   0.233   |   0.00: CSDSD -> c s d s d (✓) (forcing)\n",
      "296765  10% (  5m 48s)   0.279   |   0.00: & -> and (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.19% (    9319/   10000)\n",
      "306765  15% (  9m 31s)   0.206   |   0.00: 119 -> one hundred nineteen (✓) (forcing)\n",
      "316765  20% ( 12m 30s)   0.323   |   0.00: 2013 -> twenty thirteen (✓) \n",
      "326765  25% ( 15m 25s)   0.256   |   0.00: VPS -> v p s (✓) (forcing)\n",
      "336765  30% ( 18m 25s)   0.370   |   0.00: st -> saint (✓) \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-74ab6beeac73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-714df54beb04>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-0764d8fe0758>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_after_common\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Use own prediction as next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdecoded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dict_path = 'data/models/whole_gen_12_1_simpler/300000_'\n",
    "\n",
    "decoder_rnn.load_state_dict(torch.load(state_dict_path + 'DecoderRNN'))\n",
    "encoder_rnn.load_state_dict(torch.load(state_dict_path + 'EncoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_samples[-2]['before'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76705"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_data[balanced_data['before'].str.len()>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running balanced_data stuff again so its reseted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_data[balanced_data['before'].str.len()>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346957  10% (  2m 34s)   0.192   |   0.00: 2004 -> two thousand four (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.88% (    9388/   10000)\n",
      "356957  20% (  5m 56s)   0.262   |   0.00: GOSAT -> g o s a t (✓) (forcing)\n",
      "366957  30% (  8m 31s)   0.257   |   0.00: 90% -> ninety percent (✓) \n",
      "376957  40% (  11m 5s)   0.248   |   0.00: pp -> p p (✓) (forcing)\n",
      "386957  50% ( 13m 39s)   0.207   |   0.00: March 2012 -> march twenty twelve (✓) (forcing)\n",
      "396957  60% ( 16m 13s)   0.279   |   0.00: III -> three (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.51% (    9251/   10000)\n",
      "406957  70% (  20m 1s)   0.217   |   0.00: & -> and (✓) \n",
      "416957  80% ( 22m 32s)   0.230   |   0.00: - -> to (✓) \n",
      "426957  90% (  25m 5s)   0.283   |   0.53: 6' -> six (✗: six feet) \n",
      "436957 100% ( 27m 44s)   0.245   |   0.00: - -> to (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~              => tilde          || to \n",
      "                  ['2004', ':', 'february', '5', '<SAMPLE>', '15', ',', 'banqiao', 'city', ',', 'taipei', 'county', '(', 'now', 'is', 'banqiao', 'district', ',', 'new', 'taipei', 'city', ')', '.']\n",
      "st             => street         || saint \n",
      "                  ['entries', 'played', '18', 'holes', 'at', 'prince', \"'s\", 'and', '18', 'holes', 'at', 'royal', '<SAMPLE>', 'george', \"'s\", '.', 'the', 'top', '100', 'and', 'ties', 'qualified', '.']\n",
      "uclabruins.com => u c l a b r u i n dot u c o m dot c o m || u c l a b r u i n s dot c o m \n",
      "                  ['bruins', 'homepage', '\"', '<SAMPLE>', 'retrieved', 'on', 'october', '1', ',', '2008', '.']\n",
      "Burnsbog.org   => b u r n s b o dot e o r g || b u r n s b o g dot o r g \n",
      "                  ['<SAMPLE>', 'archived', 'december', '17', ',', '2013', ',', 'at', 'the', 'wayback', 'machine', '.']\n",
      "50th           => five           || fiftieth \n",
      "                  ['\"', 'miles', 'revisited', ':', 'sketches', 'of', 'spain', '(', '<SAMPLE>', 'anniversary', 'edition', ')', '&', 'miles', 'ahead', 'live', '\"', '.']\n",
      "2015-08-23     => the twenty third of august twenty eleven || the twenty third of august twenty fifteen \n",
      "                  ['retrieved', 'on', '<SAMPLE>', '.']\n",
      "90765          => nine hundred seventy six || nine o seven six five \n",
      "                  ['map', 'link', 'to', 'hans', 'vogel', 'strasse', ',', '<SAMPLE>', 'furth', ',', 'germany', 'google', 'maps', '.']\n",
      "BestofNewOrleans.com => b e s t o f n e w o r l e a n dot e a n v || b e s t o f n e w o r l e a n s dot c o m \n",
      "                  ['<SAMPLE>', ',', 'february', '5', ',', '2010', '.']\n",
      "P.D.           => p              || p d \n",
      "                  ['\"', 'some', 'new', 'or', 'otherwise', 'noteworthy', 'plants', 'from', 'west', 'virginia', '\"', '(', '1952', ')', ',', 'castanea', ',', '17', ':', '165', '(', 'with', '<SAMPLE>', 'strausbaugh', ')', '.']\n",
      "2014-02-27     => the twenty seventh of february twenty eleven || the twenty seventh of february twenty fourteen \n",
      "                  ['issaquah', 'girls', 'capture', '4', 'a', 'soccer', 'championshipissaquah', 'school', 'district', 'accessed', '<SAMPLE>', '\"', 'jennie', 'reed', 'foundation', '\"', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446957  10% (  2m 38s)   0.230   |   0.00: PDF -> p d f (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.89% (    9289/   10000)\n",
      "456957  20% (   6m 6s)   0.254   |   3.20: Malayalamcinema.com -> m a l a y a l a a c u n n m a i n v m a (✗: m a l a y a l a m c i n e m a dot c o m) \n",
      "466957  30% (  8m 41s)   0.238   |   0.00: theatre -> theater (✓) (forcing)\n",
      "476957  40% ( 11m 14s)   0.216   |   0.00: 8.1 -> eight point one (✓) \n",
      "486957  50% ( 13m 50s)   0.171   |   1.19: MtShastaNews.com -> m t s h a s t a n e w s e e s s (✗: m t s h a s t a n e w s dot c o m) \n",
      "496957  60% ( 16m 26s)   0.179   |   0.00: - -> to (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.61% (    9461/   10000)\n",
      "506957  70% ( 19m 51s)   0.150   |   0.00: 500 -> five hundred (✓) \n",
      "516957  80% ( 22m 25s)   0.151   |   0.00: sq -> square (✓) (forcing)\n",
      "526957  90% (  25m 2s)   0.180   |   0.00: # -> number (✓) (forcing)\n",
      "536957 100% ( 27m 38s)   0.172   |   0.00: vs -> versus (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546957  10% (  2m 33s)   0.149   |   0.00: 14 -> fourteen (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.94% (    9494/   10000)\n",
      "556957  20% (  5m 52s)   0.179   |   0.00: 1925 -> nineteen twenty five (✓) \n",
      "566957  30% (  8m 23s)   0.132   |   0.00: & -> and (✓) \n",
      "576957  40% ( 10m 55s)   0.152   |   0.00: 9 September 1977 -> the ninth of september nineteen seventy seven (✓) \n",
      "586957  50% ( 13m 25s)   0.172   |   0.00: # -> number (✓) (forcing)\n",
      "596957  60% ( 15m 57s)   0.143   |   0.00: - -> to (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.17% (    9517/   10000)\n",
      "606957  70% ( 19m 22s)   0.145   |   0.00: vol -> volume (✓) \n",
      "616957  80% (  22m 0s)   0.159   |   0.00: 10.0% -> ten point zero percent (✓) (forcing)\n",
      "626957  90% ( 24m 32s)   0.176   |   0.00: : -> to (✓) (forcing)\n",
      "636957 100% (  27m 4s)   0.144   |   0.00: UN -> u n (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646957  10% (  2m 32s)   0.139   |   0.00: June 3, 2012 -> june third twenty twelve (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.41% (    9541/   10000)\n",
      "656957  20% (  5m 50s)   0.096   |   0.00: & -> and (✓) \n",
      "666957  30% (  8m 23s)   0.124   |   0.00: - -> to (✓) \n",
      "676957  40% ( 10m 54s)   0.115   |   0.00: & -> and (✓) \n",
      "686957  50% ( 13m 26s)   0.080   |   0.00: organisation -> organization (✓) (forcing)\n",
      "696957  60% ( 15m 56s)   0.123   |   2.92: $161.03bn -> one hundred sixty one point one billion dollars (✗: one hundred sixty one point o three billion dollars) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.32% (    9532/   10000)\n",
      "706957  70% ( 19m 14s)   0.111   |   0.00: APH -> a p h (✓) \n",
      "716957  80% ( 21m 44s)   0.179   |   0.00: recognisable -> recognizable (✓) \n",
      "726957  90% ( 24m 13s)   0.152   |   0.00: Centre -> center (✓) (forcing)\n",
      "736957 100% ( 26m 45s)   0.117   |   0.00: Theatre -> theater (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746957  10% (  2m 31s)   0.108   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.23% (    9523/   10000)\n",
      "756957  20% (  5m 54s)   0.117   |   0.00: 30 January -> the thirtieth of january (✓) \n",
      "766957  30% (  8m 27s)   0.138   |   0.00: UK -> u k (✓) \n",
      "776957  40% ( 10m 59s)   0.112   |   0.00: # -> number (✓) \n",
      "786957  50% ( 13m 31s)   0.126   |   0.00: ALH -> a l h (✓) \n",
      "796957  60% (  16m 5s)   0.129   |   0.00: 10.3% -> ten point three percent (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.56% (    9556/   10000)\n",
      "806957  70% ( 19m 27s)   0.130   |   0.00: 15 -> fifteen (✓) \n",
      "816957  80% (  22m 1s)   0.116   |   0.00: 2007 -> two thousand seven (✓) (forcing)\n",
      "826957  90% ( 24m 36s)   0.097   |   0.00: st -> saint (✓) \n",
      "836957 100% (  27m 7s)   0.113   |   0.00: & -> and (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846957  10% (  2m 30s)   0.113   |   0.00: metres -> meters (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.12% (    9612/   10000)\n",
      "856957  20% (  5m 52s)   0.105   |   0.00: ATOMAL -> a t o m a l (✓) (forcing)\n",
      "866957  30% (  8m 24s)   0.138   |   0.00: popularised -> popularized (✓) \n",
      "876957  40% (  11m 7s)   0.104   |   0.00: 13th -> thirteenth (✓) (forcing)\n",
      "886957  50% ( 13m 44s)   0.148   |   0.00: mr -> mister (✓) (forcing)\n",
      "896957  60% ( 16m 23s)   0.099   |   0.00: December 17, 2009 -> december seventeenth two thousand nine (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.85% (    9585/   10000)\n",
      "906957  70% (  20m 0s)   0.102   |   0.00: organisation -> organization (✓) \n",
      "916957  80% ( 22m 45s)   0.087   |   0.00: etc -> etcetera (✓) \n",
      "926957  90% ( 25m 22s)   0.113   |   0.00: - -> to (✓) \n",
      "936957 100% (  28m 6s)   0.103   |   1.99: #NU065 -> hash four o five five (✗: hash nu o six five) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hr             => hour           || h r \n",
      "                  ['in', '1997', ',', '<SAMPLE>', 'baty', 'won', 'the', 'kosice', 'challenger', 'title', 'defeating', 'nicolas', 'lapentti', '.']\n",
      "Operissimo.comKlaus => o              || o p e r i s s i m o dot c o m k l a u s \n",
      "                  ['<SAMPLE>', 'ulrich', 'spiegel', ':', '\"', 'la', 'voix', 'de', 'corse', '.']\n",
      "monopolised    => x              || monopolized \n",
      "                  ['his', 'socialist', 'activism', '<SAMPLE>', 'his', 'time', ',', 'forcing', 'him', 'to', 'abandon', 'a', 'translation', 'of', 'the', 'persian', 'shahnameh', '.']\n",
      "flavouring     => twenty l a v o u r i i g || flavoring \n",
      "                  ['it', 'is', 'used', 'as', 'a', 'dessert', '<SAMPLE>', 'for', 'malasadas', ',', 'cheesecakes', ',', 'cookies', ',', 'ice', 'cream', 'and', 'mochi', '.']\n",
      "socialisation  => of             || socialization \n",
      "                  ['they', 'tend', 'to', 'assign', 'to', 'them', ',', 'for', 'functional', 'purposes', ',', 'the', 'role', 'of', '<SAMPLE>', 'agents', '.']\n",
      "2009           => two thousand nine || two o o nine \n",
      "                  ['starting', 'in', 'mid', '<SAMPLE>', ',', 'it', 'began', 'releasing', 'new', 'features', 'gradually', 'to', 'help', 'measure', 'customer', 'interest', ',', 'a', 'lean', 'startup', 'technique', '.']\n",
      "X              => tenth          || ten \n",
      "                  ['\"', '12', 'matchups', 'we', 'want', 'to', 'see', 'in', 'street', 'fighter', '<SAMPLE>', 'tekken', '\"', '.']\n",
      "230.50         => two hundred thirty thousand point five o || two hundred thirty point five o \n",
      "                  ['flatstone', 'lake', '192', 'l', 'is', '<SAMPLE>', 'hectares', '.']\n",
      "Leakymails.blogspot.com => l e a k y m a i l s dot b l o g dot p o o dot || l e a k y m a i l s dot b l o g s p o t dot c o m \n",
      "                  ['\"', 'argentina', ':', 'judge', 'orders', 'all', 'isps', 'to', 'block', 'the', 'sites', 'leakymails', '.', 'com', 'and', '<SAMPLE>', '\"', ',', 'opennet', 'initiative', ',', '11', 'august', '2011', '.']\n",
      "Comic-Con.org  => c o m i c d c o n dot o r g || c o m i c d a s h c o n dot o r g \n",
      "                  ['bill', 'finger', 'award', 'recipients', 'announced', ',', '<SAMPLE>', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946957  10% (  2m 39s)   0.236   |   0.00: Theatre -> theater (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.97% (    9297/   10000)\n",
      "956957  20% (  6m 12s)   0.324   |   0.00: BPS -> b p s (✓) \n",
      "966957  30% (  8m 54s)   0.279   |   2.38: IX -> the first (✗: the ninth) \n",
      "976957  40% ( 11m 31s)   0.233   |   0.00: CCi -> c c i (✓) \n",
      "986957  50% ( 14m 12s)   0.294   |   0.00: $ -> dollar (✓) \n",
      "996957  60% ( 16m 57s)   0.248   |   0.00: - -> to (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.25% (    9225/   10000)\n",
      "1006957  70% ( 20m 34s)   0.307   |   0.00: 50 -> fifty (✓) \n",
      "1016957  80% ( 23m 18s)   0.211   |   1.32: 2007-10-13 -> the thirteenth of october (✗: the thirteenth of october two thousand seven) \n",
      "1026957  90% (  26m 1s)   0.269   |   4.31: 2014-05-24 -> the twenty of april twenty fourteen (✗: the twenty fourth of may twenty fourteen) \n",
      "1036957 100% ( 28m 45s)   0.255   |   0.00: _ -> underscore (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honouring      => honor          || honoring \n",
      "                  ['essays', '<SAMPLE>', 'ervin', 'laszlo', ',', 'pp', '.']\n",
      "cand.mag       => c a n d dot a a g || c a n d dot m a g \n",
      "                  ['he', 'graduated', 'as', '<SAMPLE>', '.']\n",
      "2010s          => twenty seventies || twenty tens \n",
      "                  ['in', 'the', '<SAMPLE>', ',', 'cars', 'are', 'increasingly', 'sold', 'with', 'remote', 'control', 'door', 'locks', '.']\n",
      "1914           => nineteen fourteen || one thousand nine hundred fourteen \n",
      "                  ['sepoys', 'in', 'the', 'trenches', ':', 'the', 'indian', 'corps', 'on', 'the', 'western', 'front', ',', '<SAMPLE>', '-', '1915', '.']\n",
      "timber.coastal => t i m b e l dot c o a v t a d s d || t i m b e r dot c o a s t a l \n",
      "                  ['shrimp', ',', 'salt', ',', 'tobacco', ',', 'ground', 'nut', ',', 'mustard', 'seed', ',', 'flour', ',', 'water', 'melon', ',', '<SAMPLE>', 'fish', '.']\n",
      "WGBH's         => w g b h        || w g b h's \n",
      "                  ['\"', 'faculty', 'member', ',', 'alumna', 'among', '<SAMPLE>', \"'\", 'discovering', 'women', \"'\", '\"', '.']\n",
      "materialised   => a              || materialized \n",
      "                  ['none', 'of', 'these', '<SAMPLE>', '.']\n",
      "perspectivia.net => p e r s p e c i e || p e r s p e c t i v i a dot n e t \n",
      "                  ['in', 'cooperation', 'with', 'the', 'orient', 'institut', 'istanbul', ',', 'the', 'oib', 'publishes', 'the', 'online', 'series', 'orient', 'institut', 'studies', 'on', '<SAMPLE>', '.']\n",
      "Sixguns.com    => s i x g u s s dot c o m || s i x g u n s dot c o m \n",
      "                  ['taffin', ',', 'j', '.', '\"', 'taffin', 'tests', ':', 'the', '.', '44', 'special', '\"', '<SAMPLE>', 'website', '.']\n",
      "Foreskin.org   => f o r e s k i n dot o o r dot || f o r e s k i n dot o r g \n",
      "                  ['<SAMPLE>', '—', 'many', 'detailed', 'pictures', 'of', 'the', 'human', 'male', 'foreskininfant', 'foreskin', 'care', 'at', 'kidshealth', '.', 'org', '.', 'nzour', 'son', 'is', 'not', 'circumcised', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046957   5% (  2m 40s)   0.258   |   0.00: 14 -> fourteen (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.18% (    9318/   10000)\n",
      "1056957  10% (   6m 7s)   0.265   |   0.00: - -> to (✓) \n",
      "1066957  15% (  8m 41s)   0.221   |   0.00: AAP -> a a p (✓) \n",
      "1076957  20% ( 11m 17s)   0.182   |   0.00: ITB -> i t b (✓) \n",
      "1086957  25% ( 13m 57s)   0.170   |   0.00: 2009 -> two thousand nine (✓) \n",
      "1096957  30% ( 16m 32s)   0.157   |   0.00: # -> hash (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/1100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.85% (    9385/   10000)\n",
      "1106957  35% (  20m 0s)   0.237   |   0.00: & -> and (✓) \n",
      "1116957  40% ( 22m 48s)   0.206   |   0.00: 1837 -> eighteen thirty seven (✓) \n",
      "1126957  45% ( 25m 32s)   0.163   |   0.00: min -> minute (✓) (forcing)\n",
      "1136957  50% (  28m 9s)   0.184   |   0.00: & -> and (✓) \n",
      "1146957  55% ( 30m 50s)   0.138   |   0.00: BBC -> b b c (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.62% (    9462/   10000)\n",
      "1156957  60% ( 34m 26s)   0.209   |   0.00: J. -> j (✓) \n",
      "1166957  65% ( 37m 25s)   0.134   |   0.00: R. -> r (✓) \n",
      "1176957  70% ( 40m 19s)   0.214   |   0.00: US -> u s (✓) \n",
      "1186957  75% ( 42m 58s)   0.118   |   0.00: 2003 -> two thousand three (✓) \n",
      "1196957  80% ( 45m 45s)   0.165   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.67% (    9467/   10000)\n",
      "1206957  85% (  49m 7s)   0.138   |   0.00: & -> and (✓) (forcing)\n",
      "1216957  90% ( 51m 37s)   0.109   |   0.00: - -> to (✓) \n",
      "1226957  95% (  54m 9s)   0.131   |   0.00: H.A. -> h a (✓) \n",
      "1236957 100% ( 56m 38s)   0.173   |   0.00: 2002 -> two thousand two (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoblog.comPemberton => a u t o b l o g dot c o m || a u t o b l o g dot c o m p e m b e r t o n \n",
      "                  ['cougar', 'ace', 'salvage', 'operation', 'turns', 'deadly', 'at', '<SAMPLE>', ',', 'mary', '(', '2006', '-', '08', '-', '01', ')', '.']\n",
      "TheRecordHerald.com => t h e r e c o r d h e e o n o dot e o m n || t h e r e c o r d h e r a l d dot c o m \n",
      "                  ['\"', 'dr', 'francis', 'achampong', 'returns', 'to', 'penn', 'state', 'mont', 'alto', 'as', 'chancellor', ',', '\"', '<SAMPLE>', '.']\n",
      "DisneyInsider.com => d i s n e y i n e i d e dot n dot e e o r || d i s n e y i n s i d e r dot c o m \n",
      "                  ['<SAMPLE>', '(', 'the', 'walt', 'disney', 'company', ')', '.']\n",
      "March 1949     => march nineteen forty three || march nineteen forty nine \n",
      "                  ['in', '<SAMPLE>', ',', 'the', 'committee', 'of', 'the', 'royal', 'academy', 'in', 'england', 'accepted', 'the', 'works', 'annigoni', 'offered', 'for', 'its', 'annual', 'exhibition', '.']\n",
      "1449966373     => the three four seventy three hundred seventy three hundred seventy three hundred seventy three three four three seventy three hundred || one billion four hundred forty nine million nine hundred sixty six thousand three hundred seventy three \n",
      "                  ['isbn', '<SAMPLE>', 'scott', ',', 'sibbald', 'david', ',', 'ed', '.']\n",
      "www.thecanadianencyclopedia.com => w w w dot t h e c a n a d i n a n a r a r || w w w dot t h e c a n a d i a n e n c y c l o p e d i a dot c o m \n",
      "                  ['encyclopedia', 'of', 'music', 'in', 'canada', ',', 'biography', 'of', 'the', 'downchild', 'blues', 'band', ';', '<SAMPLE>', '.']\n",
      "satirising     => thirteenth     || satirizing \n",
      "                  ['the', 'turnip', 'prize', 'is', 'a', 'spoof', 'uk', 'award', '<SAMPLE>', 'the', 'tate', 'gallery', \"'s\", 'turner', 'prize', 'by', 'rewarding', 'deliberately', 'bad', 'modern', 'art', '.']\n",
      "http://voices.iit.edu => h t t p colon slash slash v dot i e t t i e o n o s o || h t t p colon slash slash v o i c e s dot i i t dot e d u \n",
      "                  ['recordings', 'and', 'transcriptions', 'can', 'be', 'found', 'at', ':', '<SAMPLE>', '.']\n",
      "SaluteHeroes.org => s a l u t e h e r o e dot o dot c o r dot r g || s a l u t e h e r o e s dot o r g \n",
      "                  ['bio', 'on', '<SAMPLE>', '.']\n",
      "ConstructionKnowledge.netA => c o n s t r r c t i o n a n o n n n o n || c o n s t r u c t i o n k n o w l e d g e dot n e t a \n",
      "                  ['ned', 'pelger', ',', '<SAMPLE>', '.', 'p', '.', 'usher', ',', 'a', 'history', 'of', 'mechanical', 'inventions', ',', 'harvard', 'university', 'press', ',', '1929', '(', 'dover', 'publications', ',', 'revised', 'edition', ',', '2011', ',', 'isbn', '978', '-', '0486255934', ')', 'macdonald', ',', 'joseph', 'a', '.', 'handbook', 'of', 'rigging', ':', 'for', 'construction', 'and', 'industrial', 'operations', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246957   3% (  2m 29s)   0.138   |   0.00: 12 -> twelve (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.47% (    9447/   10000)\n",
      "1256957   7% (  5m 47s)   0.139   |   0.00: SXSW -> s x s w (✓) \n",
      "1266957  10% (  8m 18s)   0.172   |   0.00: March 12, 2013 -> march twelfth twenty thirteen (✓) \n",
      "1276957  13% ( 10m 47s)   0.134   |   0.00: W.H. -> w h (✓) (forcing)\n",
      "1286957  17% ( 13m 17s)   0.219   |   5.43: ST -> saint t (✗: s t) (forcing)\n",
      "1296957  20% ( 15m 46s)   0.175   |   0.00: # -> number (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.36% (    9536/   10000)\n",
      "1306957  23% (  19m 5s)   0.140   |   0.63: Blabbermouth.net -> b l a b b e r m o u t h dot n dot t t (✗: b l a b b e r m o u t h dot n e t) \n",
      "1316957  27% ( 21m 34s)   0.124   |   0.00: 4 -> four (✓) \n",
      "1326957  30% (  24m 5s)   0.128   |   0.00: April 12, 2011 -> april twelfth twenty eleven (✓) (forcing)\n",
      "1336957  33% ( 26m 37s)   0.148   |   0.00: BioLib.cz -> b i o l i b dot c z (✓) \n",
      "1346957  37% (  29m 5s)   0.146   |   0.00: No -> number (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.16% (    9516/   10000)\n",
      "1356957  40% ( 32m 24s)   0.109   |   0.00: rediff.com -> r e d i f f dot c o m (✓) \n",
      "1366957  43% ( 34m 53s)   0.154   |   0.00: 1983 -> nineteen eighty three (✓) \n",
      "1376957  47% ( 37m 22s)   0.199   |   0.00: - -> to (✓) \n",
      "1386957  50% ( 39m 52s)   0.113   |   0.00: T. D. -> t d (✓) \n",
      "1396957  53% ( 42m 22s)   0.215   |   0.00: 40s -> forties (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/1400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.74% (    9474/   10000)\n",
      "1406957  57% ( 45m 41s)   0.133   |   0.00: Vol -> volume (✓) \n",
      "1416957  60% ( 48m 12s)   0.162   |   0.00: Paes -> p a e's (✓) (forcing)\n",
      "1426957  63% ( 50m 42s)   0.115   |   0.00: & -> and (✓) (forcing)\n",
      "1436957  67% ( 53m 13s)   0.132   |   0.00: PDF -> p d f (✓) \n",
      "1446957  70% ( 55m 44s)   0.151   |   0.00: Centre -> center (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.28% (    9528/   10000)\n",
      "1456957  73% (  59m 2s)   0.126   |   0.00: # -> number (✓) \n",
      "1466957  77% ( 61m 32s)   0.139   |   0.00: Jun 1998 -> june nineteen ninety eight (✓) \n",
      "1476957  80% (  64m 3s)   0.167   |   0.00: ltd -> limited (✓) \n",
      "1486957  83% ( 66m 34s)   0.121   |   0.00: 9 September 1943 -> the ninth of september nineteen forty three (✓) (forcing)\n",
      "1496957  87% (  69m 2s)   0.095   |   0.00: September 27 -> september twenty seventh (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.12% (    9512/   10000)\n",
      "1506957  90% ( 72m 21s)   0.160   |   0.00: C. -> c (✓) \n",
      "1516957  93% ( 74m 50s)   0.103   |   0.00: & -> and (✓) \n",
      "1526957  97% ( 77m 21s)   0.156   |   0.00: & -> and (✓) \n",
      "1536957 100% ( 79m 51s)   0.170   |   0.00: - -> to (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuttoabruzzo.it => t u t t o a b r z z z o dot c z o e i t i || t u t t o a b r u z z o dot i t \n",
      "                  ['the', 'stone', 'city', '(', '1999', ')', '(', 'isbn', '978', '-', '0', '-', '7544', '-', '0098', '-', '1', ')', '(', '15', 'december', '2008', ')', '\"', 'doktoro', 'esperanto', '\"', ',', 'stasera', 'al', 'museo', 'colonna', 'di', 'pescara', ',', '<SAMPLE>', '.']\n",
      "HonestReporting.comBackground => h o n e s t r e p o r g m e c c c g c c || h o n e s t r e p o r t i n g dot c o m b a c k g r o u n d \n",
      "                  ['not', 'an', '\"', 'apartheid', 'wall', '\"', 'on', '<SAMPLE>', 'info', ':', 'the', 'security', 'fence', 'on', 'imra', '.', 'org', '.', 'ilis', 'israel', \"'s\", 'security', 'barrier', 'unique', '?']\n",
      "4/5            => four a         || four fifths \n",
      "                  ['louis', 'proyect', ',', 'nicaragua', ',', 'about', '<SAMPLE>', 'of', 'the', 'way', 'down', '.']\n",
      "#getcorrected  => hash tag thirteen || hash tag getcorrected \n",
      "                  ['\"', 'is', 'turnham', 'green', 'in', 'west', 'london', 'being', '<SAMPLE>', '\"', '.']\n",
      "/science.aad   => s l a s s a s a s || s l a s h s c i e n c e dot a a d \n",
      "                  ['6269', '(', '2016', ')', 'doi', ':', '10', '.', '1126', '<SAMPLE>', '2622', '.']\n",
      "978-0195157413 => nine seven eight sil o nine nine five || nine seven eight sil o one nine five one five seven four one three \n",
      "                  ['isbn', '<SAMPLE>', 'helbling', ',', 'thomas', '(', '2010', ')', '.']\n",
      "Wire2Wolves.com => w i r e t w o l v t s e o s e t s w r s || w i r e t w o w o l v e s dot c o m \n",
      "                  ['\"', 'hall', 'of', 'fame', 'at', '<SAMPLE>', '\"', '.']\n",
      "578,100 m2     => five hundred seventy eight one hundred square meters || five hundred seventy eight thousand one hundred square meters \n",
      "                  ['lake', 'sirena', ',', 'an', 'oval', 'shaped', 'lake', ',', 'has', 'a', 'surface', 'area', 'of', '142', '.', '85', '-', 'acre', '(', '<SAMPLE>', ')', '.']\n",
      "legalising     => weg            || legalizing \n",
      "                  ['on', '2', 'november', '2015', ',', 'the', 'northern', 'ireland', 'assembly', 'voted', 'for', 'a', 'fifth', 'time', 'on', 'the', 'question', 'of', '<SAMPLE>', 'same', 'sex', 'marriage', '.']\n",
      "Mindat.orgQingsongite => m i n d a t dot o o g q i g i c i g i g i || m i n d a t dot o r g q i n g s o n g i t e \n",
      "                  ['qingsongite', 'on', '<SAMPLE>', ':', 'new', 'mineral', 'from', 'tibet', 'hard', 'as', 'diamond', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546957   3% (  2m 28s)   0.189   |   0.00: RS -> r s (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.00% (    9500/   10000)\n",
      "1556957   7% (  5m 46s)   0.073   |   0.00: jr -> junior (✓) \n",
      "1566957  10% (  8m 15s)   0.100   |   0.00: US -> u s (✓) \n",
      "1576957  13% ( 10m 43s)   0.114   |   0.00: .ifr -> dot i f r (✓) \n",
      "1586957  17% ( 13m 14s)   0.111   |   0.00: : -> to (✓) \n",
      "1596957  20% ( 15m 44s)   0.176   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.44% (    9544/   10000)\n",
      "1606957  23% (  19m 2s)   0.139   |   0.00: _ -> underscore (✓) \n",
      "1616957  27% ( 21m 32s)   0.145   |   0.00: PDF -> p d f (✓) \n",
      "1626957  30% (  24m 2s)   0.166   |   0.00: MS -> m s (✓) \n",
      "1636957  33% ( 26m 31s)   0.097   |   0.00: K.V. -> k v (✓) \n",
      "1646957  37% (  29m 1s)   0.104   |   0.00: DC -> d c (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.56% (    9556/   10000)\n",
      "1656957  40% ( 32m 18s)   0.092   |   0.00: CBS -> c b s (✓) \n",
      "1666957  43% ( 34m 48s)   0.121   |   0.00: st -> saint (✓) \n",
      "1676957  47% ( 37m 16s)   0.079   |   0.00: & -> and (✓) \n",
      "1686957  50% ( 39m 47s)   0.153   |   0.00: - -> to (✓) \n",
      "1696957  53% ( 42m 18s)   0.111   |   0.00: IETF -> i e t f (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.10% (    9510/   10000)\n",
      "1706957  57% ( 45m 36s)   0.152   |   0.00: programme -> program (✓) \n",
      "1716957  60% (  48m 7s)   0.125   |   0.00: S. -> s (✓) \n",
      "1726957  63% ( 50m 35s)   0.116   |   0.00: - -> to (✓) \n",
      "1736957  67% (  53m 5s)   0.118   |   0.00: October 1967 -> october nineteen sixty seven (✓) \n",
      "1746957  70% ( 55m 33s)   0.120   |   0.00: $ -> dollar (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.67% (    9567/   10000)\n",
      "1756957  73% ( 58m 52s)   0.131   |   0.00: _ -> underscore (✓) \n",
      "1766957  77% ( 61m 21s)   0.124   |   0.00: June 18, 2013 -> june eighteenth twenty thirteen (✓) \n",
      "1776957  80% ( 63m 52s)   0.108   |   0.00: SV -> s v (✓) \n",
      "1786957  83% ( 66m 23s)   0.097   |   0.00: MD -> m d (✓) \n",
      "1796957  87% ( 68m 52s)   0.135   |   0.00: football.ch -> f o o t b a l l dot c h (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.64% (    9564/   10000)\n",
      "1806957  90% ( 72m 11s)   0.136   |   0.00: 1954 -> nineteen fifty four (✓) \n",
      "1816957  93% ( 74m 51s)   0.098   |   0.00: 1953 -> nineteen fifty three (✓) \n",
      "1826957  97% ( 77m 30s)   0.131   |   0.00: vol -> volume (✓) \n",
      "1836957 100% (  80m 9s)   0.152   |   0.00: & -> and (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseball-Reference.com => b a s e b a l l a s r a l d a s dot r t n || b a s e b a l l d a s h r e f e r e n c e dot c o m \n",
      "                  ['\"', 'jarrod', 'dyson', 'statistics', 'and', 'history', '—', '<SAMPLE>', '\"', '.']\n",
      "39-68.in       => r nine d d t i i i i i i i i i i i i i i i || t h i r t y n i n e d a s h s i x t y e i g h t dot i n \n",
      "                  ['journal', 'of', 'contemporary', 'history', '(', '1990', ')', '25', '#', '1', 'pp', ':', '<SAMPLE>', 'j', 'storleslie', 'king', ',', '\"', \"'\", 'france', 'needs', 'children', \"'\", '\"', 'sociological', 'quarterly', '(', '1998', ')', '39', '#', '1', 'pp', ':', '33', '-', '52', '.']\n",
      "Calciomercato.it => c a l c i o m e r o a t o dot || c a l c i o m e r c a t o dot i t \n",
      "                  ['<SAMPLE>', '(', 'in', 'italian', ')', '.']\n",
      "42             => forty two      || four two \n",
      "                  ['\"', 'ward', 'information', 'for', 'stranmillis', 'ward', '95', 'gg', '<SAMPLE>', '\"', '.']\n",
      "Multicolour    => tumor          || multicolor \n",
      "                  ['\"', 'first', 'halley', '<SAMPLE>', 'camera', 'imaging', 'results', 'from', 'giotto', '\"', '.']\n",
      "6,000 km2      => six thousand square meters || six thousand square kilometers \n",
      "                  ['new', 'york', \"'s\", 'drainage', 'basin', 'covers', '2', ',', '300', 'square', 'miles', '(', '<SAMPLE>', ')', '.']\n",
      "1878/9         => eighteen seventy eight eight nine || one thousand eight hundred seventy eight ninths \n",
      "                  ['in', '<SAMPLE>', ',', 'he', 'produced', 'modified', 'proposals', 'using', 'the', 'greenwich', 'meridian', '.']\n",
      "fitfortravel.nhs.uk => f i t f o r t r a v e l dot n dot e dot r m || f i t f o r t r a v e l dot n h s dot u k \n",
      "                  ['hps', 'maintains', 'a', 'website', '<SAMPLE>', 'that', 'provides', 'travellers', 'and', 'their', 'advisers', 'with', 'detailed', 'information', 'on', 'health', 'risks', '.']\n",
      "479            => four hundred seventy nine || four seven nine \n",
      "                  ['these', 'are', '07', 'bujumbura', '<SAMPLE>', ',', '10', 'windhoek', '7', ',', '07', 'bujumbura', '515', ',', '09', 'state', '15113', ',', '09', 'stockholm', '194', ',', '10', 'sanaa', '5', ',', 'and', '10', 'caracas', '107', '.']\n",
      "/X.CON.PAFG.HS => s h a s h a s a s a s a s a s o a s a s || s l a s h x dot c o n dot p a f g dot h s \n",
      "                  ['akz', '212', '-', '100600', '-', '<SAMPLE>', '.', 'w', '.', '0011', '.', '071210', 't', '0000', 'z', '-', '071211', 't', '0200', 'z', '/', 'eastern', 'norton', 'sound', 'and', 'nulato', 'hills', 'including', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846957   5% (   3m 6s)   0.145   |   0.00: theatres -> theaters (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.58% (    9558/   10000)\n",
      "1856957  10% (  6m 53s)   0.155   |   2.49: 2.10.1.1 -> t w one e e v e e e e e dot e e dot e (✗: t w o dot t e n dot o n e dot o n e) \n",
      "1866957  15% (  9m 45s)   0.150   |   0.00: - -> to (✓) (forcing)\n",
      "1876957  20% ( 12m 37s)   0.132   |   0.00: 17 January 2006 -> the seventeenth of january two thousand six (✓) \n",
      "1886957  25% ( 15m 30s)   0.103   |   0.00: & -> and (✓) \n",
      "1896957  30% ( 18m 22s)   0.167   |   0.00: DX -> d x (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/1900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.68% (    9568/   10000)\n",
      "1906957  35% (  22m 6s)   0.127   |   0.00: Theatre -> theater (✓) \n",
      "1916957  40% ( 24m 57s)   0.089   |   0.00: 1970 -> nineteen seventy (✓) \n",
      "1926957  45% ( 27m 49s)   0.121   |   0.00: & -> and (✓) \n",
      "1936957  50% ( 30m 40s)   0.105   |   0.00: Ancestry.com -> a n c e s t r y dot c o m (✓) \n",
      "1946957  55% ( 33m 31s)   0.153   |   0.00: EU -> e u (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/1950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.71% (    9571/   10000)\n",
      "1956957  60% ( 37m 16s)   0.130   |   0.00: jr -> junior (✓) \n",
      "1966957  65% (  40m 8s)   0.111   |   0.00: & -> and (✓) \n",
      "1976957  70% (  43m 0s)   0.110   |   0.00: labour -> labor (✓) \n",
      "1986957  75% ( 45m 51s)   0.134   |   0.00: 8 December 2007 -> the eighth of december two thousand seven (✓) \n",
      "1996957  80% ( 48m 43s)   0.105   |   4.56: 9/13/2012 -> the twenty of twelve (✗: september thirteenth twenty twelve) (forcing)\n",
      "Saved model to data/models/whole_gen_12_1_simpler/2000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.51% (    9551/   10000)\n",
      "2006957  85% ( 52m 28s)   0.057   |   0.00: D.C. -> d c (✓) \n",
      "2016957  90% ( 55m 21s)   0.084   |   0.00: 10 -> ten (✓) \n",
      "2026957  95% ( 58m 12s)   0.127   |   0.00: & -> and (✓) \n",
      "2036957 100% (  61m 7s)   0.110   |   0.00: 1999 -> nineteen ninety nine (✓) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0071546383     => o o seven one five four six three eight || o o seven one five four six three eight three \n",
      "                  ['isbn', '<SAMPLE>', ',', 'isbn', '978', '-', '0', '-', '07', '-', '154638', '-', '6', '.']\n",
      "ChildTrends.org => c h i l d t r e n d s dot o r r r h o r g || c h i l d t r e n d s dot o r g \n",
      "                  ['<SAMPLE>', ',', 'last', 'updated', 'july', '2014', '.']\n",
      "UKGameshows.com => u k g a m e s h h s s h c o m || u k g a m e s h o w s dot c o m \n",
      "                  ['5', '-', 'star', 'family', 'reunion', 'at', '<SAMPLE>', '.']\n",
      "1988           => nineteen eighty eight || one thousand nine hundred eighty eight \n",
      "                  ['four', 'new', 'players', '—', 'gérard', 'buscher', ',', 'merry', 'krimau', ',', 'thierry', 'fernier', 'and', 'sonny', 'silooy', '—', 'were', 'signed', 'for', 'the', '1987', '-', '<SAMPLE>', 'season', '.']\n",
      "BaseCampMD.com => b a s e c a m p m o o c o m || b a s e c a m p m d dot c o m \n",
      "                  ['everest', 'bc', 'clinic', ',', '<SAMPLE>', '.']\n",
      "Broadwayworld.com => b r o a d w a y w o r l d o c o m o || b r o a d w a y w o r l d dot c o m \n",
      "                  ['<SAMPLE>', ',', 'december', '6', ',', '2010', 'jones', ',', 'kenneth', '.']\n",
      "UnlockingClave.com => u n l o c k i n g c l a g e dot c a e dot e || u n l o c k i n g c l a v e dot c o m \n",
      "                  ['thesis', ':', 'the', '3', ':', '2', 'relationship', 'as', 'the', 'foundation', 'of', 'timelines', 'in', 'west', 'african', 'musics', ',', '<SAMPLE>', '.']\n",
      "XV             => fifteen        || the fifteenth \n",
      "                  ['injustice', 'of', 'louis', '<SAMPLE>', '.']\n",
      "Twnpnews.comSolie => t w n p n e w s dot c o m s m o m o m o m || t w n p n e w s dot c o m s o l i e \n",
      "                  ['generalbrian', 'elliott', 'interviews', 'nigel', 'mcguinness', 'at', 'myreviewer', '.', 'comnigel', 'mcguinness', 'at', 'iwcwrestling', '.', 'cominterview', 'at', '<SAMPLE>', \"'\", 's', 'title', 'historiesspecific', '\"', 'online', 'world', 'of', 'wrestling', '\"', '.']\n",
      "Australian-Charts.com => a u s t r a l i a n n d a c r a c c c o || a u s t r a l i a n d a s h c h a r t s dot c o m \n",
      "                  ['archived', 'from', 'the', 'originalalbum', 'top', '50', '<SAMPLE>', '.']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2046957   2% (  2m 50s)   0.118   |   0.00: Organisation -> organization (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.87% (    9587/   10000)\n",
      "2056957   4% (  6m 35s)   0.153   |   0.00: mt -> mount (✓) \n",
      "2066957   6% (  9m 27s)   0.128   |   0.00: 1906 -> nineteen o six (✓) \n",
      "2076957   8% ( 12m 18s)   0.142   |   0.00: & -> and (✓) \n",
      "2086957  10% (  15m 7s)   0.130   |   0.00: criticised -> criticized (✓) \n",
      "2096957  12% ( 17m 57s)   0.144   |   0.00: FCA -> f c a (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.32% (    9532/   10000)\n",
      "2106957  14% ( 21m 42s)   0.130   |   0.00: - -> to (✓) \n",
      "2116957  16% ( 24m 32s)   0.094   |   0.00: : -> to (✓) \n",
      "2126957  18% ( 27m 24s)   0.115   |   0.00: jr -> junior (✓) \n",
      "2136957  20% ( 30m 16s)   0.102   |   0.00: & -> and (✓) \n",
      "2146957  22% (  33m 5s)   0.194   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.73% (    9573/   10000)\n",
      "2156957  24% ( 36m 49s)   0.154   |   0.00: PDF -> p d f (✓) \n",
      "2166957  26% ( 39m 39s)   0.094   |   0.00: 2012 -> twenty twelve (✓) \n",
      "2176957  28% ( 42m 31s)   0.111   |   0.00: characterised -> characterized (✓) \n",
      "2186957  30% ( 45m 22s)   0.088   |   0.00: 21 July 2010 -> the twenty first of july twenty ten (✓) \n",
      "2196957  32% ( 48m 12s)   0.146   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.74% (    9574/   10000)\n",
      "2206957  34% ( 51m 56s)   0.126   |   0.00: metres -> meters (✓) \n",
      "2216957  36% ( 54m 47s)   0.110   |   0.00: Viddsee.com -> v i d d s e e dot c o m (✓) \n",
      "2226957  38% ( 57m 38s)   0.107   |   0.00: NFL -> n f l (✓) \n",
      "2236957  40% ( 60m 29s)   0.130   |   0.00: 18 -> eighteen (✓) \n",
      "2246957  42% ( 63m 18s)   0.116   |   0.00: : -> to (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.07% (    9607/   10000)\n",
      "2256957  44% (  67m 2s)   0.089   |   1.42: 0.8.7.0 -> zero dot t dot e o (✗: o dot e i g h t dot s e v e n dot o) \n",
      "2266957  46% ( 69m 54s)   0.131   |   0.00: 267 -> two hundred sixty seven (✓) \n",
      "2276957  48% ( 72m 43s)   0.096   |   0.00: PhD -> p h d (✓) \n",
      "2286957  50% ( 75m 35s)   0.133   |   0.00: vol -> volume (✓) \n",
      "2296957  52% ( 78m 26s)   0.144   |   0.00: Barbecue -> barbeque (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.05% (    9605/   10000)\n",
      "2306957  54% (  82m 8s)   0.118   |   0.00: Centre -> center (✓) \n",
      "2316957  56% ( 84m 58s)   0.089   |   0.00: st -> saint (✓) \n",
      "2326957  58% ( 87m 48s)   0.130   |   0.00: & -> and (✓) \n",
      "2336957  60% ( 90m 40s)   0.102   |   0.00: - -> to (✓) \n",
      "2346957  62% ( 93m 31s)   0.090   |   0.00: - -> to (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.76% (    9576/   10000)\n",
      "2356957  64% ( 97m 14s)   0.153   |   0.00: PDF -> p d f (✓) \n",
      "2366957  66% ( 100m 6s)   0.098   |   0.00: January 1874 -> january eighteen seventy four (✓) \n",
      "2376957  68% (102m 55s)   0.116   |   0.00: - -> to (✓) \n",
      "2386957  70% (105m 46s)   0.104   |   0.00: - -> to (✓) \n",
      "2396957  72% (108m 35s)   0.137   |   0.00: 2008 -> two thousand eight (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.21% (    9621/   10000)\n",
      "2406957  74% (112m 20s)   0.090   |   0.00: # -> number (✓) \n",
      "2416957  76% (115m 12s)   0.085   |   0.00: 0.8% -> zero point eight percent (✓) \n",
      "2426957  78% ( 118m 1s)   0.129   |   0.00: Pr -> p r (✓) \n",
      "2436957  80% (120m 51s)   0.136   |   0.00: 3 -> three (✓) \n",
      "2446957  82% (123m 41s)   0.100   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.89% (    9589/   10000)\n",
      "2456957  84% (127m 25s)   0.093   |   0.00: vs -> versus (✓) \n",
      "2466957  86% (130m 17s)   0.083   |   0.00: 14 February 2014 -> the fourteenth of february twenty fourteen (✓) \n",
      "2476957  88% ( 133m 6s)   0.113   |   0.00: XML -> x m l (✓) \n",
      "2486957  90% (135m 56s)   0.104   |   0.00: L. -> l (✓) \n",
      "2496957  92% (138m 46s)   0.136   |   0.00: 426 -> four hundred twenty six (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.78% (    9578/   10000)\n",
      "2506957  94% (142m 29s)   0.135   |   0.00: sq -> square (✓) \n",
      "2516957  96% (145m 20s)   0.126   |   0.00: 1902 -> nineteen o two (✓) \n",
      "2526957  98% (148m 11s)   0.175   |   0.00: 17 April 2009 -> the seventeenth of april two thousand nine (✓) \n",
      "2536957 100% ( 151m 3s)   0.095   |   0.00: 2014 -> twenty fourteen (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ustravelweather.com => u s t r a v e l w e a t h e m r a h e m || u s t r a v e l w e a t h e r dot c o m \n",
      "                  ['\"', 'average', 'weather', 'for', 'cairo', ',', 'il', '\"', ',', 'weather', '.', 'com', '\"', 'chicago', 'weather', '\"', ',', 'ustravelweather', '.', 'com', '\"', '\"', ',', 'weather', '.', 'com', '\"', 'moline', 'weather', '\"', ',', 'ustravelweather', '.', 'com', '\"', 'peoria', 'weather', '\"', ',', 'ustravelweather', '.', 'com', '\"', 'rockford', 'weather', '\"', ',', '<SAMPLE>', '\"', 'springfield', 'weather', '\"', ',', 'ustravelweather', '.', 'comtammy', 'webber', '(', 'the', 'associated', 'press', ')', '(', '2002', '-', '01', '-', '13', ')', '.']\n",
      "Avesta.orgBeyond => a v e s t a dot o r g b e y o n dot o y l || a v e s t a dot o r g b e y o n d \n",
      "                  ['isbn', '0', '-', '8010', '-', '2160', '-', 'x', ',', '9780801021602', 'vendidad', '1', ',', 'at', '<SAMPLE>', 'is', 'arachosia', ',', '36', 'schoeni', '.']\n",
      "1-84309-164    => one sil eight four three sil nine sil sil one sil sil || one sil eight four three o nine sil one six four \n",
      "                  ['isbn', '<SAMPLE>', '-', 'x', '.', '\"', 'species', 'factsheet', ':', 'agapornis', 'fischeri', '\"', '.']\n",
      "4.1.3.36       => t point o o r e o o e h o r e r o r o r o r || f o u r dot o n e dot t h r e e dot t h i r t y s i x \n",
      "                  ['naphthoate', 'synthase', '(', 'menb', ',', 'or', 'dhna', 'synthetase', ';', 'ec', '<SAMPLE>', ')', ',', 'a', 'bacterial', 'enzyme', 'involved', 'in', 'the', 'biosynthesis', 'of', 'menaquinone', '(', 'vitamin', 'k', '2', ')', '.']\n",
      "NashuaTelegraph.com => n a s h u a t e l e g r a p p h a a p h || n a s h u a t e l e g r a p h dot c o m \n",
      "                  ['<SAMPLE>', ':', 'college', 'officials', 'cited', 'finances', ',', 'accreditation', 'in', 'court', 'petition', '.']\n",
      "20             => twenty         || two o \n",
      "                  ['\"', 'seven', 'samurai', '<SAMPLE>', 'xx', 'walkthrough', '\"', '.']\n",
      "0-385-15090-3  => o sil three eight five sil five five o nine o sil three || o sil three eight five sil one five o nine o sil three \n",
      "                  ['new', 'york', ':', 'doubleday', ',', '1979', 'isbn', '<SAMPLE>', 'hogg', ',', 'ian', 'v', '.', 'german', 'artillery', 'of', 'world', 'war', 'two', '.']\n",
      "beehive.govt.nz => b e e h i v e dot g o v t dot t dot n dot n v dot || b e e h i v e dot g o v t dot n z \n",
      "                  ['margaret', 'wilson', ',', 'deed', 'of', 'settlement', 'between', 'the', 'crown', 'and', 'ngati', 'awa', ',', 'press', 'release', ',', '<SAMPLE>', ',', '27', 'march', '2003', '.']\n",
      "Blu-express.com => b l u d e o    || b l u d a s h e x p r e s s dot c o m \n",
      "                  ['the', '2012', '<SAMPLE>', 'tennis', 'cup', 'was', 'a', 'professional', 'tennis', 'tournament', 'played', 'on', 'clay', 'courts', '.']\n",
      "#So            => hash tag o     || hash tag so \n",
      "                  ['example', ':', '-', '-', 'this', 'is', 'a', 'one', 'line', 'comment', '<SAMPLE>', 'is', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2546957   2% (  2m 48s)   0.117   |   0.00: NoviList.hr -> n o v i l i s t dot h r (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.91% (    9591/   10000)\n",
      "2556957   4% (  6m 31s)   0.106   |   0.00: mr -> mister (✓) \n",
      "2566957   6% (  9m 20s)   0.086   |   0.00: behaviour -> behavior (✓) \n",
      "2576957   8% (  12m 8s)   0.133   |   0.00: # -> number (✓) \n",
      "2586957  10% ( 14m 58s)   0.163   |   0.00: & -> and (✓) \n",
      "2596957  12% ( 17m 48s)   0.183   |   0.00: cDNA -> c d n a (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.45% (    9545/   10000)\n",
      "2606957  14% ( 21m 34s)   0.129   |   0.00: - -> to (✓) \n",
      "2616957  16% ( 24m 25s)   0.174   |   0.00: - -> to (✓) \n",
      "2626957  18% ( 27m 15s)   0.135   |   0.00: vs -> versus (✓) \n",
      "2636957  20% (  30m 6s)   0.079   |   0.00: 1953 -> nineteen fifty three (✓) \n",
      "2646957  22% ( 32m 56s)   0.118   |   0.00: 2007-01-23 -> the twenty third of january two thousand seven (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.75% (    9575/   10000)\n",
      "2656957  24% ( 36m 40s)   0.097   |   0.00: U.S. -> u s (✓) \n",
      "2666957  26% ( 39m 30s)   0.143   |   0.00: # -> number (✓) \n",
      "2676957  28% ( 42m 22s)   0.164   |   0.00: RJ -> r j (✓) \n",
      "2686957  30% ( 45m 15s)   0.113   |   0.00: 1969 -> nineteen sixty nine (✓) \n",
      "2696957  32% ( 48m 12s)   0.084   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.39% (    9639/   10000)\n",
      "2706957  34% (  52m 1s)   0.093   |   0.00: - -> to (✓) \n",
      "2716957  36% ( 54m 56s)   0.112   |   0.00: 3.17 -> three point one seven (✓) \n",
      "2726957  38% ( 57m 50s)   0.071   |   0.00: CBC -> c b c (✓) \n",
      "2736957  40% ( 60m 47s)   0.124   |   0.00: 1879 -> eighteen seventy nine (✓) \n",
      "2746957  42% ( 63m 43s)   0.089   |   0.00: April 3, 1978 -> april third nineteen seventy eight (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.21% (    9621/   10000)\n",
      "2756957  44% ( 67m 32s)   0.124   |   0.00: 69 -> sixty nine (✓) \n",
      "2766957  46% ( 70m 28s)   0.084   |   0.00: EXPN.com -> e x p n dot c o m (✓) \n",
      "2776957  48% ( 73m 24s)   0.099   |   0.00: & -> and (✓) \n",
      "2786957  50% ( 76m 20s)   0.110   |   0.00: & -> and (✓) \n",
      "2796957  52% ( 79m 19s)   0.093   |   0.00: J. W. -> j w (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.12% (    9612/   10000)\n",
      "2806957  54% ( 83m 10s)   0.090   |   0.00: centre -> center (✓) \n",
      "2816957  56% (  86m 8s)   0.135   |   0.00: 14 May 2012 -> the fourteenth of may twenty twelve (✓) \n",
      "2826957  58% (  89m 5s)   0.133   |   0.00: 2 -> two (✓) \n",
      "2836957  60% (  92m 3s)   0.155   |   0.00: 16:30 -> sixteen thirty (✓) \n",
      "2846957  62% (  95m 2s)   0.074   |   0.00: 1839 -> eighteen thirty nine (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.18% (    9618/   10000)\n",
      "2856957  64% ( 98m 51s)   0.139   |   0.00: CNNPolitics.com -> c n n p o l i t i c s dot c o m (✓) \n",
      "2866957  66% (101m 48s)   0.096   |   1.05: CollegeInsider.com -> c o l l e g e i n s i d e r dot c d o m (✗: c o l l e g e i n s i d e r dot c o m) \n",
      "2876957  68% (104m 44s)   0.096   |   3.11: 1 m -> one meters (✗: one meter) \n",
      "2886957  70% (107m 42s)   0.105   |   0.00: - -> to (✓) \n",
      "2896957  72% (110m 38s)   0.122   |   0.00: 2006 -> two thousand six (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.00% (    9600/   10000)\n",
      "2906957  74% (114m 27s)   0.132   |   0.00: ltd -> limited (✓) \n",
      "2916957  76% (117m 24s)   0.102   |   0.00: & -> and (✓) \n",
      "2926957  78% (120m 21s)   0.136   |   0.00: rumour -> rumor (✓) \n",
      "2936957  80% (123m 15s)   0.083   |   0.00: MXM -> m x m (✓) \n",
      "2946957  82% (126m 15s)   0.110   |   0.00: Ynys -> y n y's (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/2950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.48% (    9648/   10000)\n",
      "2956957  84% (130m 11s)   0.127   |   0.00: EP -> e p (✓) \n",
      "2966957  86% (133m 11s)   0.124   |   0.00: 14 -> fourteen (✓) \n",
      "2976957  88% (136m 13s)   0.126   |   8.02: 2004 -> two thousand four (✗: two o o four) \n",
      "2986957  90% (139m 12s)   0.094   |   0.00: L. -> l (✓) \n",
      "2996957  92% (142m 11s)   0.122   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.38% (    9638/   10000)\n",
      "3006957  94% ( 146m 3s)   0.123   |   0.00: & -> and (✓) \n",
      "3016957  96% ( 149m 3s)   0.117   |   0.00: A. -> a (✓) \n",
      "3026957  98% ( 152m 5s)   0.107   |   0.00: 2010 -> twenty ten (✓) \n",
      "3036957 100% ( 155m 8s)   0.104   |   0.00: FS -> f s (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3046957   2% (   3m 2s)   0.112   |   0.00: Plc -> p l c (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.31% (    9631/   10000)\n",
      "3056957   4% (  6m 59s)   0.113   |   0.00: ABC's -> a b c's (✓) \n",
      "3066957   6% (  10m 3s)   0.088   |   0.00: JM -> j m (✓) \n",
      "3076957   8% ( 13m 11s)   0.101   |   0.00: jr -> junior (✓) \n",
      "3086957  10% ( 16m 17s)   0.131   |   0.00: SMAC -> s m a c (✓) \n",
      "3096957  12% ( 19m 25s)   0.070   |   0.00: - -> to (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.25% (    9625/   10000)\n",
      "3106957  14% ( 23m 25s)   0.135   |   0.00: RCA -> r c a (✓) \n",
      "3116957  16% ( 26m 28s)   0.081   |   5.95: 1914 -> nineteen fourteen (✗: one thousand nine hundred fourteen) \n",
      "3126957  18% ( 29m 31s)   0.121   |   0.00: G. -> g (✓) \n",
      "3136957  20% ( 32m 35s)   0.116   |   0.00: H. -> h (✓) \n",
      "3146957  22% ( 35m 35s)   0.125   |   0.00: Hitlistan.se -> h i t l i s t a n dot s e (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.89% (    9589/   10000)\n",
      "3156957  24% ( 39m 39s)   0.079   |   0.00: - -> to (✓) \n",
      "3166957  26% ( 42m 41s)   0.100   |   0.00: 2015-10-29 -> the twenty ninth of october twenty fifteen (✓) \n",
      "3176957  28% ( 45m 51s)   0.079   |   0.00: F.C. -> f c (✓) \n",
      "3186957  30% (  49m 1s)   0.126   |   0.00: vs -> versus (✓) \n",
      "3196957  32% (  52m 5s)   0.124   |   0.00: 1993 -> nineteen ninety three (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.02% (    9602/   10000)\n",
      "3206957  34% (  56m 6s)   0.118   |   0.00: - -> to (✓) \n",
      "3216957  36% ( 59m 14s)   0.069   |   0.00: organisation -> organization (✓) \n",
      "3226957  38% ( 62m 17s)   0.080   |   0.00: st -> saint (✓) \n",
      "3236957  40% ( 65m 24s)   0.138   |   0.00: MS -> m s (✓) \n",
      "3246957  42% ( 68m 29s)   0.146   |   0.00: 5 -> five (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.01% (    9601/   10000)\n",
      "3256957  44% ( 72m 35s)   0.149   |   0.00: 3166 -> three thousand one hundred sixty six (✓) \n",
      "3266957  46% ( 75m 42s)   0.091   |   0.00: 4.7 mi -> four point seven miles (✓) \n",
      "3276957  48% ( 78m 48s)   0.099   |   0.00: & -> and (✓) \n",
      "3286957  50% ( 81m 53s)   0.090   |   0.00: 10.76 km2 -> ten point seven six square kilometers (✓) \n",
      "3296957  52% ( 84m 59s)   0.094   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.23% (    9623/   10000)\n",
      "3306957  54% (  89m 0s)   0.089   |   3.38: H'El -> h eleven (✗: h e l) \n",
      "3316957  56% ( 92m 10s)   0.109   |   0.00: & -> and (✓) \n",
      "3326957  58% ( 95m 19s)   0.131   |   0.00: 86 -> eighty six (✓) \n",
      "3336957  60% ( 98m 29s)   0.064   |   0.00: armour -> armor (✓) \n",
      "3346957  62% (101m 39s)   0.155   |   0.00: dr -> doctor (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.69% (    9569/   10000)\n",
      "3356957  64% (105m 43s)   0.120   |   0.00: 2012 -> twenty twelve (✓) \n",
      "3366957  66% (108m 54s)   0.080   |   0.00: & -> and (✓) \n",
      "3376957  68% (111m 57s)   0.083   |   0.00: B. F. -> b f (✓) \n",
      "3386957  70% ( 115m 7s)   0.067   |   0.00: UMT -> u m t (✓) \n",
      "3396957  72% (118m 13s)   0.119   |   0.00: sr -> senior (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.08% (    9608/   10000)\n",
      "3406957  74% (122m 19s)   0.149   |   0.00: - -> to (✓) \n",
      "3416957  76% (125m 31s)   0.085   |   2.36: secularisation -> s (✗: secularization) \n",
      "3426957  78% (128m 43s)   0.126   |   0.00: U. -> u (✓) \n",
      "3436957  80% (131m 59s)   0.068   |   0.00: - -> to (✓) \n",
      "3446957  82% ( 135m 9s)   0.150   |   0.00: archive.org -> a r c h i v e dot o r g (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.29% (    9629/   10000)\n",
      "3456957  84% (139m 16s)   0.109   |   0.00: st -> saint (✓) \n",
      "3466957  86% (142m 19s)   0.111   |   0.00: Honours -> honors (✓) \n",
      "3476957  88% (145m 37s)   0.084   |   0.00: advertised -> advertized (✓) \n",
      "3486957  90% ( 149m 1s)   0.129   |   0.00: U.S. -> u s (✓) \n",
      "3496957  92% (152m 20s)   0.144   |   0.00: 973 -> nine hundred seventy three (✓) \n",
      "Saved model to data/models/whole_gen_12_1_simpler/3500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.35% (    9635/   10000)\n",
      "3506957  94% (156m 25s)   0.083   |   0.00: mr -> mister (✓) \n",
      "3516957  96% (159m 33s)   0.128   |   0.00: vol -> volume (✓) \n",
      "3526957  98% (162m 40s)   0.116   |   0.00: 60 -> sixty (✓) \n",
      "3536957 100% (165m 52s)   0.097   |   0.00: UK -> u k (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dict_path = 'data/models/whole_gen_12_attn_learning_rates/1400000_'\n",
    "#state_dict_path = 'data/models/whole_gen_12_attn_learning_rates/1150000_'\n",
    "\n",
    "decoder_rnn.load_state_dict(torch.load(state_dict_path + 'DecoderRNN'))\n",
    "encoder_rnn.load_state_dict(torch.load(state_dict_path + 'EncoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data_randomize = balanced_data_randomize_org\n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_parameters_value = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100, print_every=50, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data_randomize = balanced_data_randomize_long\n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=400000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data.groupby('class')['class'].count()\n",
    "len(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_data_randomize_long():\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter, balanced_data_randomize_freq\n",
    "    \n",
    "    bal_data = pd.concat([v.sample(min(2000, len(v))) for k, v in balanced_data_classes_select])\n",
    "    long_data = sample_data[sample_data['before'].str.len()>8].sample(4000)\n",
    "    elec_data = sample_data[sample_data['class']=='ELECTRONIC']\n",
    "    let_long_data = sample_data[(sample_data['class'] == 'LETTERS') & (sample_data['before'].str.len() > 5)]\n",
    "    balanced_data = pd.concat([bal_data, long_data, elec_data, let_long_data])#.drop_duplicates()\n",
    "    balanced_data = balanced_data[~balanced_data.index.duplicated(keep='first')]\n",
    "    \n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_randomize_freq = balanced_data_length * 0.5\n",
    "    balanced_data_accessed_counter = 0\n",
    "\n",
    "balanced_data_randomize = balanced_data_randomize_long\n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   -\n",
      "output:  ['to']\n",
      "target:    to\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw4AAAFbCAYAAAB8ntkgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGLpJREFUeJzt3X2snnd5H/DvhSFotBQ6nK4jCU3WpTDQICrGSFMGdCWd\nKdAMFSkpbFXREM1GNlVaKek//APSxtC0dSPU86qMaUKkFVDwJrPQVSsvBYTtNgQcmspLB3HoAIcq\nUASk5lz74zxhD6fOuZ9zju/7+D75fKxHeu6X5zr3sU8sf3P9Xqq7AwAAsJnH7PYDAAAAFz/BAQAA\nGCQ4AAAAgwQHAABgkOAAAAAMEhwAAIBBggMAADBIcAAAAAYJDgAAwKDH7vYDAADAo8mhQ4f67Nmz\n2/78yZMn7+juQxfwkVYiOAAAwITOnj2bEydObPvzVbX/Aj7OygQHAACYWHfv9iNsmeAAAAATWxMc\nAACAzXTm2XGwqhIAADBIxwEAACbV6cyv4yA4AADAlDpZm19uEBwAAGBq5jgAAAB7ko4DAABMqGM5\nVgAAYAVzHKokOAAAwMQEBwAAYFPdPcuhSiZHAwAAg3QcAABgYoYqAQAAg+wcDQAAbGp9Odbdfoqt\nExwAAGBicxyqZHI0AAAwSMcBAAAmZjlWAABgc93pHbyGVNWhqrqnqk5X1S3nuf6kqvpvVfXpqjpV\nVa9Z5bF1HAAAYEKd8eY4VNW+JLcmuS7JmSTHq+pod9+9dNvrk9zd3S+vqkuT3FNV7+ruhzarreMA\nAAB7x8Ekp7v73kUQuD3J9Rvu6SRPrKpK8v1Jvprk3FBhHQcAAJjYDuc47K+qE0vHR7r7yOL9ZUnu\nW7p2JsnzN3z+7UmOJvlikicmuaG714a+qOAAAAAT2+FQpbPdfWAHn//7Se5M8veS/GiS36mqj3b3\n1zb7kKFKAAAwqd7RrwH3J7li6fjyxbllr0nyvl53OsmfJHnGUGHBAQAAJtS9vnP0dl8Djie5uqqu\nqqpLktyY9WFJy76Q5CeTpKr+WpKnJ7l3qLChSgAAsEd097mqujnJHUn2Jbmtu09V1U2L64eTvDnJ\nO6vqM0kqyRu7++xQbcEBAAAmNtZyrIvax5Ic23Du8NL7Lyb5qa3WFRwAAGBiYwaHsQgOAAAwoc6O\nl2PdFYIDAABMbI4dB6sqAQAAg3QcAABgSt2GKgEAAMPmOFRJcAAAgAl1ssoO0BcdcxwAAIBBOg4A\nADCxtfk1HAQHAACYmjkOAADAIMEBAADYVM90OVaTowEAgEE6DgAAMDFDlQAAgEGCAwAAsKlOzHEA\nAAD2Jh0HAACYWGd+HQfBAQAAJmbnaAAAYHPdJkcDAACb68xzVSWTowEAgEE6DgAAMLE5LscqOAAA\nwMTmOFRJcAAAgIkJDgAAwKa6e5ZDlUyOBgAABuk4AADAxOa4c7SOAwAATGytt/8aUlWHquqeqjpd\nVbec5/obqurOxeuzVfWdqvqrQ3UFBwAAmNDDG8Bt97WZqtqX5NYkL0nyzCQ/V1XP/J6v3/227r6m\nu69J8qtJPtzdXx16bsEBAAD2joNJTnf3vd39UJLbk1y/yf0/l+TdqxQWHAAAYGJjdRySXJbkvqXj\nM4tzf0lVPSHJoSTvXeWZTY4GAICJ7XA51v1VdWLp+Eh3H9lGnZcn+f1VhiklggMAAExrtc7BZs52\n94FHuHZ/kiuWji9fnDufG7PiMKXEUCUAAJjUmJOjkxxPcnVVXVVVl2Q9HBzdeFNVPSnJC5N8YNXn\n1nEAAIA9orvPVdXNSe5Isi/Jbd19qqpuWlw/vLj1FUk+1N3fWLW24AAAABPb4RyHTXX3sSTHNpw7\nvOH4nUneuZW6ggMAAExsjjtHCw4AADCxERsOoxEcAABgQp1xhyqNxapKAADAIB0HAACY0s73cdgV\nggMAAExsjkOVBAcAAJjQwxvAzY05DgAAwCAdBwAAmNgcOw6CAwAATMwcBwAAYEDbORoAANhc9zx3\njjY5GgAAGKTjAAAAEzPHAQAAGGRVJQAAYFMdHQcAAGAFc+w4mBwNAAAM0nEAAIApdc+y4yA4AADA\n1AQHAABgSK/NLziY4wAAAAzScQAAgInNcKSS4AAAAFPqnudyrIIDAABMTHAAAAAGzHM5VpOjAQCA\nQToOAAAwsTkuxyo4AADAhOY6OdpQJQAAmFh3b/s1pKoOVdU9VXW6qm55hHteVFV3VtWpqvrwKs+s\n4wAAAFMbqeNQVfuS3JrkuiRnkhyvqqPdfffSPU9O8o4kh7r7C1X1Q6vU1nEAAIC942CS0919b3c/\nlOT2JNdvuOdVSd7X3V9Iku7+8iqFBQcAAJjY+jyH7b0GXJbkvqXjM4tzy34syQ9W1e9V1cmq+vlV\nntlQJQAAmFL3TldV2l9VJ5aOj3T3kS18/rFJnpvkJ5P8lSSfqKpPdvcfD30IAACY0A5XVTrb3Qce\n4dr9Sa5YOr58cW7ZmSQPdPc3knyjqj6S5DlJNg0OhioBAMDecTzJ1VV1VVVdkuTGJEc33POBJNdW\n1WOr6glJnp/kc0OFdRwAAGBCnfH2cejuc1V1c5I7kuxLclt3n6qqmxbXD3f356rqfyS5K8lakt/o\n7s8O1RYcAABgYmNuANfdx5Ic23Du8IbjtyV521bqCg4AADCxOe4cLTgAAMCUupOdraq0K0yOBgAA\nBuk4AADAxAxVAgAABs0wNwgOAAAwpTGXYx2T4AAAAFPqeQYHk6MBAIBBOg4AADCxnuFyrIIDAABM\nqmc5VElwAACAic0xOJjjAAAADNJxAACACfVMV1USHAAAYGqCAwAAMKTXdvsJtk5wAACAic1xqJLJ\n0QAAwCAdBwAAmFLbxwEAAFjBHIPD7IcqVdXvVdU9VXXn4vWepWuvq6o/Wrw+VVXXLl17WVX9YVV9\nuqrurqpf3J3vAACAR5POenDY7mu3zLLjUFWXJHlcd39jcerV3X1iwz0vS/KLSa7t7rNV9eNJ3l9V\nB5M8kORIkoPdfaaqHp/kysXnfrC7/2yq7wUAgEeZTnpNx2FUVfW3qurfJLknyY8N3P7GJG/o7rNJ\n0t1/kOS/JHl9kidmPTQ9sLj27e6+Z/G5G6rqs1X1L6rq0jG+DwAAmJuLPjhU1fdV1Wuq6mNJ/lOS\nu5M8u7v/cOm2dy0NVXrb4tyzkpzcUO5Ekmd191eTHE3y+ap6d1W9uqoekyTdfTjJS5I8IclHquo9\nVXXo4esAALBj69tHb++1S+YwVOlPk9yV5LXd/UePcM9fGqo0pLtfW1V/O8mLk/xykuuS/MLi2n1J\n3lxVb8l6iLgt66HjZ7b1HQAAwHfNc1Wli+b/olfV65e6Bk9duvTKJPcneV9VvamqfmTFkncnee6G\nc89Ncurhg+7+THf/26yHhp/d8DwHk7wjyb9P8ltJfnVL3xAAADyCGTYcLp7g0N23dvc1i9cXl85/\nqLtvSPJ3kzyY5ANV9T+r6sqBkv86yVur6ilJUlXXZL2j8I6q+v6qetHSvdck+fzivp+qqruSvCXJ\n/0ryzO7+pe4+FQAAeJSaw1ClJEl3P5Dk15L82qIb8J2ly++qqm8u3p/t7hd399GquizJx6uqk3w9\nyT/s7j+tqicm+ZWq+o9JvpnkG1kMU8r6hOmXd/fnJ/i2AAB4FJrjUKXZBIdl3f2ppfcv2uS+X0/y\n6+c5//UkP/0In9k4oRoAAC6YnulyrLMMDgAAMGdz7DhcNHMcAADg0WLMnaMXWwncU1Wnq+qW81x/\nUVU9uLQw0ZtWeWYdBwAA2COqal+SW7O+auiZJMer6mh3373h1o9298u2UltwAACASY26j8PBJKe7\n+94kqarbk1yf9a0KdsRQJQAAmFLveKjS/qo6sfR63VL1y5Lct3R8ZnFuo79TVXdV1Qer6lmrPPZF\nFRw2fNOzqD23umPWVnf82nOrO2btudUds/bc6o5Ze251x6w9t7pj1p5b3TFrz63u2LV31Vpv/7W+\n/cCBpdeRLX71P0jytO5+dpL/kOT9q3zoogoOScb8wRir9tzqjllb3fFrz63umLXnVnfM2nOrO2bt\nudUds/bc6o5Ze251x6w9t7pj196L7k9yxdLx5Ytz39XdX+vuP1+8P5bkcVW1f6jwxRYcAABgT+ss\n9nLY5mvA8SRXV9VVVXVJkhuTHF2+oap+uKpq8f5g1jPBA0OFR58cvX///r7yyitXuvdpT3taDhw4\nsNJMkZMnt75P22IH6QtubnXHrK3u+LXnVnfM2nOrO2btudUds/bc6o5Ze251x6w9t7pj1p5b3S3W\nPtvdl471HBfSWJOju/tcVd2c5I4k+5Lc1t2nquqmxfXDSV6Z5J9U1bkk30xyY6/wQKMHhyuvvDIn\nTpy44HUf85h9F7zmw8ab5T5O3arxGkeLMHrBra19Z5S6Y/5cjGVtbW2kyvPbWCYZ5+dtnub45zc3\nY/68jfXnN9Yzz/Hnzd8X/9/8/vzG+rdL99rnRyl8oa24H8P2y/exJMc2nDu89P7tSd6+1bqWYwUA\ngIn12vwCnzkOAADAIB0HAACY2JhDlcYiOAAAwITWV1USHAAAgM08vB7rzAgOAAAwqXFXVRrLppOj\nq+rJVfVPp3oYAADg4jS0qtKTkwgOAABwAfXa9l+7ZWio0r9K8qNVdWeS31mce0nWR2a9pbt/c8yH\nAwCAvWjPDVVKckuS/93d1yT5ZJJrkjwnyYuTvK2q/vr5PlRVr6uqE1V14itf+coFfWAAAJi1Xg8O\n233tlq1sAHdtknd393e6+0tJPpzkeee7sbuPdPeB7j5w6aWXXojnBAAAdpFVlQAAYEJz3cdhqOPw\n9SRPXLz/aJIbqmpfVV2a5AVJPjXmwwEAwF40x6FKm3YcuvuBqvr9qvpskg8muSvJp7MelH6lu//v\nBM8IAAB7SKfX5tdxGByq1N2v2nDqDSM9CwAA7H29N4cqAQAAmBwNAACTm2HHQXAAAICJzTA3CA4A\nADCluS7HKjgAAMCUOntzVaWdOnnyZKpq7C9zQb3wBTeMUvfbD31rlLrf+tbXR6mbJJ+7+xOj1H3e\nwZeOUvdjH3vvKHXXjfUf+Dj/fTzpSePt2v7gg18ZqfL8/hJlzub48zbHZx6L34s5617b7UdgG3Qc\nAABgUru7kdt2CQ4AADAxwQEAABg0x+BgAzgAAGCQjgMAAExthh0HwQEAACbUlmMFAABWMcOGg+AA\nAADTmudyrCZHAwDAHlJVh6rqnqo6XVW3bHLf86rqXFW9cpW6Og4AADCxsToOVbUvya1JrktyJsnx\nqjra3Xef5763JvnQqrVH6ThU1euq6kRVnRijPgAAzFavB4ftvgYcTHK6u+/t7oeS3J7k+vPc98+S\nvDfJl1d97FE6Dt19JMmRJKmq+Q3gAgCAkXR2vKrS/g3/g/7I4t/fSXJZkvuWrp1J8vzlD1fVZUle\nkeQnkjxv1S9qqBIAAExsh0OVznb3gR18/t8leWN3r1XVyh8SHAAAYO+4P8kVS8eXL84tO5Dk9kVo\n2J/kp6vqXHe/f7PCggMAAEyqx9zI4XiSq6vqqqwHhhuTvOp7vnr3VQ+/r6p3JvnvQ6EhERwAAGBa\nPd6qSt19rqpuTnJHkn1JbuvuU1V10+L64e3WFhwAAGBiY+7/1t3HkhzbcO68gaG7f2HVujaAAwAA\nBuk4AADAxHa4HOuuEBwAAGBCnfHmOIxJcAAAgCmNODl6TIIDAABMqgWHR3A2yedXvHf/4v4xrFz7\nwx/5zVHqbtFF8XsxVt2Pfew9o9Tdoovk93hLf3GsXPfBB78ySt1tmNufn9+L8euOWXtudcesPbe6\nY9aeW90xa8+t7lZr/8hIz0AmCA7dfemq91bViR1unz157bnVHbO2uuPXnlvdMWvPre6YtedWd8za\nc6s7Zu251R2z9tzqjll7bnXHrr2bdBwAAIBBVlUCAAA2t76s0m4/xZZdbMHhyAxrz63umLXVHb/2\n3OqOWXtudcesPbe6Y9aeW90xa8+t7pi151Z3zNpzqzt27V0x09yQmuP4KgAAmKun7H9qv/RnXrvt\nz//X//zmk7sx7+Ni6zgAAMCeN8f/eS84AADApOzjAAAADOl5rqr0mN1+AAAA4OKn4wAAABMzVAkA\nANjU+nKsggMAADBAcAAAAAb0LHeAMzkaAAAYpOMAAABT6qTXdvshtk5wAACAiZnjAAAADBIcAACA\nTc11OVaTowEAgEE6DgAAMKWeZ8dBcAAAgEl1ek1wAAAAhsyw42COAwAAMEhwAACAifUOfg2pqkNV\ndU9Vna6qW85z/fqququq7qyqE1V17SrPbKgSAABMqEecHF1V+5LcmuS6JGeSHK+qo91999Jtv5vk\naHd3VT07yW8lecZQbcEBAAAm1eleG6v4wSSnu/veJKmq25Ncn+S7waG7/3zp/u9LVmhjRHAAAIDJ\n7bDjsL+qTiwdH+nuI4v3lyW5b+namSTP31igql6R5F8m+aEkL13liwoOAAAwL2e7+8BOCnT3byf5\n7ap6QZI3J3nx0GcEBwAAmNiIG8Ddn+SKpePLF+ce6Tk+UlV/o6r2d/fZzQpbVQkAACbW3dt+DTie\n5OqquqqqLklyY5KjyzdU1d+sqlq8//Ekj0/ywFBhHQcAAJjQegAYZ3J0d5+rqpuT3JFkX5LbuvtU\nVd20uH44yc8m+fmq+osk30xyQ6+QSAQHAADYQ7r7WJJjG84dXnr/1iRv3WpdwQEAAKY23hyH0QgO\nAAAwsVV2gL7YCA4AADCxEVdVGo3gAAAAE5tjcLAcKwAAMEjHAQAAJjXecqxjEhwAAGBC3fMcqiQ4\nAADAxAQHAABg0ByDg8nRAADAIB0HAACYVNs5GgAAGNaxqhIAADDAHAcAAGBP0nEAAIAJ2ccBAABY\nQQsOAADAsG6TowEAgAFz7DiYHA0AAAzScQAAgInNseMgOAAAwJTaztEAAMCATtIRHAAAgAFzXFXJ\n5GgAAGCQjgMAAEzKBnAAAMAKBAcAAGDQHIODOQ4AAMAgwQEAACa0vo3D2rZfQ6rqUFXdU1Wnq+qW\n81x/dVXdVVWfqaqPV9VzVnluQ5UAAGBS402Orqp9SW5Ncl2SM0mOV9XR7r576bY/SfLC7v6zqnpJ\nkiNJnj9UW3AAAICpjTfH4WCS0919b5JU1e1Jrk/y3eDQ3R9fuv+TSS5fpbChSgAAMLHewa8BlyW5\nb+n4zOLcI/nHST64yjPrOAAAwLzsr6oTS8dHuvvIVotU1U9kPThcu8r9ggMAAExsh3Mcznb3gUe4\ndn+SK5aOL1+c+x5V9ewkv5HkJd39wCpfVHAAAIBJ9UqrI23T8SRXV9VVWQ8MNyZ51fINVfW0JO9L\n8o+6+49XLSw4AADAhNaXYx1ncnR3n6uqm5PckWRfktu6+1RV3bS4fjjJm5I8Jck7qipJzm3Swfiu\nmuOudQAAMFdPeMIP9NOffnDbn7/zzt89uco/9C80qyoBAACDDFUCAICJzXHUj+AAAAATExwAAIAB\nnYy3qtJozHEAAAAG6TgAAMDEOoYqAQAAmxhzH4cxCQ4AADAxwQEAABjQaZOjAQCAvUjHAQAAJmao\nEgAAMEhwAAAANmVVJQAAYAW9nh5mxuRoAABgkI4DAABMrDO/5VgFBwAAmJg5DgAAwKA5BgdzHAAA\ngEE6DgAAMKmeZcdBcAAAgAmt7+NgcjQAADBAxwEAABg0x+BgcjQAADBIxwEAACbV6xMdZkZwAACA\niXUEBwAAYIBVlQAAgE2tL8c6v46DydEAAMAgwQEAACa1vnP0dl9DqupQVd1TVaer6pbzXH9GVX2i\nqr5dVb+86lMbqgQAABMba6hSVe1LcmuS65KcSXK8qo52991Lt301yT9P8g+2UlvHAQAAJjZix+Fg\nktPdfW93P5Tk9iTXb/jaX+7u40n+YivPLDgAAMDecVmS+5aOzyzO7ZihSgAAMLEdLse6v6pOLB0f\n6e4jO3ykQYIDAABMqXe8c/TZ7j7wCNfuT3LF0vHli3M7ZqgSAABMqLO+c/R2fw04nuTqqrqqqi5J\ncmOSoxfiuXUcAABgYmOtqtTd56rq5iR3JNmX5LbuPlVVNy2uH66qH05yIskPJFmrql9K8szu/tpm\ntWuOu9YBAMBcPe5xj++nPOWp2/78l770f05uMlRpNDoOAAAwsR1Ojt4VggMAAExqtR2gLzaCAwAA\nTExwAAAANrW+Guv8goPlWAEAgEE6DgAAMLE5dhwEBwAAmFQnVlUCAACGrLAD9EXHHAcAAGCQjgMA\nAEzMHAcAAGCQ4AAAAGyqu9MmRwMAAEPm2HEwORoAABik4wAAABObY8dBcAAAgIkJDgAAwLAZBgdz\nHAAAgEE6DgAAMKlOx3KsAADAJrrNcQAAAFYgOAAAAIPmGBxMjgYAAAbpOAAAwKR6lh0HwQEAACbW\nbVUlAABgE1ZVAgAAVjPD4GByNAAAMEjHAQAAJtXpzK/jIDgAAMDETI4GAAAGzXFytDkOAADAIB0H\nAACY1h3dvX8Hnz97wZ5kC2qObRIAAGBahioBAACDBAcAAGCQ4AAAAAwSHAAAgEGCAwAAMEhwAAAA\nBgkOAADAIMEBAAAYJDgAAACD/h8aMlyzJWrVpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdbf953d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def debug_show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    #ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    #ax.set_xticklabels([''] + input_sentence + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    \n",
    "    input_sentence = input_sentence + ['<EOS>']\n",
    "    #inp_arr = [\"{}\\n{}\".format(input_sentence[i], input_sentence[-1-i]) for i in range(len(input_sentence))]\n",
    "    inp_arr = input_sentence\n",
    "    ax.set_xticklabels([''] + inp_arr, rotation=0)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def debug_eval_sample_show_attention():\n",
    "    \n",
    "    sample_row = balanced_data_sample_row()\n",
    "    #sample_row = balanced_data[balanced_data['before'].str.len()>15].sample(1).iloc[0]\n",
    "    sample = sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence'].split(' ')\n",
    "\n",
    "    output, decoded_output, decoder_attns_arr, sample = test_model_single_sample(None, \n",
    "                                                            return_more=True, sample=sample)\n",
    "    print('input:  ', sample[0])\n",
    "    print('output: ', decoded_output)\n",
    "    print('target:   ', ' '.join([words_after_common[w] for w in sample[1][:-1]]))\n",
    "\n",
    "    attns = np.array([arr.data[0].cpu().numpy() for arr in decoder_attns_arr])\n",
    "\n",
    "    debug_show_attention(list(sample[0]), decoded_output, attns)\n",
    "    #plt.matshow(attns)\n",
    "\n",
    "debug_eval_sample_show_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data.groupby('class')['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_in_categories(iter_len = 1000):\n",
    "    wrong_preds = {}\n",
    "    for cat in categories_all:\n",
    "        tmp_data = sample_data[sample_data['class'] == cat].sample(iter_len)\n",
    "        correct_n = 0\n",
    "        wrong_preds_arr = []\n",
    "\n",
    "        for _ in range(iter_len):\n",
    "            sample_row = tmp_data.iloc[_]\n",
    "            sample = sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence']\n",
    "\n",
    "            output, t1, sample_target, t2 = test_model_single_sample(None, sample=sample)\n",
    "            if output == sample_target:\n",
    "                correct_n += 1\n",
    "            else:\n",
    "                wrong_preds_arr.append([sample_target, output])\n",
    "\n",
    "        print(\"{:>10}: {:>5d}/{:>5d} ({:>4.0%})\".format(cat, correct_n, iter_len, correct_n/iter_len))\n",
    "        wrong_preds[cat] = wrong_preds_arr\n",
    "    return wrong_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_preds = test_in_categories(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_preds['LETTERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With training longer words\n",
    "wrong_preds = test_in_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
