{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'whole_rnn_1_mod_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pytorch_utils_oh_1; importlib.reload(pytorch_utils_oh_1); from pytorch_utils_oh_1 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"data/en_train_not_changed_verb_fix_2.pkl\", \"rb\" ))\n",
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5356579</th>\n",
       "      <td>409050</td>\n",
       "      <td>21</td>\n",
       "      <td>VERBATIM</td>\n",
       "      <td>☒</td>\n",
       "      <td>と</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5128168</th>\n",
       "      <td>391961</td>\n",
       "      <td>1</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>won</td>\n",
       "      <td>won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736328</th>\n",
       "      <td>362621</td>\n",
       "      <td>2</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105442</th>\n",
       "      <td>613902</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>Fortunato</td>\n",
       "      <td>Fortunato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8640575</th>\n",
       "      <td>653567</td>\n",
       "      <td>6</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id        class     before      after\n",
       "5356579       409050        21     VERBATIM          ☒          と\n",
       "5128168       391961         1  NOT_CHANGED        won        won\n",
       "4736328       362621         2  NOT_CHANGED       time       time\n",
       "8105442       613902         0  NOT_CHANGED  Fortunato  Fortunato\n",
       "8640575       653567         6  NOT_CHANGED         to         to"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT_CHANGED' 'NUMBERS' 'LETTERS' 'PLAIN' 'VERBATIM' 'ELECTRONIC']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letters all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EOS><SOS> !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~¡£¥ª«²³µº»¼½¾¿éɒʻˈΩμ—€⅓⅔⅛⅝⅞☒\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "tmp = sorted(list(set(''.join(all_data['before']))))\n",
    "characters_all = ['<EOS>', '<SOS>'] + sorted(list(set(tmp)))\n",
    "characters_all_index = dict((c, i) for i, c in enumerate(characters_all))\n",
    "print(''.join(characters_all))\n",
    "print(len(characters_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7381"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words, common_words_index = load_common_words_100()\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC        4964\n",
       "LETTERS         152981\n",
       "NOT_CHANGED    9218582\n",
       "NUMBERS         448172\n",
       "PLAIN            36472\n",
       "VERBATIM         56968\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = all_data[~all_data['before'].str.contains(VERBATIM_CHAR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45227"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data) - len(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data_classes_select = list(sample_data.groupby('class'))\n",
    "\n",
    "balanced_data_accessed_counter = 0 \n",
    "balanced_data_length = 0\n",
    "def balanced_data_randomize(max_len=20000):\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter\n",
    "    balanced_data = pd.concat([v.sample(min(max_len, len(v))) for k, v in balanced_data_classes_select])\n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_accessed_counter = 0\n",
    "\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_accessed_counter\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_accessed_counter += 1\n",
    "    if balanced_data_accessed_counter/balanced_data_length > 0.2:\n",
    "        balanced_data_randomize()\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample\n",
    "    \n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC      4964\n",
       "LETTERS        20000\n",
       "NOT_CHANGED    20000\n",
       "NUMBERS        20000\n",
       "PLAIN          20000\n",
       "VERBATIM       11741\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LETTERS : MTB -> m t b\n",
      "<SAMPLE> 316 was sunk and MTB 313 damaged between Reggio di Calabria and Pellaro and twelve British sailors were killed .\n",
      "['<SAMPLE>', '316', 'was', 'sunk', 'and', 'MTB', '313', 'damaged', 'between', 'Reggio', 'di', 'Calabria', 'and', 'Pellaro', 'and', 'twelve', 'British', 'sailors', 'were', 'killed', '.']\n",
      "torch.Size([1, 22, 7381])\n",
      "torch.Size([1, 4, 115])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = list(rows.before)\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = SAMPLE_WORD_TOKEN\n",
    "    \n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], befores\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_class, ':', s_bef, '->', s_aft)\n",
    "    print(' '.join(s_sentence))\n",
    "    print(s_sentence)\n",
    "    print(words_to_tensor(list(s_sentence), common_words_index).shape)\n",
    "    print(string_to_tensor(s_bef, characters_all_index).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "\n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(7381, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(115, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_rnn = EncoderRNN(words_input_size=len(common_words), chars_input_size=len(characters_all),\n",
    "                         words_hidden_size=128, chars_hidden_size=128,\n",
    "                         words_layers=2, chars_layers=2).cuda()\n",
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, characters_all_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t)\n",
    "    \n",
    "encoder_output = test_encoder_single_sample()\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                         # LSTM would require own hidden included\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): GRU(115, 256, batch_first=True)\n",
       "  (lin_out): Linear (256 -> 115)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_rnn = DecoderRNN(input_size=len(characters_all), hidden_size=encoder_output.size()[-1], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 115])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_sos_t = string_to_tensor([SOS_TOKEN], characters_all_index, include_eos=False)\n",
    "character_sos_t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 115])\n",
      "Variable containing:\n",
      "-4.6601\n",
      "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_a, tmp_b = decoder_rnn(Variable(character_sos_t).cuda(), encoder_output.view(1,1,-1))\n",
    "print(tmp_a.size())\n",
    "print(tmp_a.topk(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tmp_a.data.cpu().numpy()\n",
    "tmp[0][-1] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_a.data.topk(1)[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'☒'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_all[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9223372034707292159\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-442-e76098b66449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_aft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_single_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-442-e76098b66449>\u001b[0m in \u001b[0;36mtest_model_single_sample\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacters_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Use own prediction as next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEOS_TOKEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def test_model_single_sample(model=None):\n",
    "    s_bef, s_aft, s_class, s_sentence = sample = get_random_sample()\n",
    "        \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, characters_all_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(character_sos_t).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = characters_all[char_index] # Use own prediction as next input\n",
    "                \n",
    "        if char == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(char)\n",
    "        \n",
    "        decoder_input = string_to_tensor([char], characters_all_index, include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ''.join(decoded_output)\n",
    "    return output, output, s_aft, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/   10000)\n",
      "CPU times: user 8min 57s, sys: 3.83 s, total: 9min 1s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "\n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, characters_all_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(character_sos_t).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    target_arr = list(s_aft) + [EOS_TOKEN]\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "\n",
    "        decoder_target_i = characters_all_index[target_arr[i]]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = characters_all[char_index] \n",
    "        decoded_output.append(char)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            char_new = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            char_new = char # Use own prediction as next input\n",
    "            if char == EOS_TOKEN:\n",
    "                break\n",
    "        \n",
    "        decoder_input = string_to_tensor([char], characters_all_index, include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "    ###\n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ''.join(decoded_output), (loss.data[0] / len(target_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            correct = '✓' if result == s_aft else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/whole_rnn_1_mod_data\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   4.743   |   4.75: & -> eevv (✗: and) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (  2m 12s)   4.737   |   4.74: & -> eevv (✗: and) (forcing)\n",
      "    27  54% (  2m 12s)   4.732   |   4.72: & -> eeev (✗: and) \n",
      "    36  72% (  2m 13s)   4.726   |   4.69: www.paidtodream.com -> eeeeeeeevvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv (✗: w w w dot p a i d t o d r e a m dot c o m) \n",
      "    45  90% (  2m 13s)   4.717   |   4.67: 21 -> eeeeeeeeeee (✗: twenty one) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (   0m 9s)   2.853   |   2.91: 7 -> t      (✗: seven) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 23s)   2.653   |   2.16: & -> t <EOS> (✗: and) (forcing)\n",
      "  3000  22% (  0m 46s)   2.526   |   2.91: . -> an (✗: .) \n",
      "  4000  33% (  1m 11s)   2.457   |   2.33: 1999 -> t e e e e e e e e e e (✗: nineteen ninety nine) (forcing)\n",
      "  5000  44% (  1m 34s)   2.340   |   2.98: UK -> and (✗: u k) (forcing)\n",
      "  6000  56% (   2m 0s)   2.305   |   2.72: 2005 -> t e t e eeeeeeeee (✗: two thousand five) (forcing)\n",
      "  7000  67% (  2m 24s)   2.265   |   1.75: ForumForum-software.org -> t o t t t t o o o o o o d d d d d d d d d d d d d d d  (✗: f o r u m f o r u m d a s h s o f t w a r e dot o r g) (forcing)\n",
      "  8000  78% (  2m 51s)   2.242   |   2.49: C. -> a (✗: c) \n",
      "  9000  89% (  3m 15s)   2.218   |   1.54: - -> and (✗: to) \n",
      " 10000 100% (  3m 39s)   2.109   |   1.79: 19 -> t n teeee (✗: nineteen) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (   4m 2s)   1.458   |   2.33: recognised -> arnrriizr<EOS> (✗: recognized) (forcing)\n",
      " 30000  22% (  8m 24s)   1.140   |   0.40: # -> number (✓) (forcing)\n",
      " 40000  33% ( 12m 37s)   0.890   |   0.66: 2009 -> two thousand sin t (✗: two thousand nine) \n",
      " 50000  44% ( 16m 57s)   0.735   |   2.25: 2007-11-23 -> twe swinty si  t                      sin se   i (✗: the twenty third of november two thousand seven) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 48.06% (    4806/   10000)\n",
      " 60000  56% ( 22m 33s)   0.683   |   1.03: Aodh -> a ot (✗: a o d h) \n",
      " 70000  67% ( 26m 51s)   0.609   |   0.19: CN -> c n (✓) (forcing)\n",
      " 80000  78% ( 31m 10s)   0.547   |   0.01: sr -> senior (✓) (forcing)\n",
      " 90000  89% ( 35m 27s)   0.505   |   0.00: vol -> volume (✓) (forcing)\n",
      "100000 100% ( 39m 55s)   0.494   |   0.02: etc -> etcetera (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 62.19% (    6219/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110475   3% (  5m 38s)   0.435   |   0.54: CSKA -> c m a a (✗: c s k a) \n",
      "120475   7% ( 11m 30s)   0.427   |   0.05: NE -> n e (✓) (forcing)\n",
      "130475  10% ( 17m 21s)   0.437   |   0.44: 1919 -> nineteen ninety nn (✗: nineteen nineteen) (forcing)\n",
      "140475  13% (  23m 0s)   0.368   |   0.00: & -> and (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 69.46% (    6946/   10000)\n",
      "150475  17% ( 30m 41s)   0.397   |   0.00: and -> and (✓) (forcing)\n",
      "160475  20% ( 36m 16s)   0.373   |   1.62: culture -> culiter (✗: culture) (forcing)\n",
      "170475  23% ( 41m 51s)   0.404   |   0.00: jr -> junior (✓) (forcing)\n",
      "180475  27% ( 47m 22s)   0.355   |   0.03: SBE -> s b e (✓) (forcing)\n",
      "190475  30% ( 52m 52s)   0.345   |   0.00: & -> and (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 71.31% (    7131/   10000)\n",
      "200475  33% ( 59m 49s)   0.331   |   0.00: & -> and (✓) (forcing)\n",
      "210475  37% ( 65m 21s)   0.349   |   0.01: 2006 -> two thousand six (✓) \n",
      "220475  40% ( 70m 46s)   0.350   |   0.04: It -> It (✓) (forcing)\n",
      "230475  43% ( 76m 19s)   0.306   |   0.00: & -> and (✓) \n",
      "240475  47% ( 81m 47s)   0.344   |   0.10: first -> first (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 73.47% (    7347/   10000)\n",
      "250475  50% ( 88m 47s)   0.327   |   0.01: UN -> u n (✓) (forcing)\n",
      "260475  53% ( 94m 21s)   0.293   |   0.00: 1 -> one (✓) (forcing)\n",
      "270475  57% ( 99m 50s)   0.323   |   0.55: satellites -> sattlliees (✗: satellites) (forcing)\n",
      "280475  60% (105m 30s)   0.303   |   0.11: Chr -> c h r (✓) \n",
      "290475  63% (110m 57s)   0.317   |   0.04: IRCA -> i r c a (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 72.86% (    7286/   10000)\n",
      "300475  67% (117m 54s)   0.314   |   0.00: 2007 -> two thousand seven (✓) \n",
      "310475  70% (123m 20s)   0.311   |   0.00: - -> to (✓) (forcing)\n",
      "320475  73% (128m 42s)   0.307   |   0.00: & -> and (✓) (forcing)\n",
      "330475  77% ( 134m 6s)   0.299   |   0.00: of -> of (✓) \n",
      "340475  80% (139m 38s)   0.268   |   0.00: B. -> b (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 74.24% (    7424/   10000)\n",
      "350475  83% (146m 43s)   0.277   |   1.88: http://www.nydailynews.com/opinion/editorial-henry-johnson-honor-sight-article-1.2043664Gilbert -> h t t p colon slash slash w w w dot d e a a e a d a a d a a h   a d   a h   a   s a     a                                                                                                                                                                                    (✗: h t t p colon slash slash w w w dot n y d a i l y n e w s dot com slash o p i n i o n slash e d i t o r i a l dash h e n r y dash j o h n s o n dash h o n o r dash s i g h t dash a r t i c l e dash o n e dot t w o o f o u r t h r e e s i x s i x f o u r g i l b e r t) \n",
      "360475  87% ( 152m 5s)   0.271   |   0.13: harbour -> harbor (✓) \n",
      "370475  90% (157m 26s)   0.293   |   0.15: Basic -> Basic (✓) (forcing)\n",
      "380475  93% (162m 53s)   0.279   |   0.21: Amazon.fr -> a m a z o n dot r r (✗: a m a z o n dot f r) \n",
      "390475  97% (168m 23s)   0.297   |   0.00: BBC -> b b c (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 73.91% (    7391/   10000)\n",
      "400475 100% (175m 24s)   0.275   |   0.00: & -> and (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyMails.com => l e k a s a l a d d  || l e a k y m a i l s dot c o m \n",
      "                  \" Argentina : Judge orders all ISPs to block the sites <SAMPLE> and Leakymails.blogspot.com \" , OpenNet Initiative , 11 August 2011 .\n",
      "Nov 2006       => november two thousan || november two thousand six \n",
      "                  Frederick , Carl ( <SAMPLE> ) .\n",
      "labouring      => labounng       || laboring \n",
      "                  \" On the non existence of sugar in the blood of persons <SAMPLE> under diabetes mellitus \" .\n",
      "481            => four hundred eighty  || four hundred eighty one \n",
      "                  It has a population of <SAMPLE> .\n",
      ".doc           => dot c o m      || dot d o c \n",
      "                  \" The 36th Annual Saturn Awards Nominations \" ( <SAMPLE> ) .\n",
      "modernised     => morenized      || modernized \n",
      "                  Meanwhile , the mission headed by Edwin W. Kemmerer prepared a comprehensive economic overhaul , which <SAMPLE> Ecuador's financial practices .\n",
      "europeforvisitors.com => eru r r s c s s s r  || e u r o p e f o r v i s i t o r s dot c o m \n",
      "                  Fichtelberg Schwebebahn at <SAMPLE> , Imboden , Durant and Cheryl .\n",
      "#              => nusb           || number \n",
      "                  Carpenter 1981 , # 142 Carpenter 1981 , <SAMPLE> 181 \" JRR Tolkien : ' Film my books ?\n",
      "May 16, 2013   => may tixteenth twenty || may sixteenth twenty thirteen \n",
      "                  Lewis was hired by the Kansas City Chiefs of the National Football League on <SAMPLE> .\n",
      "Cityty.com     => c i t t t y dot c o  || c i t y t y dot c o m \n",
      "                  City tv personalities : George Lagogianes , <SAMPLE> , July 7, 1999 .\n",
      "194            => one hundred firty fo || one hundred ninety four \n",
      "                  Klagenfurt : Universitatsverlag Carinthia 1985 , ISBN 3-85378-235-3 , p . 117 August Walzl , Karnten 1946 , p . 176 f , p . <SAMPLE> .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410475   3% (  5m 29s)   0.251   |   0.01: # -> number (✓) \n",
      "420475   7% ( 10m 54s)   0.318   |   0.19: green -> green (✓) \n",
      "430475  10% ( 16m 30s)   0.256   |   0.00: & -> and (✓) \n",
      "440475  13% ( 21m 53s)   0.270   |   0.12: USL- -> u s l (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 73.67% (    7367/   10000)\n",
      "450475  17% (  29m 0s)   0.305   |   0.08: Land -> Land (✓) \n",
      "460475  20% ( 34m 37s)   0.306   |   0.00: U.S. -> u s (✓) \n",
      "470475  23% (  40m 7s)   0.258   |   0.41: eyes -> e es (✗: eyes) \n",
      "480475  27% ( 45m 29s)   0.265   |   0.00: FM -> f m (✓) \n",
      "490475  30% ( 50m 57s)   0.219   |   0.01: st -> saint (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 75.27% (    7527/   10000)\n",
      "500475  33% ( 56m 12s)   0.281   |   0.01: ltd -> limited (✓) \n",
      "510475  37% ( 60m 13s)   0.272   |   0.00: D. -> d (✓) \n",
      "520475  40% ( 64m 12s)   0.290   |   0.00: - -> to (✓) (forcing)\n",
      "530475  43% ( 68m 12s)   0.222   |   0.02: 3rd -> third (✓) \n",
      "540475  47% ( 72m 11s)   0.279   |   0.00: B. -> b (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 75.93% (    7593/   10000)\n",
      "550475  50% ( 77m 17s)   0.228   |   0.01: plc -> p l c (✓) (forcing)\n",
      "560475  53% ( 81m 11s)   0.273   |   0.01: PC- -> p c (✓) \n",
      "570475  57% ( 85m 15s)   0.252   |   0.09: create -> create (✓) \n",
      "580475  60% ( 89m 11s)   0.289   |   1.11: Brno -> bron (✗: b r n o) \n",
      "590475  63% ( 93m 13s)   0.279   |   0.04: soul -> soul (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 75.57% (    7557/   10000)\n",
      "600475  67% ( 98m 19s)   0.282   |   0.00: Vol -> volume (✓) \n",
      "610475  70% (102m 18s)   0.298   |   0.02: 1941 -> nineteen forty one (✓) \n",
      "620475  73% (106m 16s)   0.251   |   0.12: é -> e acute (✓) \n",
      "630475  77% ( 110m 7s)   0.240   |   0.03: st -> saint (✓) \n",
      "640475  80% ( 114m 2s)   0.263   |   1.88: Aimag -> a i m  (✗: Aimag) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 73.49% (    7349/   10000)\n",
      "650475  83% (119m 11s)   0.280   |   0.02: 1981 -> nineteen eighty one (✓) \n",
      "660475  87% (123m 15s)   0.293   |   0.00: ( -> ( (✓) \n",
      "670475  90% (127m 19s)   0.275   |   0.00: & -> and (✓) \n",
      "680475  93% (131m 16s)   0.294   |   0.00: vs -> versus (✓) \n",
      "690475  97% (135m 13s)   0.249   |   1.76: dehumanised -> dehbaaaaized (✗: dehumanized) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 76.03% (    7603/   10000)\n",
      "700475 100% (140m 13s)   0.252   |   0.01: is -> is (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:00          => tex t ooe      || ten o'clock \n",
      "                  Collins officially commenced sea trials at when she departed the ASC wharf at <SAMPLE> on 31 October 1994 .\n",
      "colonisation   => conooizatioo   || colonization \n",
      "                  The pigeon became extinct following human <SAMPLE> of Henderson , an event that had occurred by 1050 CE.\n",
      "competed       => compeeed       || competed \n",
      "                  Majorie Homer Dixon ( born August 10, 1945 ) is a Canadian sprint canoer who <SAMPLE> in the late 1960s and early 1970s .\n",
      "July 2009      => july two thousand ni || july two thousand nine \n",
      "                  This game is still under development but an alpha version was released in the end of <SAMPLE> .\n",
      ".201           => point two t oo     || point two o one \n",
      "                  G.W. B Huntingford , The Historical Geography of Ethiopia ( London : The British Academy , 1989 ) , p <SAMPLE> .\n",
      "qctimes.com    => s c m i m e s dot c  || q c t i m e s dot c o m \n",
      "                  Retrieved 2014-11-06 Profile , <SAMPLE> ; accessed December 16, 2014 .\n",
      "gamonts        => aamonts        || gamonts \n",
      "                  The extracellular sporozoites develop directly into <SAMPLE> .\n",
      "contemporary   => contorrurarry  || contemporary \n",
      "                  Knyazhnin's <SAMPLE> success rested largely on his witty comedies The Braggart ( 1786 ) and The Cranks ( 1790 ) .\n",
      "participated   => particapateed  || participated \n",
      "                  Egypt's men's team <SAMPLE> in the Summer Olympic Games seven times .\n",
      "CSPS           => c s s s        || c s p s \n",
      "                  In 1967 , Governor General of Canada Georges Vanier became the first patron on the <SAMPLE> .\n",
      "January 22, 1973 => january twenty siith || january twenty second nineteen seventy three \n",
      "                  Reggie Devon Barlow ( born <SAMPLE> ) was a head coach of the Alabama State Hornets football team .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710475   3% (   4m 4s)   0.275   |   0.00: F. -> f (✓) (forcing)\n",
      "720475   7% (   8m 4s)   0.210   |   6.77: I -> tn (✗: I) \n",
      "730475  10% ( 12m 11s)   0.225   |   0.00: VC -> v c (✓) \n",
      "740475  13% (  16m 8s)   0.174   |   0.01: 1849 -> eighteen forty nine (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 80.10% (    8010/   10000)\n",
      "750475  17% ( 21m 22s)   0.201   |   0.00: AD -> a d (✓) (forcing)\n",
      "760475  20% ( 25m 28s)   0.186   |   1.91: FishBase -> fishbaae (✗: FishBase) (forcing)\n",
      "770475  23% ( 29m 26s)   0.174   |   0.11: August 14, 2013 -> august fourteenth twenty thirteen (✓) \n",
      "780475  27% ( 33m 24s)   0.179   |   0.00: in -> in (✓) \n",
      "790475  30% ( 37m 23s)   0.143   |   0.00: A- -> a (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 80.14% (    8014/   10000)\n",
      "800475  33% ( 42m 36s)   0.153   |   0.01: Eds -> e d's (✓) (forcing)\n",
      "810475  37% ( 46m 33s)   0.149   |   0.04: colonises -> colonizes (✓) (forcing)\n",
      "820475  40% ( 50m 35s)   0.157   |   0.01: 17 -> seventeen (✓) (forcing)\n",
      "830475  43% ( 54m 36s)   0.211   |   0.00: sr -> senior (✓) \n",
      "840475  47% ( 58m 44s)   0.179   |   0.00: CD -> c d (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 80.66% (    8066/   10000)\n",
      "850475  50% ( 63m 58s)   0.168   |   0.00: vs -> versus (✓) (forcing)\n",
      "860475  53% ( 67m 56s)   0.173   |   0.00: vol -> volume (✓) (forcing)\n",
      "870475  57% ( 71m 53s)   0.147   |   0.63: Karachi -> Karacch (✗: Karachi) \n",
      "880475  60% ( 75m 59s)   0.163   |   0.05: such -> such (✓) (forcing)\n",
      "890475  63% (  80m 5s)   0.149   |   0.33: 20 February 1933 -> the twentieth of february nineteen thirty two (✗: the twentieth of february nineteen thirty three) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 81.45% (    8145/   10000)\n",
      "900475  67% ( 85m 12s)   0.190   |   0.63: convenience -> connennnnce (✗: convenience) \n",
      "910475  70% (  89m 9s)   0.167   |   0.00: SH -> s h (✓) (forcing)\n",
      "920475  73% ( 93m 17s)   0.162   |   0.00: 2000 -> two thousand (✓) \n",
      "930475  77% ( 97m 17s)   0.135   |   0.00: 1893 -> eighteen ninety three (✓) (forcing)\n",
      "940475  80% (101m 16s)   0.152   |   0.01: 1992 -> nineteen ninety two (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 80.89% (    8089/   10000)\n",
      "950475  83% (106m 28s)   0.150   |   0.00: Centre -> center (✓) \n",
      "960475  87% (110m 33s)   0.138   |   0.63: Holiness -> Holliess (✗: Holiness) (forcing)\n",
      "970475  90% (114m 36s)   0.125   |   0.00: & -> and (✓) (forcing)\n",
      "980475  93% (118m 40s)   0.147   |   0.19: Urbanisation -> urbanization (✓) (forcing)\n",
      "990475  97% (122m 45s)   0.154   |   0.01: BJTC -> b j t c (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/1000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 81.58% (    8158/   10000)\n",
      "1000475 100% (127m 58s)   0.153   |   1.06: September 1st -> septemberstrrtt (✗: september first) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleophoridae  => Colehoporrpe   || Coleophoridae \n",
      "                  Coleophora pyrrhulipennella is a moth of the <SAMPLE> family .\n",
      "PDRacer.com    => p r e r c r r dot c  || p d r a c e r dot c o m \n",
      "                  \" Puddle Duck Racers with Cabins \" <SAMPLE> .\n",
      "Chinaview.cn   => c h i n n a n e e do || c h i n a v i e w dot c n \n",
      "                  <SAMPLE> , 15 August 2008 Nepali PM Prachanda Sworn In .\n",
      "24 February 2014 => the twenty fourth of || the twenty fourth of february twenty fourteen \n",
      "                  ABC News Radio ( <SAMPLE> ) .\n",
      "December 14, 2011 => december fourteenth  || december fourteenth twenty eleven \n",
      "                  Caulfield , Keith ( <SAMPLE> ) .\n",
      "April 30, 1998 => april thirtieth nige || april thirtieth nineteen ninety eight \n",
      "                  Amy Silverman , \" Framing Marilyn Zeitlin \" ( <SAMPLE> ) .\n",
      "containing     => contaniing     || containing \n",
      "                  Two missile pods each <SAMPLE> two missiles are mounted , one of each side of the turret .\n",
      "7 October 2010 => the seventh of octob || the seventh of october twenty ten \n",
      "                  Accessed <SAMPLE> .\n",
      "$18,145        => eighteen thousand fi || eighteen thousand one hundred forty five dollars \n",
      "                  The per capita income for the borough was <SAMPLE> .\n",
      "75,000         => seventy fiv  huuusan || seventy five thousand \n",
      "                  Today , at least <SAMPLE> citizens live in 40 unrecognized villages , among which Wadi al Na'am is the largest .\n",
      "1977           => nineteen seventy sev || nineteen seventy seven \n",
      "                  <SAMPLE> , p . 77 .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010475   3% (  3m 57s)   0.279   |   0.00: jr -> junior (✓) \n",
      "1020475   7% (  8m 10s)   0.235   |   0.00: US -> u s (✓) (forcing)\n",
      "1030475  10% ( 12m 17s)   0.268   |   2.77: 4.5 -> fovry point five (✗: four point five) (forcing)\n",
      "1040475  13% ( 16m 18s)   0.285   |   0.00: S. -> s (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/1050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 76.74% (    7674/   10000)\n",
      "1050475  17% ( 21m 37s)   0.257   |   2.08: 1st -> fixtee (✗: first) \n",
      "1060475  20% ( 25m 35s)   0.280   |   0.51: capitalized -> capitizized (✗: capitalized) \n",
      "1070475  23% ( 29m 36s)   0.354   |   0.04: 1990s -> nineteen nineties (✓) (forcing)\n",
      "1080475  27% ( 33m 33s)   0.314   |   0.35: larger -> laarer (✗: larger) \n",
      "1090475  30% ( 37m 27s)   0.326   |   0.00: & -> and (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/1100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 75.91% (    7591/   10000)\n",
      "1100475  33% ( 42m 34s)   0.301   |   0.01: F.G. -> f g (✓) \n",
      "1110475  37% ( 46m 39s)   0.281   |   1.53: http://brain.oxfordjournals.org/content/137/2/621.longBeaumont -> h t t p colon slash slash t a r t e d a r e t e a o e r o e a e r o e e a e r o e e a e r e e e a e e e a e e e e a e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e  (✗: h t t p colon slash slash b r a i n dot o x f o r d j o u r n a l s dot o r g slash c o n t e n t slash o n e t h r e e s e v e n slash t w o slash s i x t w o o n e dot l o n g b e a u m o n t) (forcing)\n",
      "1120475  40% ( 50m 36s)   0.302   |   0.02: eds -> e d s (✓) (forcing)\n",
      "1130475  43% ( 54m 40s)   0.340   |   0.00: J. -> j (✓) (forcing)\n",
      "1140475  47% ( 58m 37s)   0.323   |   0.01: favour -> favor (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/1150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 75.17% (    7517/   10000)\n",
      "1150475  50% ( 63m 45s)   0.306   |   0.03: 2 -> two (✓) (forcing)\n",
      "1160475  53% ( 67m 44s)   0.325   |   0.00: # -> number (✓) (forcing)\n",
      "1170475  57% ( 71m 42s)   0.382   |   0.04: located -> located (✓) (forcing)\n",
      "1180475  60% ( 75m 53s)   0.383   |   2.90: http://www.coffeyweb.com/ashley.htm -> h t t                                                                                      (✗: h t t p colon slash slash w w w dot c o f f e y w e b dot com slash a s h l e y dot h t m) (forcing)\n",
      "1190475  63% (  80m 1s)   0.336   |   0.03: 1973 -> nineteen seventy three (✓) (forcing)\n",
      "Saved model to data/models/whole_rnn_1_mod_data/1200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 74.15% (    7415/   10000)\n",
      "1200475  67% ( 85m 11s)   0.364   |   0.04: behaviour -> behavior (✓) (forcing)\n",
      "1210475  70% ( 89m 16s)   0.424   |   0.00: \" -> \" (✓) (forcing)\n",
      "1220475  73% ( 93m 20s)   0.390   |   0.00: vs -> versus (✓) (forcing)\n",
      "1230475  77% ( 97m 23s)   0.372   |   0.01: - -> to (✓) \n",
      "1240475  80% (101m 21s)   0.394   |   0.00: ltd -> limited (✓) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/1250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 71.11% (    7111/   10000)\n",
      "1250475  83% (106m 37s)   0.361   |   0.45: Maid -> Maid (✓) (forcing)\n",
      "1260475  87% (110m 35s)   0.398   |   0.00: ( -> ( (✓) \n",
      "1270475  90% (114m 37s)   0.357   |   2.08: synonymous -> synnomems (✗: synonymous) \n",
      "1280475  93% (118m 39s)   0.393   |   0.06: 2010 -> twenty ten (✓) (forcing)\n",
      "1290475  97% (122m 40s)   0.453   |   0.39: 1988 -> nineteen teghty eight (✗: nineteen eighty eight) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/1300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 67.86% (    6786/   10000)\n",
      "1300475 100% (127m 51s)   0.443   |   0.08: these -> these (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 June 1949    => she f on f  f f of n || the seventh of june nineteen forty nine \n",
      "                  Surinder Singh Nijjar ( born <SAMPLE> ) is a former judge of the Supreme Court of India .\n",
      "Dog            => Don            || Dog \n",
      "                  German Shepherd <SAMPLE> Club of Victoria inc Archived from the original on 2008-07-20 .\n",
      "4: 25          => jont wonty t i tte || four twenty five \n",
      "                  Biodiversity and Ecology <SAMPLE> -39 .\n",
      "1937           => nintteen sary twvent || nineteen thirty seven \n",
      "                  Pauline Rhodes ( born <SAMPLE> ) is a New Zealand artist .\n",
      "2011-11-19     => the twen        f    || the nineteenth of november twenty eleven \n",
      "                  Kim , Seung Hyup ( <SAMPLE> ) .\n",
      "Anthologised   => antooliized    || anthologized \n",
      "                  <SAMPLE> in : Poets' Choice , 1977 ; Holes in the Evening ( 1982 ) , Fat Possum Press ; and That Moon Filled Urge ( 1985 ) , Kardoorair Press .\n",
      "29 April       => the twonty stott  ni || the twenty ninth of april \n",
      "                  He was appointed governor on 30 March 1844 and took office on <SAMPLE> .\n",
      "$              => dolloo         || dollar \n",
      "                  He was one of the developers of the Turkish interactive dictionary Ek <SAMPLE> i sozluk .\n",
      "Onet.pl        => o p t p dot t o d || o n e t dot p l \n",
      "                  \" MEDIUM - \" Graal \" - Recenzja — <SAMPLE> \" .\n",
      "June 28, 2012  => jute twenty twttwot  || june twenty eighth twenty twelve \n",
      "                  Kuruvila , Matthai ( <SAMPLE> ) .\n",
      "Sellam         => Slllmm         || Sellam \n",
      "                  Future African champion Aida <SAMPLE> of Tunisia won the javelin throw and a shot put bronze medal on home turf .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310475   3% (   4m 1s)   0.453   |   1.29: Side-line.com -> s i d d d e e e e e e e e c e c m  (✗: s i d e d a s h l i n e dot c o m) \n",
      "1320475   7% (  7m 59s)   0.412   |   0.39: University -> Univervity (✗: University) (forcing)\n",
      "1330475  10% (  12m 0s)   0.414   |   0.02: st -> saint (✓) (forcing)\n",
      "1340475  13% ( 16m 13s)   0.428   |   1.55: footballbrisbane.com.au -> f o t a a a a a a a a a a a a s t s t t d t t t t  (✗: f o o t b a l l b r i s b a n e dot c o m dot a u) \n",
      "Saved model to data/models/whole_rnn_1_mod_data/1350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 70.40% (    7040/   10000)\n",
      "1350475  17% ( 21m 17s)   0.396   |   0.01: - -> to (✓) \n",
      "1360475  20% ( 25m 16s)   0.356   |   0.00: - -> to (✓) \n",
      "1370475  23% ( 29m 24s)   0.373   |   0.00: - -> to (✓) \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-41d2a92b922c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-142-0f4e32ab0de3>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m                              \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-2c97307a8802>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacters_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdecoded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.2, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-ae80356ad3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_local_wrong_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-185-bcc32fc032c7>\u001b[0m in \u001b[0;36mprint_local_wrong_predictions\u001b[0;34m(max_results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_local_wrong_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_some_wrong_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model_single_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0ms_bef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_aft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:<14} => {:<14} || {} \\n{:>17} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_bef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_aft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/koodi/kesken/kaggle_text_normalization_en/pytorch_utils_oh_1.py\u001b[0m in \u001b[0;36mget_some_wrong_predictions\u001b[0;34m(model, test_model_single_sample_fn, max_iterations, max_results)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mwrong_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_single_sample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mguess\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mwrong_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-c1d90b48f4ef>\u001b[0m in \u001b[0;36mtest_model_single_sample\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacters_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Use own prediction as next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEOS_TOKEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.4, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', ' '.join(s_sentence), ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 21, 1960 => december tienty nine || december twenty first nineteen sixty \n",
      "                  Erin Leigh Callin Kenny ( born <SAMPLE> ) is a former Democratic politician from Nevada .\n",
      "MUSL           => m u l l        || m u s l \n",
      "                  In December 2010 , a jackpot winning ticket for Hot Lotto jackpot was purchased near <SAMPLE> headquarters .\n",
      "ETSI           => e t t          || e t s i \n",
      "                  TransNexus , along with Cisco , 3 Com and others was the creator of the <SAMPLE> Open settlement protocol ( OSP ) .\n",
      "FSM            => f m m          || f s m \n",
      "                  The COM <SAMPLE> system also includes the Fisheries and Maritime Institute ( FMI ) on the Yap islands .\n",
      "September 5, 2012 => sepeember twentyetee || september fifth twenty twelve \n",
      "                  Kindelan , Katie ( <SAMPLE> ) .\n",
      "http://www.nytimes.com/2015/06/12/business/media/line-music-a-new-streaming-service-aims-at-japanese-market.html => h t t p colon slash  || h t t p colon slash slash w w w dot n y t i m e s dot com slash t w e n t y f i f t e e n slash o s i x slash t w e l v e slash b u s i n e s s slash m e d i a slash l i n e dash m u s i c dash a dash n e w dash s t r e a m i n g dash s e r v i c e dash a i m s dash a t dash j a p a n e s e dash m a r k e t dot h t m l \n",
      "                  New York Times 11 June 2015 : <SAMPLE> ?\n",
      "televise       => televive       || televize \n",
      "                  In contrast , IndiePlex and RetroPlex will <SAMPLE> films with those assigned ratings .\n",
      "OmegaTiming.com => o m e g i i i i i i  || o m e g a t i m i n g dot c o m \n",
      "                  Published by <SAMPLE> ( official timer of the ' 07 Worlds ) ; retrieved 2009-07-11 .\n",
      "#              => number         || hash \n",
      "                  Code 15 , as amended by the Amendment <SAMPLE> 243/2014 of April 18, 2014 .\n",
      "NYPress.com    => y y n e s d dot c m || n y p r e s s dot c o m \n",
      "                  \" Pressed for Time : Molly Davies' Traditions , Inventions , Exchange \" , <SAMPLE> .\n",
      "I              => twe            || one \n",
      "                  He began teaching music during World War <SAMPLE> , first to Army bands and later to public schoolchildren .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
