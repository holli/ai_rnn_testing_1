{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'whole_gen_13_only_chars_and_attn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pickle.load(open(\"data/en_train_fixed_4_sentences.pkl\", \"rb\" ))\n",
    "all_data = pickle.load(open(\"data/en_train_fixed_5_manual.pkl\", \"rb\" ))\n",
    "# all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 1654333,  (dropped rows: 8263859)\n"
     ]
    }
   ],
   "source": [
    "sample_data = all_data.copy()\n",
    "sample_data = sample_data[sample_data['class'] != 'NOT_CHANGED']\n",
    "sample_data = sample_data[sample_data['class'] != 'MANUAL']\n",
    "sample_data = pd.concat([sample_data, all_data[all_data['class_org'] == 'PLAIN'].sample(1000000)])\n",
    "# sample_data = sample_data[sample_data['class'] != 'ELECTRONIC']\n",
    "sample_data = sample_data.reset_index(drop=True)\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(sample_data), len(all_data)-len(sample_data)))\n",
    "#del(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>679572</th>\n",
       "      <td>203927</td>\n",
       "      <td>17</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>affluent</td>\n",
       "      <td>affluent</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hudnut 's beauty products were sold in departm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166155</th>\n",
       "      <td>190828</td>\n",
       "      <td>1</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>.256</td>\n",
       "      <td>point two five six</td>\n",
       "      <td>DECIMAL</td>\n",
       "      <td>[46, 5, 14, 20, 0]</td>\n",
       "      <td>the &lt;SAMPLE&gt; newton and the 6 are not intercha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085611</th>\n",
       "      <td>725759</td>\n",
       "      <td>13</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>receive</td>\n",
       "      <td>receive</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>if they handed to rose over to the correct wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519840</th>\n",
       "      <td>594469</td>\n",
       "      <td>2</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>F.C.</td>\n",
       "      <td>f c</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>[37, 21, 0]</td>\n",
       "      <td>cardiff city &lt;SAMPLE&gt; 2009 - 11 - 28 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309072</th>\n",
       "      <td>356414</td>\n",
       "      <td>1</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>PS</td>\n",
       "      <td>p s</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>[24, 17, 0]</td>\n",
       "      <td>\" &lt;SAMPLE&gt; e psd rejeitam proposta para reconh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047151</th>\n",
       "      <td>354179</td>\n",
       "      <td>10</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pines of rome ( italian : pini di roma ) &lt;SAMP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456288</th>\n",
       "      <td>473851</td>\n",
       "      <td>8</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>Mockingbird</td>\n",
       "      <td>Mockingbird</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>isbn 0 - 8071 - 0390 - x \" to kill a &lt;SAMPLE&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659711</th>\n",
       "      <td>652395</td>\n",
       "      <td>4</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>Scott</td>\n",
       "      <td>Scott</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sammon wrote that ridley &lt;SAMPLE&gt; thought it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53348</th>\n",
       "      <td>61430</td>\n",
       "      <td>5</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>2009</td>\n",
       "      <td>two thousand nine</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[5, 8, 15, 0]</td>\n",
       "      <td>brandeis , magdalene ( spring &lt;SAMPLE&gt; ) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482564</th>\n",
       "      <td>552884</td>\n",
       "      <td>16</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>PDF</td>\n",
       "      <td>p d f</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>[24, 26, 37, 0]</td>\n",
       "      <td>\" wealth and happiness revisited growing wealt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id        class       before               after  \\\n",
       "679572        203927        17  NOT_CHANGED     affluent            affluent   \n",
       "166155        190828         1      NUMBERS         .256  point two five six   \n",
       "1085611       725759        13  NOT_CHANGED      receive             receive   \n",
       "519840        594469         2      LETTERS         F.C.                 f c   \n",
       "309072        356414         1      LETTERS           PS                 p s   \n",
       "1047151       354179        10  NOT_CHANGED           is                  is   \n",
       "1456288       473851         8  NOT_CHANGED  Mockingbird         Mockingbird   \n",
       "659711        652395         4  NOT_CHANGED        Scott               Scott   \n",
       "53348          61430         5      NUMBERS         2009   two thousand nine   \n",
       "482564        552884        16      LETTERS          PDF               p d f   \n",
       "\n",
       "        class_org          a_word_ind  \\\n",
       "679572      PLAIN                 NaN   \n",
       "166155    DECIMAL  [46, 5, 14, 20, 0]   \n",
       "1085611     PLAIN                 NaN   \n",
       "519840    LETTERS         [37, 21, 0]   \n",
       "309072    LETTERS         [24, 17, 0]   \n",
       "1047151     PLAIN                 NaN   \n",
       "1456288     PLAIN                 NaN   \n",
       "659711      PLAIN                 NaN   \n",
       "53348        DATE       [5, 8, 15, 0]   \n",
       "482564    LETTERS     [24, 26, 37, 0]   \n",
       "\n",
       "                                                  sentence  \n",
       "679572   hudnut 's beauty products were sold in departm...  \n",
       "166155   the <SAMPLE> newton and the 6 are not intercha...  \n",
       "1085611  if they handed to rose over to the correct wom...  \n",
       "519840              cardiff city <SAMPLE> 2009 - 11 - 28 .  \n",
       "309072   \" <SAMPLE> e psd rejeitam proposta para reconh...  \n",
       "1047151  pines of rome ( italian : pini di roma ) <SAMP...  \n",
       "1456288  isbn 0 - 8071 - 0390 - x \" to kill a <SAMPLE> ...  \n",
       "659711   sammon wrote that ridley <SAMPLE> thought it w...  \n",
       "53348           brandeis , magdalene ( spring <SAMPLE> ) .  \n",
       "482564   \" wealth and happiness revisited growing wealt...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ELECTRONIC', 'LETTERS', 'NOT_CHANGED', 'NUMBERS', 'PLAIN', 'VERBATIM']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "categories_all = sorted(sample_data[\"class\"].unique())\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1300719</th>\n",
       "      <td>547543</td>\n",
       "      <td>9</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>Literary</td>\n",
       "      <td>Literary</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>manchester is also the recipient of the abraha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126530</th>\n",
       "      <td>505044</td>\n",
       "      <td>1</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>Sugiura</td>\n",
       "      <td>Sugiura</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ken &lt;SAMPLE&gt; ( 2008 - 12 - 19 ) .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id        class    before     after class_org  \\\n",
       "1300719       547543         9  NOT_CHANGED  Literary  Literary     PLAIN   \n",
       "1126530       505044         1  NOT_CHANGED   Sugiura   Sugiura     PLAIN   \n",
       "\n",
       "        a_word_ind                                           sentence  \n",
       "1300719        NaN  manchester is also the recipient of the abraha...  \n",
       "1126530        NaN                  ken <SAMPLE> ( 2008 - 12 - 19 ) .  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ 'abcdefghijklmnopqrstuvwxyzé\n"
     ]
    }
   ],
   "source": [
    "chars_after = sorted(list(set(''.join(list(sample_data['after'].unique())).lower())))\n",
    "chars_after = [SOS_TOKEN, EOS_TOKEN, UNKNOWN_CHAR] + chars_after\n",
    "chars_after_index = dict((c, i) for i, c in enumerate(chars_after))\n",
    "print(''.join(chars_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chars_after, chars_after_index = load_characters_pkl('data/en_features/chars_after_1.pkl')\n",
    "print(''.join(chars_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_tensor = torch.zeros(1, 1, len(chars_after_index))\n",
    "sos_tensor[0, 0, chars_after_index[SOS_TOKEN]] = 1\n",
    "sos_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_classes_select = list(sample_data.groupby('class'))\n",
    "\n",
    "balanced_data_accessed_counter = 0 \n",
    "balanced_data_randomize_freq = False\n",
    "balanced_data_length = 0\n",
    "\n",
    "def balanced_data_randomize_org(max_len=30000):\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter, balanced_data_randomize_freq\n",
    "    balanced_data = pd.concat([v.sample(min(max_len, len(v))) for k, v in balanced_data_classes_select])\n",
    "    balanced_data = pd.concat([balanced_data, sample_data[sample_data['class']=='NOT_CHANGED'].sample(int(max_len*2))])\n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_randomize_freq = balanced_data_length * 0.2\n",
    "    balanced_data_accessed_counter = 0\n",
    "balanced_data_randomize = balanced_data_randomize_org\n",
    "\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_accessed_counter\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_accessed_counter += 1\n",
    "    if balanced_data_randomize_freq and balanced_data_accessed_counter > balanced_data_randomize_freq:\n",
    "        balanced_data_randomize()\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample\n",
    "    \n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC      4964\n",
       "LETTERS        30000\n",
       "NOT_CHANGED    90000\n",
       "NUMBERS        30000\n",
       "PLAIN          30000\n",
       "VERBATIM       11741\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data.groupby('class')['class'].count()\n",
    "#sample_data.groupby('class')['class'].count()\n",
    "balanced_data.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id                                               376723\n",
       "token_id                                                      13\n",
       "class                                                NOT_CHANGED\n",
       "before                                           amitochondriate\n",
       "after                                            amitochondriate\n",
       "class_org                                                  PLAIN\n",
       "a_word_ind                                                   NaN\n",
       "sentence       \" the chimeric eukaryote : origin of the nucle...\n",
       "Name: 934523, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LETTERS : U.S. -> u s\n",
      "['22', '<SAMPLE>', 'at', '675', '.']\n",
      "torch.Size([1, 5, 104])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()   \n",
    "    return sample_row['before'], sample_row['after'].lower(), sample_row['class'], sample_row['sentence'].split(' ')\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_class, ':', s_bef, '->', s_aft)\n",
    "    print(s_sentence)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 291 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('.',\n",
       " '.',\n",
       " 'NOT_CHANGED',\n",
       " ['in', '1925', 'william', 'barclay', 'peat', '&', 'co', '<SAMPLE>'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ATTENTION_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19757</th>\n",
       "      <td>22689</td>\n",
       "      <td>10</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.kiwiaircraftimages.com/mustang.html</td>\n",
       "      <td>h t t p colon slash slash w w w dot k i w i a ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>missing plane , evening post , 5 september 194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9517</th>\n",
       "      <td>11138</td>\n",
       "      <td>2</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.jaabc.com/brcv14n1preview.htmlHarvard</td>\n",
       "      <td>h t t p colon slash slash w w w dot j a a b c ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>retrieved from &lt;SAMPLE&gt; business review , hbr ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id  token_id       class  \\\n",
       "19757        22689        10  ELECTRONIC   \n",
       "9517         11138         2  ELECTRONIC   \n",
       "\n",
       "                                                 before  \\\n",
       "19757    http://www.kiwiaircraftimages.com/mustang.html   \n",
       "9517   http://www.jaabc.com/brcv14n1preview.htmlHarvard   \n",
       "\n",
       "                                                   after   class_org  \\\n",
       "19757  h t t p colon slash slash w w w dot k i w i a ...  ELECTRONIC   \n",
       "9517   h t t p colon slash slash w w w dot j a a b c ...  ELECTRONIC   \n",
       "\n",
       "                                              a_word_ind  \\\n",
       "19757  [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "9517   [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "\n",
       "                                                sentence  \n",
       "19757  missing plane , evening post , 5 september 194...  \n",
       "9517   retrieved from <SAMPLE> business review , hbr ...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = sample_data[sample_data['before'].str.len()>MAX_ATTENTION_LENGTH]\n",
    "len(tmp)\n",
    "tmp.sample(2)\n",
    "# tmp[~tmp['before'].str.contains('/')].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): GRU(50, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): GRU(104, 256, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "        self.hidden_size = words_hidden_size + chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.GRU(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.GRU(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = hidden_words.view(1, -1)\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = hidden_chars.view(1, -1)\n",
    "        \n",
    "        #hidden_states_cat = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.hidden_size)).cuda()\n",
    "        #for ei in range(min(MAX_ATTENTION_LENGTH, len(string_tensor[0]))):\n",
    "        #    hidden_states_cat[ei] = torch.cat((output_words, all_outputs_chars[0, ei].view(1,-1)), 1)\n",
    "\n",
    "        all_outputs_chars_padded = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.chars_hidden_size)).cuda()\n",
    "        att_length = min(len(all_outputs_chars[0]), MAX_ATTENTION_LENGTH-1)\n",
    "        all_outputs_chars_padded[0:att_length] = all_outputs_chars[0][0:att_length]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "\n",
    "        #output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #return output, all_outputs_chars\n",
    "        return output, all_outputs_chars_padded\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1 = var1.cuda(); var2 = var2.cuda()\n",
    "        return (var1, var2)\n",
    "    \n",
    "    \n",
    "encoder_rnn = EncoderRNN(words_input_size=wv_vecs.shape[-1], chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=512,\n",
    "                         words_layers=1, chars_layers=1).cuda()\n",
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rough\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rough'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 512])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    #s_bef, s_aft, s_class, s_sentence = get_random_sample(True)\n",
    "    print(s_bef)\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(s_sentence, wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t), s_bef\n",
    "    \n",
    "(tmp_encoder_output, tmp_encoder_outputs), tmp = test_encoder_single_sample()\n",
    "tmp\n",
    "tmp_encoder_output.size()\n",
    "tmp_encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (emb_lin): Linear (32 -> 640)\n",
       "  (dropout): Dropout (p = 0.1)\n",
       "  (attn): Linear (1280 -> 30)\n",
       "  (attn_combine): Linear (1152 -> 640)\n",
       "  (rnn): GRU(640, 640, batch_first=True)\n",
       "  (lin_out): Linear (640 -> 32)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 32]), torch.Size([1, 640]), torch.Size([1, 30])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, chars_encoded_size,\n",
    "                 n_layers=1, dropout_p=0.1, max_length=MAX_ATTENTION_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.emb_lin = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = nn.Linear(self.hidden_size+self.hidden_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size+chars_encoded_size, self.hidden_size)\n",
    "        \n",
    "        #self.module_attn = torch.nn.ModuleList([self.emb_lin, self.dropout, self.attn, self.attn_combine])\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, bidirectional=False)\n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        #self.module_rnn = torch.nn.ModuleList([self.rnn, self.lin_out])\n",
    "\n",
    "    def forward(self, last_input, hidden, encoder_outputs):\n",
    "        embedded = self.emb_lin(last_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = embedded[0]\n",
    "                \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded, hidden), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        #return embedded, attn_applied\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, attn_applied[0]), 1)\n",
    "        rnn_input = self.attn_combine(rnn_input).unsqueeze(0)\n",
    "        rnn_input = F.relu(rnn_input)\n",
    "    \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        output = F.log_softmax(self.lin_out(output[0]))\n",
    "        \n",
    "        return output, hidden[0], attn_weights\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        if self.n_layers > 1:\n",
    "            hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "            res = torch.cat((input_var, hid_var), 0)\n",
    "            return res\n",
    "        else:\n",
    "            return input_var\n",
    "        \n",
    "    def mods_split(self):\n",
    "        mods = list(decoder_rnn.modules())[1:]\n",
    "        for gru_index, mod in enumerate(mods):\n",
    "            #print(mod)\n",
    "            if type(mod) == torch.nn.modules.rnn.GRU:\n",
    "                break\n",
    "        return mods[:gru_index], mods[gru_index:]\n",
    "        \n",
    "    def mods_attn(self):\n",
    "        return self.mods_split()[0]\n",
    "        \n",
    "    def mods_gru(self):\n",
    "        return self.mods_split()[1]\n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(chars_after), hidden_size=tmp_encoder_output.size()[1],\n",
    "                         chars_encoded_size=tmp_encoder_outputs.size()[1], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn\n",
    "\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_encoder_output, tmp_encoder_outputs)\n",
    "#tmp\n",
    "[v.size() for v in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 3, 'params': <generator object Module.parameters at 0x7fc10bbee4c0>},\n",
       " {'lr': 3, 'params': <generator object Module.parameters at 0x7fc10bbee990>},\n",
       " {'lr': 3, 'params': <generator object Module.parameters at 0x7fc10bbee780>},\n",
       " {'lr': 3, 'params': <generator object Module.parameters at 0x7fc10bbee6d0>},\n",
       " {'params': <generator object Module.parameters at 0x7fc10bbee8e0>},\n",
       " {'params': <generator object Module.parameters at 0x7fc10d5eaba0>},\n",
       " {'params': <generator object Module.parameters at 0x7fc10d5eaf68>}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [{'params': mod.parameters(), 'lr': 3} for mod in decoder_rnn.mods_attn()]\n",
    "tmp += [{'params': mod.parameters()} for mod in decoder_rnn.mods_gru()]\n",
    "tmp.append(\n",
    "    {'params': encoder_rnn.parameters()}\n",
    ")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iiiissssssssssssssss',\n",
       " 'iiiissssssssssssssss',\n",
       " 'mister',\n",
       " ('mr',\n",
       "  'mister',\n",
       "  'PLAIN',\n",
       "  ['<SAMPLE>', 'lucton', \"'s\", 'freedom', '(', 'hardcover', ')', '.']))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None, return_more=False, sample=False):\n",
    "    decoder_rnn.eval()\n",
    "    encoder_rnn.eval()\n",
    "    \n",
    "    if not sample:\n",
    "        sample = get_random_sample()\n",
    "    s_bef, s_aft, s_class, s_sentence = sample\n",
    "        \n",
    "    words_t = Variable(words_to_word_vectors_tensor(s_sentence, wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "    decoder_hidden = encoder_output\n",
    "    \n",
    "    decoded_output = []\n",
    "    decoder_attns_arr = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attns_arr.append(decoder_attns)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = chars_after[char_index] # Use own prediction as next input\n",
    "                \n",
    "        if char == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(char)\n",
    "                \n",
    "        decoder_input = torch.zeros(1, 1, len(chars_after_index))\n",
    "        decoder_input[0, 0, char_index] = 1\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ''.join(decoded_output)\n",
    "    sample_target = s_aft\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "    \n",
    "    if return_more:\n",
    "        return output, decoded_output, decoder_attns_arr, sample\n",
    "    \n",
    "    return output, output, sample_target, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972           => iiiissssssssssssssss || nineteen seventy two \n",
      "                  ['in', '<SAMPLE>', ',', 'he', 'joined', 'hk', 'jesenice', '.']\n",
      "-              => iiiissssssssssssssss || to \n",
      "                  ['the', 'practical', 'upper', 'limit', 'for', 'wooden', 'constructions', 'fast', 'and', 'maneuverable', 'enough', 'for', 'warfare', 'was', 'around', '25', '<SAMPLE>', '30', 'oars', 'per', 'side', '.']\n"
     ]
    }
   ],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', s_sentence, ))\n",
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/     100)\n",
      "CPU times: user 2.11 s, sys: 20 ms, total: 2.13 s\n",
      "Wall time: 2.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(s_sentence, wv_vecs, wv_idx)).cuda() \n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    target_arr = [chars_after_index[c] for c in list(s_aft)]\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        decoder_target_i = target_arr[i]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = chars_after[char_index] # Use own prediction as next input\n",
    "        decoded_output.append(char)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            char_index = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if char == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_input = torch.zeros(1, 1, len(chars_after_index))\n",
    "        decoder_input[0, 0, char_index] = 1\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    return ''\n",
    "    # not used yet\n",
    "    # https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n",
    "    if clip_parameters_value:\n",
    "        for m in [decoder_rnn, encoder_rnn]:\n",
    "            torch.nn.utils.clip_grad_norm(m.parameters(), clip_parameters_value)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return ''.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-2bccc77e4839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-b943d87a23ec>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=1, print_every=1, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = encoder_rnn.parameters()\n",
    "parameters = list(filter(lambda p: p.grad is not None, parameters)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  8.0750\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[0].grad.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0807504802942276"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = max(p.grad.data.abs().max() for p in parameters)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3423733531166572"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 0 \n",
    "for p in parameters:\n",
    "    param_norm = p.grad.data.norm(2)\n",
    "    tmp += param_norm ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29207880546102816"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-05 *\n",
       "  7.9407\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[0].grad[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1000\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1).mul_(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "    global optimizer\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    tmp = [{'params': mod.parameters(), 'lr': (lr/10)} for mod in decoder_rnn.mods_attn()]\n",
    "    tmp += [{'params': mod.parameters()} for mod in decoder_rnn.mods_gru()]\n",
    "    tmp.append(\n",
    "        {'params': encoder_rnn.parameters()}\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(tmp, lr=lr)\n",
    "    \n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             optimizer=optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            correct = '✓' if result == s_aft else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/whole_gen_13_only_chars_and_attn\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   3.436   |   3.42: & -> iio (✗: and) (forcing)\n",
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (  3m 31s)   3.420   |   3.39: - -> ii (✗: to) \n",
      "    27  54% (  3m 31s)   3.400   |   3.32: 1958 -> iiooooeoeeeooooeooeo (✗: nineteen fifty eight) (forcing)\n",
      "    36  72% (  3m 31s)   3.378   |   3.26: 18 April 1840 -> iieeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee (✗: the eighteenth of april eighteen forty) \n",
      "    45  90% (  3m 32s)   3.359   |   3.23: rBST -> ieeeeee (✗: r b s t) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (  0m 14s)   2.992   |   3.12: ICC -> ateee (✗: i c c) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr']\n",
    "optimizer.param_groups[6]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 26s)   2.733   |   2.97: her -> ton (✗: her) \n",
      "  3000  22% (  0m 53s)   2.622   |   0.83: & -> and (✓) \n",
      "  4000  33% (  1m 21s)   2.491   |   2.09: May 2010 -> t n teente n e (✗: may twenty ten) (forcing)\n",
      "  5000  44% (  1m 51s)   2.339   |   2.45: Vol -> aonina (✗: volume) (forcing)\n",
      "  6000  56% (  2m 18s)   2.206   |   0.30: & -> and (✓) (forcing)\n",
      "  7000  67% (  2m 45s)   2.048   |   0.72: - -> to (✓) (forcing)\n",
      "  8000  78% (  3m 14s)   1.889   |   1.55: January 2015 -> aaadnne twenty tintyen (✗: january twenty fifteen) (forcing)\n",
      "  9000  89% (  3m 41s)   1.737   |   2.25: Clitheroe -> cetheeter (✗: clitheroe) \n",
      " 10000 100% (  4m 11s)   1.748   |   3.67: up -> t  (✗: up) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (  4m 50s)   0.399   |   0.06: Realising -> realizing (✓) \n",
      " 30000  22% (  9m 36s)   0.262   |   0.02: side -> side (✓) (forcing)\n",
      " 40000  33% ( 14m 32s)   0.225   |   0.00: vol -> volume (✓) (forcing)\n",
      " 50000  44% ( 19m 19s)   0.213   |   0.11: I. -> i (✓) \n",
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.61% (      61/   10000)\n",
      " 60000  56% ( 27m 34s)   0.165   |   0.00: Machine -> machine (✓) (forcing)\n",
      " 70000  67% (  32m 4s)   0.198   |   0.00: rock -> rock (✓) (forcing)\n",
      " 80000  78% ( 36m 36s)   0.226   |   0.01: : -> to (✓) (forcing)\n",
      " 90000  89% (  41m 7s)   0.149   |   0.00: - -> to (✓) (forcing)\n",
      "100000 100% ( 45m 38s)   0.150   |   0.01: st -> saint (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.64% (      64/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000   5% (  4m 26s)   0.209   |   0.00: for -> for (✓) \n",
      "120000  10% (  8m 59s)   0.604   |   3.36: 31 July 2008 -> thei t twalatlousalaousawenuloulanente ttt  (✗: the thirty first of july two thousand eight) \n",
      "130000  15% ( 13m 35s)   0.499   |   0.01: PDF -> p d f (✓) (forcing)\n",
      "140000  20% (  18m 9s)   0.390   |   0.00: white -> white (✓) (forcing)\n",
      "150000  25% ( 22m 42s)   0.333   |   1.49: 453 -> four  undr d niv y  oi d (✗: four hundred fifty three) (forcing)\n",
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "160000  30% ( 30m 34s)   0.329   |   0.01: R. -> r (✓) \n",
      "170000  35% (  35m 8s)   0.903   |   0.00: & -> and (✓) (forcing)\n",
      "180000  40% ( 39m 49s)   0.468   |   0.25: demobilisation -> domobilization (✗: demobilization) (forcing)\n",
      "190000  45% ( 44m 19s)   0.669   |   0.00: a -> a (✓) (forcing)\n",
      "200000  50% ( 48m 56s)   0.576   |   0.60: tumours -> tumurs (✗: tumors) (forcing)\n",
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "210000  55% ( 56m 51s)   0.675   |   0.97: publication -> publkcatt   (✗: publication) (forcing)\n",
      "220000  60% ( 61m 27s)   0.597   |   0.36: st -> saint (✓) \n",
      "230000  65% ( 65m 58s)   0.631   |   0.08: moved -> moved (✓) (forcing)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-17229cbbb125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-b943d87a23ec>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-491fc91207be>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars_after\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Use own prediction as next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdecoded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "        ...          ⋱          ...       \n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       " [torch.cuda.FloatTensor of size 640x32 (GPU 0)], Parameter containing:\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " [torch.cuda.FloatTensor of size 640 (GPU 0)], Parameter containing:\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "        ...          ⋱          ...       \n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       " [torch.cuda.FloatTensor of size 30x1280 (GPU 0)], Parameter containing:\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " nan\n",
       " [torch.cuda.FloatTensor of size 30 (GPU 0)], Parameter containing:\n",
       "  1.6752e-02 -2.1305e-02  1.9601e-02  ...  -5.7003e-03 -5.6631e-04  2.2138e-02\n",
       " -3.0109e-02 -1.7835e-02 -2.5097e-02  ...  -3.3590e-02  2.6348e-03  3.3311e-02\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       " -3.4734e-02 -7.5427e-03 -2.3478e-02  ...   3.0528e-02  4.4119e-02  3.1897e-02\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       " [torch.cuda.FloatTensor of size 640x1152 (GPU 0)], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -3.1228\n",
       "  -1.0642\n",
       "      nan\n",
       "  -0.0849\n",
       "      nan\n",
       "  -3.3031\n",
       "  -0.0718\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.2239\n",
       "      nan\n",
       "      nan\n",
       "  -2.7433\n",
       "  -3.9175\n",
       "      nan\n",
       "  -2.5435\n",
       "      nan\n",
       "  -0.7482\n",
       "      nan\n",
       "      nan\n",
       "   0.0027\n",
       "      nan\n",
       "      nan\n",
       "  -2.5191\n",
       "      nan\n",
       "  -6.4895\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.7633\n",
       "      nan\n",
       "  -3.2093\n",
       "  -5.8596\n",
       "  -2.2849\n",
       "  -3.7125\n",
       "      nan\n",
       "  -2.0131\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.1526\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.7285\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.3590\n",
       "      nan\n",
       "      nan\n",
       "  -2.3762\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "   1.3426\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.4178\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.0729\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.0052\n",
       "  -4.2813\n",
       "  -5.5219\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.2470\n",
       "  -5.9802\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -5.0890\n",
       "      nan\n",
       "   1.2480\n",
       "      nan\n",
       "      nan\n",
       "  -3.0581\n",
       "  -3.2147\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.8537\n",
       "      nan\n",
       "  -6.8801\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.3704\n",
       "      nan\n",
       "  -1.9516\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.7799\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.8006\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -5.3942\n",
       "  -1.4441\n",
       "      nan\n",
       "   0.3668\n",
       "      nan\n",
       "      nan\n",
       "  -2.4238\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.6366\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "   0.1646\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.3331\n",
       "  -3.1674\n",
       "  -5.0571\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.9880\n",
       "  -3.4242\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.3389\n",
       "      nan\n",
       "  -3.3416\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.9540\n",
       "      nan\n",
       "      nan\n",
       "   0.0693\n",
       "  -1.4426\n",
       "      nan\n",
       "      nan\n",
       "  -4.3129\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.0000\n",
       "      nan\n",
       "  -5.2159\n",
       "  -2.9373\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.0532\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.6896\n",
       "      nan\n",
       "  -4.6018\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.4967\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.3115\n",
       "      nan\n",
       "      nan\n",
       "  -4.5681\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.5715\n",
       "  -5.9693\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.9354\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.3341\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.3748\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.4153\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "   0.3478\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.7212\n",
       "  -2.0264\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.8346\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.8012\n",
       "      nan\n",
       "      nan\n",
       "  -1.6964\n",
       "  -4.9112\n",
       "   2.1395\n",
       "      nan\n",
       "  -3.3383\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.7778\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.7459\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.8039\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.9382\n",
       "      nan\n",
       "  -2.1475\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -5.0049\n",
       "      nan\n",
       "  -2.7596\n",
       "      nan\n",
       "      nan\n",
       "  -4.1983\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.7316\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -5.5326\n",
       "  -4.1666\n",
       "   1.4922\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.8044\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.7126\n",
       "      nan\n",
       "  -0.4298\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.7098\n",
       "      nan\n",
       "  -3.5093\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.6853\n",
       "      nan\n",
       "      nan\n",
       "  -1.3658\n",
       "      nan\n",
       "      nan\n",
       "  -2.5433\n",
       "  -6.2334\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.1911\n",
       "      nan\n",
       "      nan\n",
       "  -5.2332\n",
       "  -2.8929\n",
       "  -1.1494\n",
       "      nan\n",
       "      nan\n",
       "  -4.1685\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.9402\n",
       "      nan\n",
       "      nan\n",
       "   0.2741\n",
       "      nan\n",
       "      nan\n",
       "  -3.7395\n",
       "      nan\n",
       "  -3.7596\n",
       "      nan\n",
       "  -3.9286\n",
       "      nan\n",
       "      nan\n",
       "  -2.4808\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "   0.6119\n",
       "      nan\n",
       "      nan\n",
       "  -3.1640\n",
       "      nan\n",
       "  -1.4801\n",
       "      nan\n",
       "  -6.1736\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.9595\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.2518\n",
       "  -1.6191\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -5.2925\n",
       "  -2.6557\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.3482\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.5508\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.9459\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -6.0145\n",
       "      nan\n",
       "      nan\n",
       "  -2.1631\n",
       "      nan\n",
       "      nan\n",
       "  -2.4315\n",
       "      nan\n",
       "  -0.5730\n",
       "      nan\n",
       "      nan\n",
       "  -3.1547\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.4949\n",
       "      nan\n",
       "      nan\n",
       "  -4.6256\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.9493\n",
       "      nan\n",
       "   1.2322\n",
       "      nan\n",
       "  -2.7957\n",
       "      nan\n",
       "  -2.9362\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.2329\n",
       "  -4.4266\n",
       "      nan\n",
       "      nan\n",
       "  -0.2606\n",
       "  -3.4666\n",
       "      nan\n",
       "      nan\n",
       "  -1.2058\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.5019\n",
       "      nan\n",
       "  -4.4172\n",
       "  -6.1602\n",
       "      nan\n",
       "      nan\n",
       "  -6.2420\n",
       "  -2.5500\n",
       "  -3.2625\n",
       "      nan\n",
       "   1.0164\n",
       "      nan\n",
       "      nan\n",
       "  -3.8434\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -4.8186\n",
       "      nan\n",
       "      nan\n",
       "  -3.4898\n",
       "      nan\n",
       "   0.2512\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.6498\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -1.0217\n",
       "      nan\n",
       "  -5.3008\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -3.1326\n",
       "  -2.6170\n",
       "      nan\n",
       "  -4.0498\n",
       "  -2.6463\n",
       "  -1.0189\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.5027\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -6.6274\n",
       "      nan\n",
       "  -0.8971\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -2.0222\n",
       "      nan\n",
       "  -2.4058\n",
       "      nan\n",
       "  -2.6140\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "      nan\n",
       "  -0.5464\n",
       "      nan\n",
       "      nan\n",
       " [torch.cuda.FloatTensor of size 640 (GPU 0)], Parameter containing:\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "        ...          ⋱          ...       \n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       " [torch.cuda.FloatTensor of size 1920x640 (GPU 0)], Parameter containing:\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "        ...          ⋱          ...       \n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       "   nan   nan   nan  ...    nan   nan   nan\n",
       " [torch.cuda.FloatTensor of size 1920x640 (GPU 0)], Parameter containing:\n",
       " nan\n",
       " nan\n",
       " nan\n",
       "  ⋮ \n",
       " nan\n",
       " nan\n",
       " nan\n",
       " [torch.cuda.FloatTensor of size 1920 (GPU 0)], Parameter containing:\n",
       " nan\n",
       " nan\n",
       " nan\n",
       "  ⋮ \n",
       " nan\n",
       " nan\n",
       " nan\n",
       " [torch.cuda.FloatTensor of size 1920 (GPU 0)], Parameter containing:\n",
       "  7.0765e-02  2.4310e-02 -1.0471e+00  ...   1.1067e-01 -2.2757e-01 -2.1986e-01\n",
       "  6.7908e-02  3.0487e-02 -1.0279e+00  ...   1.0620e-01 -2.6346e-01 -1.9972e-01\n",
       "  4.5692e-02  3.8908e-02 -1.0359e+00  ...   1.3456e-01 -2.4776e-01 -2.3976e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.0599e-02 -5.8734e-03  1.9756e-01  ...   9.3505e-02 -4.0492e-02 -2.5251e-02\n",
       "  1.0969e-01 -7.2745e-02 -1.7533e-01  ...  -2.9232e-02 -2.3478e-01  4.6116e-02\n",
       "  2.2044e-01 -5.9714e-02 -9.5562e-01  ...  -3.6751e-01  2.1959e-01 -1.4441e-01\n",
       " [torch.cuda.FloatTensor of size 32x640 (GPU 0)], Parameter containing:\n",
       " -3.5139\n",
       " -3.4560\n",
       " -3.5496\n",
       "  0.5750\n",
       " -0.9265\n",
       "  0.1564\n",
       " -0.1996\n",
       " -0.0224\n",
       "  0.0682\n",
       "  0.4488\n",
       " -0.0646\n",
       " -0.3318\n",
       "  0.1963\n",
       "  0.1332\n",
       " -0.3560\n",
       " -0.2108\n",
       "  0.1365\n",
       " -0.1278\n",
       "  0.2791\n",
       "  0.4116\n",
       " -0.1258\n",
       " -0.1563\n",
       "  0.1983\n",
       "  0.2669\n",
       "  0.3074\n",
       "  0.1156\n",
       " -0.1120\n",
       " -0.1031\n",
       " -0.3519\n",
       " -0.2146\n",
       " -0.6710\n",
       " -0.6913\n",
       " [torch.cuda.FloatTensor of size 32 (GPU 0)]]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in decoder_rnn.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dict_path = 'data/models/whole_gen_13_only_chars_and_attn/200000_'\n",
    "\n",
    "decoder_rnn.load_state_dict(torch.load(state_dict_path + 'DecoderRNN'))\n",
    "encoder_rnn.load_state_dict(torch.load(state_dict_path + 'EncoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.0160 -0.0180  0.0727  ...  -0.0082 -0.1126  0.1833\n",
       "  0.0362  0.1504 -0.1483  ...   0.0192  0.0254 -0.1061\n",
       "  0.0463 -0.0354  0.0112  ...   0.0652  0.0557  0.0154\n",
       "           ...             ⋱             ...          \n",
       "  0.0565 -0.0500 -0.1276  ...   0.1339 -0.0295 -0.1846\n",
       "  0.0903 -0.0143  0.0025  ...   0.0004  0.2131 -0.0659\n",
       " -0.1217  0.1283  0.1447  ...  -0.1420 -0.0426 -0.0893\n",
       " [torch.cuda.FloatTensor of size 640x32 (GPU 0)], Parameter containing:\n",
       " -0.0050\n",
       " -0.0018\n",
       "  0.1565\n",
       " -0.0335\n",
       " -0.0937\n",
       "  0.0640\n",
       " -0.1053\n",
       "  0.0412\n",
       "  0.0524\n",
       "  0.0312\n",
       " -0.0904\n",
       " -0.0394\n",
       " -0.0696\n",
       "  0.0449\n",
       "  0.1610\n",
       " -0.0237\n",
       "  0.0710\n",
       "  0.0841\n",
       " -0.0801\n",
       " -0.0465\n",
       " -0.0946\n",
       " -0.0561\n",
       " -0.0086\n",
       "  0.1023\n",
       "  0.0728\n",
       "  0.0560\n",
       " -0.0618\n",
       " -0.1034\n",
       "  0.0572\n",
       " -0.0217\n",
       "  0.0586\n",
       "  0.1533\n",
       "  0.0727\n",
       " -0.0962\n",
       "  0.0953\n",
       "  0.0851\n",
       " -0.0310\n",
       " -0.0842\n",
       " -0.0061\n",
       " -0.1213\n",
       "  0.1314\n",
       " -0.1919\n",
       " -0.0804\n",
       "  0.0349\n",
       " -0.1110\n",
       "  0.0452\n",
       "  0.0658\n",
       "  0.0664\n",
       " -0.0486\n",
       "  0.0106\n",
       "  0.0187\n",
       " -0.1310\n",
       "  0.1380\n",
       " -0.1122\n",
       "  0.0577\n",
       "  0.1361\n",
       "  0.0823\n",
       " -0.0090\n",
       " -0.0073\n",
       " -0.0142\n",
       "  0.1066\n",
       " -0.0396\n",
       " -0.0262\n",
       " -0.0758\n",
       " -0.0502\n",
       " -0.0269\n",
       "  0.0978\n",
       " -0.1554\n",
       "  0.0060\n",
       "  0.1069\n",
       " -0.0325\n",
       "  0.0505\n",
       "  0.1050\n",
       "  0.0054\n",
       "  0.1974\n",
       "  0.0231\n",
       "  0.0517\n",
       "  0.0923\n",
       " -0.0185\n",
       " -0.1273\n",
       "  0.1389\n",
       " -0.1012\n",
       " -0.0210\n",
       "  0.1319\n",
       " -0.0918\n",
       " -0.0081\n",
       "  0.0175\n",
       " -0.1007\n",
       " -0.1399\n",
       " -0.0108\n",
       "  0.0205\n",
       "  0.0192\n",
       "  0.0217\n",
       " -0.0915\n",
       "  0.0219\n",
       "  0.1136\n",
       " -0.1443\n",
       " -0.0130\n",
       " -0.0064\n",
       "  0.1406\n",
       " -0.0384\n",
       "  0.0801\n",
       " -0.0756\n",
       " -0.0004\n",
       " -0.1303\n",
       "  0.0866\n",
       "  0.0541\n",
       " -0.0290\n",
       " -0.0028\n",
       "  0.0408\n",
       "  0.0530\n",
       "  0.0246\n",
       " -0.0764\n",
       " -0.0474\n",
       "  0.0595\n",
       "  0.0394\n",
       "  0.1153\n",
       " -0.0988\n",
       "  0.0072\n",
       " -0.0436\n",
       "  0.0985\n",
       "  0.1702\n",
       "  0.0740\n",
       "  0.0597\n",
       "  0.0702\n",
       " -0.1314\n",
       " -0.0886\n",
       "  0.0803\n",
       "  0.0829\n",
       " -0.0334\n",
       " -0.0330\n",
       "  0.0550\n",
       " -0.0806\n",
       "  0.1405\n",
       "  0.1031\n",
       "  0.0085\n",
       " -0.0565\n",
       " -0.0830\n",
       "  0.1488\n",
       " -0.1486\n",
       " -0.0232\n",
       "  0.0482\n",
       "  0.0284\n",
       " -0.1495\n",
       "  0.0893\n",
       "  0.1324\n",
       "  0.0552\n",
       " -0.1215\n",
       " -0.1474\n",
       "  0.1330\n",
       " -0.0698\n",
       "  0.0778\n",
       "  0.0009\n",
       " -0.1824\n",
       " -0.0866\n",
       " -0.1002\n",
       " -0.0306\n",
       "  0.0339\n",
       " -0.0247\n",
       "  0.1341\n",
       " -0.1244\n",
       " -0.0170\n",
       "  0.0821\n",
       "  0.1752\n",
       " -0.0068\n",
       " -0.0429\n",
       " -0.0609\n",
       " -0.1589\n",
       "  0.0494\n",
       " -0.0316\n",
       " -0.0203\n",
       "  0.1102\n",
       " -0.0345\n",
       "  0.1387\n",
       " -0.0991\n",
       " -0.0918\n",
       " -0.0257\n",
       " -0.1151\n",
       "  0.0619\n",
       "  0.0026\n",
       " -0.0895\n",
       " -0.0702\n",
       " -0.1020\n",
       " -0.0066\n",
       " -0.1269\n",
       " -0.0430\n",
       " -0.0741\n",
       "  0.0090\n",
       "  0.0448\n",
       "  0.1176\n",
       "  0.0602\n",
       "  0.0622\n",
       "  0.0602\n",
       " -0.0615\n",
       "  0.0579\n",
       " -0.0221\n",
       " -0.0836\n",
       "  0.1333\n",
       "  0.0383\n",
       " -0.0279\n",
       "  0.1266\n",
       " -0.0421\n",
       "  0.0070\n",
       "  0.0355\n",
       "  0.0382\n",
       " -0.0696\n",
       " -0.0881\n",
       " -0.0602\n",
       "  0.0204\n",
       "  0.0644\n",
       "  0.0954\n",
       " -0.0858\n",
       " -0.0052\n",
       " -0.0350\n",
       "  0.0666\n",
       " -0.0471\n",
       "  0.1347\n",
       " -0.0187\n",
       "  0.1332\n",
       " -0.0265\n",
       " -0.0741\n",
       "  0.0214\n",
       "  0.0367\n",
       "  0.0131\n",
       " -0.1609\n",
       "  0.0075\n",
       " -0.0964\n",
       "  0.0188\n",
       " -0.1106\n",
       " -0.1209\n",
       "  0.0171\n",
       "  0.0233\n",
       "  0.0515\n",
       "  0.1034\n",
       " -0.1012\n",
       "  0.1174\n",
       "  0.1937\n",
       "  0.0300\n",
       " -0.0128\n",
       "  0.0520\n",
       "  0.0396\n",
       "  0.0797\n",
       " -0.1167\n",
       "  0.1286\n",
       "  0.0400\n",
       " -0.0414\n",
       "  0.0349\n",
       " -0.0146\n",
       "  0.1059\n",
       "  0.1316\n",
       "  0.0653\n",
       " -0.0967\n",
       " -0.0813\n",
       "  0.0691\n",
       "  0.0634\n",
       " -0.1437\n",
       " -0.0859\n",
       " -0.0534\n",
       " -0.0013\n",
       "  0.0978\n",
       " -0.0921\n",
       " -0.0609\n",
       " -0.0764\n",
       "  0.0669\n",
       "  0.1512\n",
       "  0.1505\n",
       " -0.0648\n",
       " -0.0842\n",
       " -0.1329\n",
       " -0.0353\n",
       "  0.0681\n",
       "  0.0043\n",
       " -0.0152\n",
       "  0.0207\n",
       "  0.1267\n",
       " -0.2183\n",
       " -0.1293\n",
       " -0.1254\n",
       " -0.0180\n",
       " -0.0239\n",
       " -0.0174\n",
       "  0.0975\n",
       "  0.0047\n",
       " -0.0693\n",
       "  0.0038\n",
       " -0.0212\n",
       "  0.0566\n",
       " -0.0625\n",
       " -0.0503\n",
       "  0.1776\n",
       " -0.1120\n",
       "  0.1414\n",
       "  0.1395\n",
       " -0.0012\n",
       " -0.0412\n",
       " -0.0878\n",
       " -0.0012\n",
       "  0.1723\n",
       " -0.0074\n",
       "  0.0858\n",
       "  0.0713\n",
       "  0.0189\n",
       " -0.0633\n",
       " -0.0667\n",
       "  0.1473\n",
       "  0.0732\n",
       " -0.0152\n",
       " -0.0839\n",
       "  0.0986\n",
       " -0.0315\n",
       "  0.1389\n",
       "  0.0262\n",
       "  0.0329\n",
       "  0.0955\n",
       " -0.0270\n",
       " -0.0981\n",
       "  0.1225\n",
       " -0.0862\n",
       " -0.0786\n",
       "  0.0361\n",
       "  0.0749\n",
       " -0.0806\n",
       " -0.0812\n",
       "  0.0788\n",
       " -0.0391\n",
       "  0.0085\n",
       "  0.0313\n",
       " -0.0606\n",
       " -0.1249\n",
       " -0.0491\n",
       " -0.1336\n",
       "  0.0165\n",
       "  0.0540\n",
       "  0.0753\n",
       "  0.0913\n",
       " -0.1079\n",
       " -0.0669\n",
       "  0.0204\n",
       " -0.0970\n",
       " -0.0371\n",
       " -0.1516\n",
       "  0.1165\n",
       "  0.0356\n",
       " -0.1338\n",
       "  0.0109\n",
       " -0.0114\n",
       " -0.0332\n",
       " -0.0154\n",
       "  0.0739\n",
       " -0.1413\n",
       "  0.0425\n",
       " -0.0088\n",
       "  0.0263\n",
       " -0.1056\n",
       "  0.0290\n",
       " -0.0949\n",
       " -0.0488\n",
       "  0.1674\n",
       "  0.1221\n",
       " -0.0732\n",
       " -0.0391\n",
       " -0.0134\n",
       " -0.0512\n",
       " -0.1037\n",
       "  0.0766\n",
       "  0.0621\n",
       " -0.1696\n",
       "  0.0352\n",
       " -0.0994\n",
       " -0.0624\n",
       "  0.1033\n",
       "  0.0476\n",
       " -0.0244\n",
       " -0.0431\n",
       " -0.0116\n",
       " -0.0221\n",
       " -0.0659\n",
       " -0.0417\n",
       " -0.0446\n",
       "  0.0426\n",
       " -0.1001\n",
       "  0.0500\n",
       " -0.0360\n",
       " -0.0055\n",
       "  0.0174\n",
       "  0.0035\n",
       " -0.0580\n",
       "  0.0064\n",
       "  0.0698\n",
       " -0.0980\n",
       " -0.0593\n",
       " -0.0789\n",
       " -0.1395\n",
       " -0.0036\n",
       "  0.1078\n",
       " -0.0920\n",
       "  0.0962\n",
       "  0.0411\n",
       " -0.1206\n",
       "  0.1241\n",
       " -0.1150\n",
       " -0.1557\n",
       "  0.0095\n",
       "  0.0548\n",
       "  0.1463\n",
       " -0.0921\n",
       " -0.0650\n",
       " -0.1611\n",
       " -0.0425\n",
       "  0.0036\n",
       "  0.0473\n",
       "  0.0161\n",
       " -0.0319\n",
       " -0.0251\n",
       "  0.0966\n",
       " -0.1071\n",
       "  0.0173\n",
       " -0.0505\n",
       "  0.0405\n",
       " -0.0834\n",
       "  0.1448\n",
       " -0.0868\n",
       "  0.0127\n",
       "  0.0580\n",
       " -0.0427\n",
       " -0.0580\n",
       "  0.0075\n",
       "  0.1193\n",
       " -0.0232\n",
       "  0.0873\n",
       "  0.0639\n",
       "  0.0031\n",
       " -0.0645\n",
       " -0.0789\n",
       "  0.0068\n",
       "  0.0842\n",
       " -0.0878\n",
       " -0.0429\n",
       "  0.1473\n",
       "  0.1557\n",
       "  0.1191\n",
       " -0.0376\n",
       " -0.1269\n",
       " -0.1108\n",
       "  0.1091\n",
       "  0.0383\n",
       "  0.0645\n",
       " -0.0579\n",
       " -0.0536\n",
       "  0.0679\n",
       " -0.0441\n",
       "  0.0832\n",
       "  0.0395\n",
       " -0.0687\n",
       " -0.0912\n",
       "  0.0313\n",
       " -0.1058\n",
       " -0.0238\n",
       " -0.0612\n",
       "  0.1023\n",
       " -0.0758\n",
       " -0.0783\n",
       " -0.0869\n",
       "  0.0734\n",
       "  0.0869\n",
       " -0.1404\n",
       "  0.0093\n",
       "  0.1194\n",
       " -0.0153\n",
       "  0.0963\n",
       " -0.0216\n",
       "  0.0711\n",
       " -0.1323\n",
       " -0.0477\n",
       " -0.0120\n",
       "  0.1198\n",
       " -0.0949\n",
       "  0.1254\n",
       "  0.0456\n",
       " -0.0652\n",
       "  0.0271\n",
       " -0.1543\n",
       "  0.0654\n",
       "  0.1508\n",
       "  0.0848\n",
       "  0.0384\n",
       " -0.0182\n",
       "  0.0056\n",
       "  0.1040\n",
       "  0.0273\n",
       "  0.0246\n",
       " -0.0189\n",
       " -0.0202\n",
       " -0.1577\n",
       " -0.1395\n",
       " -0.0612\n",
       "  0.0122\n",
       "  0.0764\n",
       "  0.0473\n",
       "  0.1643\n",
       " -0.0245\n",
       " -0.0240\n",
       " -0.0719\n",
       " -0.1198\n",
       "  0.1450\n",
       " -0.0126\n",
       "  0.0167\n",
       " -0.0808\n",
       " -0.1476\n",
       " -0.0634\n",
       "  0.1360\n",
       "  0.0037\n",
       "  0.0588\n",
       "  0.0344\n",
       "  0.1087\n",
       "  0.0696\n",
       "  0.1450\n",
       " -0.0877\n",
       "  0.1078\n",
       "  0.0280\n",
       " -0.0723\n",
       " -0.0590\n",
       "  0.0844\n",
       "  0.0156\n",
       " -0.0610\n",
       "  0.0205\n",
       "  0.0340\n",
       "  0.1030\n",
       "  0.0117\n",
       " -0.0629\n",
       " -0.0731\n",
       "  0.0142\n",
       " -0.0240\n",
       " -0.0681\n",
       "  0.0478\n",
       "  0.0290\n",
       " -0.1037\n",
       " -0.1020\n",
       " -0.0620\n",
       " -0.0288\n",
       "  0.1088\n",
       "  0.1075\n",
       "  0.0588\n",
       " -0.0905\n",
       "  0.1061\n",
       " -0.0149\n",
       "  0.1317\n",
       "  0.0966\n",
       " -0.0826\n",
       "  0.0642\n",
       " -0.0277\n",
       " -0.0435\n",
       "  0.0157\n",
       " -0.0370\n",
       " -0.0765\n",
       "  0.0125\n",
       " -0.0893\n",
       "  0.0438\n",
       "  0.0013\n",
       "  0.0441\n",
       "  0.0633\n",
       " -0.0302\n",
       "  0.0139\n",
       " -0.0034\n",
       " -0.1236\n",
       "  0.0064\n",
       " -0.0975\n",
       "  0.0723\n",
       " -0.0142\n",
       " -0.0407\n",
       " -0.0674\n",
       "  0.0894\n",
       "  0.0253\n",
       "  0.0761\n",
       "  0.1018\n",
       " -0.0586\n",
       " -0.0102\n",
       "  0.1092\n",
       "  0.0040\n",
       "  0.0500\n",
       "  0.1220\n",
       "  0.1456\n",
       " -0.0868\n",
       " -0.0768\n",
       " -0.0278\n",
       "  0.0052\n",
       " -0.0365\n",
       " -0.0108\n",
       "  0.0391\n",
       " -0.0799\n",
       " -0.1145\n",
       "  0.0350\n",
       " -0.0656\n",
       "  0.0838\n",
       "  0.0108\n",
       "  0.0204\n",
       "  0.0254\n",
       "  0.1077\n",
       "  0.0229\n",
       " -0.0877\n",
       "  0.0095\n",
       "  0.0628\n",
       "  0.0899\n",
       " -0.1163\n",
       "  0.0764\n",
       "  0.0625\n",
       " -0.0825\n",
       "  0.0529\n",
       "  0.1273\n",
       "  0.1232\n",
       " -0.1403\n",
       "  0.0976\n",
       " -0.1136\n",
       "  0.0137\n",
       " -0.0641\n",
       "  0.0451\n",
       " -0.1182\n",
       " -0.1307\n",
       " -0.0281\n",
       "  0.0532\n",
       " -0.0267\n",
       "  0.0250\n",
       "  0.0945\n",
       "  0.0009\n",
       "  0.0318\n",
       " -0.0592\n",
       " -0.0248\n",
       " -0.0892\n",
       "  0.1813\n",
       " -0.0289\n",
       " -0.0253\n",
       "  0.0055\n",
       "  0.0324\n",
       "  0.0931\n",
       "  0.0023\n",
       "  0.1645\n",
       " -0.0166\n",
       " -0.0132\n",
       " -0.0967\n",
       "  0.1240\n",
       " [torch.cuda.FloatTensor of size 640 (GPU 0)], Parameter containing:\n",
       "  5.8820e-02  4.5329e-02  9.8724e-03  ...   3.6963e-02 -6.0421e-03  2.4835e-02\n",
       " -6.3256e-02  6.4673e-02  1.8348e-02  ...   2.4074e-02 -5.4784e-02 -4.3046e-02\n",
       "  2.3115e-02  4.5728e-02  1.5407e-02  ...  -4.4041e-02 -6.8073e-04  3.5069e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.8371e-02  4.5679e-02  4.4937e-02  ...   1.4015e-02 -2.7366e-03 -6.4554e-02\n",
       "  6.7779e-03 -2.3662e-02 -1.1400e-02  ...  -5.2643e-03 -9.8850e-03 -4.8205e-02\n",
       "  1.9814e-02 -3.1712e-02  6.6768e-02  ...   5.2106e-02 -5.6612e-02 -3.5168e-02\n",
       " [torch.cuda.FloatTensor of size 30x1280 (GPU 0)], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "   0.0100\n",
       "   3.1643\n",
       "   0.1218\n",
       "   2.9276\n",
       "  -0.9576\n",
       "  -1.5274\n",
       "  -3.5961\n",
       "   1.5704\n",
       "  -2.9383\n",
       "  -1.8513\n",
       "   1.9218\n",
       "  -0.2285\n",
       "  -0.2170\n",
       "   1.8534\n",
       "   2.8647\n",
       "  -3.2033\n",
       "   4.7280\n",
       "   4.2500\n",
       "   2.9298\n",
       "  -3.7394\n",
       "  -1.0745\n",
       "  -0.0325\n",
       "   2.3684\n",
       "   3.4102\n",
       "  -1.3243\n",
       "  -1.7904\n",
       "   0.0173\n",
       "   1.5908\n",
       "   0.0410\n",
       "   6.2332\n",
       " [torch.cuda.FloatTensor of size 30 (GPU 0)], Parameter containing:\n",
       " -2.3456e-03 -2.7567e-03  8.3503e-03  ...   7.9685e-03 -2.3062e-02  1.3863e-02\n",
       " -1.7936e-02 -1.8162e-02 -1.8305e-02  ...  -1.6188e-02  4.0675e-03  2.4864e-02\n",
       " -3.6198e-02 -1.3807e-02  6.0870e-02  ...   7.4700e-02 -6.8882e-03 -8.9696e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -3.6508e-02 -9.5091e-03 -2.5014e-02  ...   2.9037e-02  4.4708e-02  3.3485e-02\n",
       "  1.8033e-02 -2.3832e-02 -1.7502e-02  ...   1.7973e-02 -1.3058e-02 -4.4886e-02\n",
       "  4.3285e-03 -1.3709e-03 -2.1609e-02  ...  -3.4808e-02 -1.9885e-02  2.6167e-02\n",
       " [torch.cuda.FloatTensor of size 640x1152 (GPU 0)], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -3.6034\n",
       "  -0.0217\n",
       "  -1.0667\n",
       "   0.5518\n",
       "  -2.3957\n",
       "  -3.4901\n",
       "   0.2719\n",
       "  -5.8183\n",
       "  -2.5460\n",
       "   0.4494\n",
       "  -1.1740\n",
       "   0.6653\n",
       "   1.6097\n",
       "   1.3794\n",
       "  -1.4325\n",
       "  -3.8716\n",
       "  -2.2531\n",
       "  -1.0743\n",
       "  -2.3870\n",
       "  -3.4327\n",
       "  -0.9175\n",
       "  -2.5435\n",
       "   1.2441\n",
       "  -0.7482\n",
       "  -2.1734\n",
       "   2.5611\n",
       "   0.0027\n",
       "  -0.4712\n",
       "   0.1846\n",
       "  -2.5191\n",
       "  -1.6052\n",
       "  -5.7278\n",
       "  -0.0488\n",
       "   1.2087\n",
       "   2.3284\n",
       "   0.8125\n",
       "   1.0152\n",
       "  -0.6394\n",
       "  -3.6522\n",
       "  -0.2580\n",
       "  -3.7633\n",
       "   2.0633\n",
       "  -3.8261\n",
       "  -5.8596\n",
       "  -2.0808\n",
       "  -3.0533\n",
       "   0.1304\n",
       "  -1.2459\n",
       "   3.9836\n",
       "   1.5309\n",
       "  -2.7430\n",
       "  -1.1526\n",
       "  -2.2544\n",
       "   0.8486\n",
       "  -0.9584\n",
       "  -1.7285\n",
       "  -3.1403\n",
       "  -3.4558\n",
       "   2.4846\n",
       "  -0.8253\n",
       "   0.2168\n",
       "  -1.1162\n",
       "  -3.7902\n",
       "  -2.4205\n",
       "  -2.3762\n",
       "  -3.6992\n",
       "   1.5636\n",
       "   1.8427\n",
       "  -2.0218\n",
       "   1.5216\n",
       "  -3.0170\n",
       "   2.8364\n",
       "  -0.6391\n",
       "  -0.5055\n",
       "   0.3663\n",
       "  -0.7853\n",
       "   1.7365\n",
       "  -3.4178\n",
       "   0.0721\n",
       "  -2.0344\n",
       "  -0.8162\n",
       "   2.0344\n",
       "  -0.0729\n",
       "  -1.4924\n",
       "  -2.1811\n",
       "  -3.1145\n",
       "   0.2852\n",
       "  -1.9469\n",
       "   1.4951\n",
       "   2.3151\n",
       "  -3.9758\n",
       "   0.1249\n",
       "  -4.9974\n",
       "   1.1126\n",
       "  -2.8002\n",
       "   0.6327\n",
       "  -1.3526\n",
       "   0.9930\n",
       "   2.2390\n",
       "  -1.1960\n",
       "  -0.4049\n",
       "  -4.1171\n",
       "  -3.6702\n",
       "  -3.9638\n",
       "   1.2046\n",
       "  -0.2629\n",
       "   0.2305\n",
       "   0.2182\n",
       "  -5.6631\n",
       "  -2.0566\n",
       "  -0.6615\n",
       "  -0.8779\n",
       "  -4.7029\n",
       "  -5.1182\n",
       "   0.0569\n",
       "   1.5444\n",
       "  -3.1001\n",
       "  -0.5940\n",
       "  -3.0581\n",
       "  -3.2147\n",
       "   2.0177\n",
       "   3.4175\n",
       "   2.3239\n",
       "   0.8646\n",
       "   0.1495\n",
       "   2.8252\n",
       "  -1.5361\n",
       "  -0.2491\n",
       "  -6.8801\n",
       "  -1.1405\n",
       "  -2.7121\n",
       "  -2.1503\n",
       "  -4.3704\n",
       "   1.7835\n",
       "  -1.0868\n",
       "  -1.3669\n",
       "  -3.5163\n",
       "   2.6537\n",
       "  -0.6980\n",
       "  -0.6076\n",
       "  -2.1512\n",
       "  -4.5428\n",
       "   3.4294\n",
       "  -3.0264\n",
       "   0.1234\n",
       "  -2.2258\n",
       "   1.1415\n",
       "  -2.2514\n",
       "  -0.6739\n",
       "  -3.1586\n",
       "  -5.2021\n",
       "  -0.0868\n",
       "   2.0983\n",
       "   0.3668\n",
       "  -2.2928\n",
       "  -1.4637\n",
       "  -2.4238\n",
       "  -2.0229\n",
       "   0.8978\n",
       "  -1.5866\n",
       "  -0.5715\n",
       "  -0.1219\n",
       "  -3.1979\n",
       "   0.0228\n",
       "  -2.7499\n",
       "   0.4823\n",
       "  -1.3706\n",
       "   0.5450\n",
       "  -0.7562\n",
       "  -0.8509\n",
       "   1.2758\n",
       "  -4.3331\n",
       "  -4.5012\n",
       "  -4.8290\n",
       "   2.8116\n",
       "  -2.2125\n",
       "   1.3647\n",
       "  -0.4201\n",
       "  -3.4964\n",
       "   2.5176\n",
       "  -2.2971\n",
       "  -0.0660\n",
       "   0.6580\n",
       "   0.4765\n",
       "  -0.6970\n",
       "  -2.6703\n",
       "  -3.4242\n",
       "  -1.1805\n",
       "   1.1223\n",
       "  -1.9008\n",
       "  -5.0092\n",
       "  -5.0542\n",
       "   2.6750\n",
       "  -3.0239\n",
       "  -3.3234\n",
       "   1.4638\n",
       "  -1.0887\n",
       "  -0.8055\n",
       "  -4.9540\n",
       "   0.3450\n",
       "  -2.0853\n",
       "   0.7205\n",
       "  -0.6941\n",
       "   0.0660\n",
       "  -0.9503\n",
       "  -4.3129\n",
       "  -4.9428\n",
       "  -2.3634\n",
       "  -3.1163\n",
       "  -2.9520\n",
       "  -2.9809\n",
       "  -2.0535\n",
       "  -3.0000\n",
       "  -0.4877\n",
       "  -5.3697\n",
       "  -2.9373\n",
       "  -1.7854\n",
       "   0.2400\n",
       "   0.3259\n",
       "  -2.2244\n",
       "  -4.7054\n",
       "  -3.8108\n",
       "  -0.5122\n",
       "   1.5734\n",
       "   2.4463\n",
       "  -0.5580\n",
       "   0.6756\n",
       "  -2.4605\n",
       "   0.8533\n",
       "  -2.2162\n",
       "  -2.1267\n",
       "  -2.5771\n",
       "  -1.8586\n",
       "  -4.5940\n",
       "   0.4891\n",
       "  -1.3568\n",
       "  -5.7528\n",
       "  -3.2776\n",
       "   1.0109\n",
       "  -0.4939\n",
       "  -0.3837\n",
       "  -1.6363\n",
       "   0.7124\n",
       "  -0.6109\n",
       "  -4.2504\n",
       "  -0.8555\n",
       "  -1.7589\n",
       "  -0.8443\n",
       "   1.7308\n",
       "  -2.4334\n",
       "  -2.2538\n",
       "  -4.6191\n",
       "   3.7279\n",
       "   0.6887\n",
       "  -1.5123\n",
       "   3.3595\n",
       "   0.5948\n",
       "  -4.2387\n",
       "  -2.5404\n",
       "   0.4585\n",
       "   0.0123\n",
       "  -1.8200\n",
       "   4.2271\n",
       "  -4.4570\n",
       "  -1.8579\n",
       "  -4.3341\n",
       "  -2.9897\n",
       "   0.8396\n",
       "   0.3836\n",
       "  -0.0573\n",
       "  -2.3748\n",
       "  -2.1835\n",
       "  -0.9395\n",
       "  -4.4365\n",
       "  -4.7178\n",
       "  -4.2934\n",
       "   2.4339\n",
       "   0.2754\n",
       "  -1.4153\n",
       "   2.8981\n",
       "  -1.3391\n",
       "  -3.3925\n",
       "   0.3478\n",
       "  -0.7734\n",
       "  -3.6254\n",
       "  -0.0703\n",
       "  -1.0875\n",
       "  -2.4598\n",
       "   1.0413\n",
       "  -1.4164\n",
       "  -0.9870\n",
       "  -2.6164\n",
       "  -1.8346\n",
       "  -1.1205\n",
       "   1.9883\n",
       "   0.3515\n",
       "  -1.9083\n",
       "  -0.0916\n",
       "  -0.6290\n",
       "   0.0126\n",
       "  -2.0340\n",
       "  -1.2130\n",
       "  -1.6964\n",
       "  -4.9112\n",
       "   1.7189\n",
       "  -0.6655\n",
       "  -3.1730\n",
       "  -0.0241\n",
       "  -0.9331\n",
       "   1.2531\n",
       "   0.7327\n",
       "  -3.3226\n",
       "   1.1644\n",
       "  -1.5661\n",
       "   0.7823\n",
       "  -2.9711\n",
       "  -1.3107\n",
       "  -0.5970\n",
       "  -3.8423\n",
       "   0.2890\n",
       "   1.4583\n",
       "   1.2590\n",
       "   1.9182\n",
       "  -2.7459\n",
       "  -1.3726\n",
       "   0.8089\n",
       "   1.5498\n",
       "   1.4200\n",
       "  -2.9306\n",
       "   1.2704\n",
       "  -0.4693\n",
       "   1.8101\n",
       "  -1.0090\n",
       "  -1.8039\n",
       "  -2.9097\n",
       "   0.5489\n",
       "   2.2309\n",
       "  -0.2105\n",
       "   0.1937\n",
       "   0.7373\n",
       "   1.3649\n",
       "   0.5662\n",
       "  -0.9325\n",
       "  -1.2965\n",
       "  -1.5120\n",
       "   3.0708\n",
       "  -0.6288\n",
       "   0.1513\n",
       "  -1.4638\n",
       "  -0.2845\n",
       "  -3.5866\n",
       "  -0.1206\n",
       "  -2.2906\n",
       "  -1.9454\n",
       "  -4.4543\n",
       "  -5.0724\n",
       "   1.7633\n",
       "   3.0570\n",
       "   1.3314\n",
       "  -3.5503\n",
       "   4.6493\n",
       "  -0.8623\n",
       "   1.0191\n",
       "   1.0675\n",
       "  -0.0059\n",
       "  -5.5326\n",
       "  -4.1666\n",
       "   1.7123\n",
       "   0.8787\n",
       "   2.1559\n",
       "  -1.5736\n",
       "  -2.6937\n",
       "   2.6763\n",
       "  -2.3970\n",
       "  -2.0477\n",
       "   1.4393\n",
       "  -0.6855\n",
       "   2.2472\n",
       "  -2.4296\n",
       "  -1.6909\n",
       "  -4.3652\n",
       "  -1.6528\n",
       "   0.4695\n",
       "  -2.6982\n",
       "  -2.1574\n",
       "  -4.3950\n",
       "  -0.8600\n",
       "  -0.4298\n",
       "  -1.3545\n",
       "   5.6703\n",
       "   0.7740\n",
       "  -3.5944\n",
       "   1.1193\n",
       "  -3.6153\n",
       "  -1.5466\n",
       "  -1.3129\n",
       "  -3.7360\n",
       "  -2.2451\n",
       "  -2.5420\n",
       "  -0.0192\n",
       "   2.5566\n",
       "  -1.0481\n",
       "  -3.0259\n",
       "  -0.3591\n",
       "  -2.2717\n",
       "  -5.9157\n",
       "  -2.2800\n",
       "  -3.6014\n",
       "   2.9112\n",
       "  -1.7186\n",
       "   4.3679\n",
       "  -2.0680\n",
       "  -5.2332\n",
       "  -2.8929\n",
       "  -1.1494\n",
       "  -2.7390\n",
       "  -2.1665\n",
       "  -3.6514\n",
       "   3.0348\n",
       "  -2.0241\n",
       "  -0.8434\n",
       "  -3.8431\n",
       "  -0.3526\n",
       "  -0.5701\n",
       "  -2.1854\n",
       "  -0.2471\n",
       "  -1.0392\n",
       "   0.3521\n",
       "   0.4539\n",
       "  -0.0698\n",
       "   2.9301\n",
       "  -4.3997\n",
       "  -1.6798\n",
       "  -3.7596\n",
       "   0.0640\n",
       "  -3.1238\n",
       "  -3.0864\n",
       "  -1.5017\n",
       "  -2.9401\n",
       "  -2.2718\n",
       "  -2.1896\n",
       "  -4.3173\n",
       "   2.9040\n",
       "  -2.9918\n",
       "  -2.4132\n",
       "  -0.0013\n",
       "  -0.1607\n",
       "   0.0509\n",
       "  -2.3398\n",
       "  -0.1707\n",
       "  -0.8101\n",
       "   0.1314\n",
       "  -5.8103\n",
       "   0.3284\n",
       "  -1.1181\n",
       "  -0.7888\n",
       "   3.6666\n",
       "  -0.2917\n",
       "  -2.8944\n",
       "  -0.9587\n",
       "  -4.4960\n",
       "  -1.4638\n",
       "  -1.8993\n",
       "  -2.8050\n",
       "  -3.9889\n",
       "   0.1763\n",
       "  -1.2518\n",
       "  -1.6191\n",
       "   0.7740\n",
       "   3.1538\n",
       "  -5.7144\n",
       "  -5.2554\n",
       "  -2.6557\n",
       "   0.0965\n",
       "  -3.2348\n",
       "   0.1165\n",
       "   0.0691\n",
       "  -1.1809\n",
       "  -2.3300\n",
       "  -2.4811\n",
       "  -1.4577\n",
       "  -4.7472\n",
       "  -1.6649\n",
       "  -2.7783\n",
       "  -0.4763\n",
       "  -1.9674\n",
       "   2.4453\n",
       "  -4.5508\n",
       "  -1.7631\n",
       "   1.1265\n",
       "  -3.6234\n",
       "  -3.2428\n",
       "  -4.7244\n",
       "  -1.1304\n",
       "  -1.3296\n",
       "  -0.1616\n",
       "  -6.0145\n",
       "  -0.3696\n",
       "  -1.4519\n",
       "  -1.9069\n",
       "  -3.7976\n",
       "   0.4820\n",
       "  -2.4315\n",
       "  -1.3032\n",
       "  -0.5730\n",
       "  -3.0247\n",
       "  -0.8079\n",
       "  -3.1547\n",
       "   1.8260\n",
       "   1.7220\n",
       "  -1.6432\n",
       "  -2.4949\n",
       "  -0.3968\n",
       "  -1.2851\n",
       "  -4.3678\n",
       "  -0.3539\n",
       "   1.2108\n",
       "   1.5256\n",
       "  -0.0991\n",
       "  -1.8544\n",
       "  -0.7698\n",
       "   1.2963\n",
       "  -0.3517\n",
       "  -2.7957\n",
       "   0.4390\n",
       "  -4.5645\n",
       "  -2.6978\n",
       "  -1.7062\n",
       "  -0.5289\n",
       "  -0.2026\n",
       "  -4.8155\n",
       "  -0.2041\n",
       "  -0.2001\n",
       "  -0.4915\n",
       "  -2.8996\n",
       "   0.8193\n",
       "   0.8404\n",
       "  -1.2058\n",
       "  -2.8806\n",
       "  -3.2004\n",
       "   1.1848\n",
       "  -0.4011\n",
       "  -3.8186\n",
       "  -3.0714\n",
       "   1.9282\n",
       "  -3.1187\n",
       "  -1.9379\n",
       "  -0.7934\n",
       "  -3.0223\n",
       "  -1.7041\n",
       "  -2.4348\n",
       "   0.5401\n",
       "  -2.5019\n",
       "  -3.0890\n",
       "  -4.4172\n",
       "  -5.9724\n",
       "  -3.0063\n",
       "  -2.4900\n",
       "  -6.1539\n",
       "  -2.2323\n",
       "  -2.6348\n",
       "  -0.6135\n",
       "   1.1689\n",
       "   1.9131\n",
       "  -3.3829\n",
       "  -4.1261\n",
       "  -1.1765\n",
       "   0.7987\n",
       "   0.9088\n",
       "   2.4651\n",
       "  -4.8186\n",
       "   0.8451\n",
       "   2.9425\n",
       "  -3.4898\n",
       "   2.2909\n",
       "   0.2512\n",
       "  -4.5403\n",
       "   1.1212\n",
       "  -3.5881\n",
       "  -1.2277\n",
       "  -3.5318\n",
       "  -0.0350\n",
       "  -1.4290\n",
       "   2.2893\n",
       "   0.4808\n",
       "   2.6778\n",
       "   0.2936\n",
       "   2.2190\n",
       "  -5.7288\n",
       "  -2.5495\n",
       "   0.1835\n",
       "  -0.5875\n",
       "  -2.9596\n",
       "  -3.5951\n",
       "  -1.9480\n",
       "  -3.6572\n",
       "  -2.6170\n",
       "  -2.6107\n",
       "  -2.6544\n",
       "  -2.1924\n",
       "  -0.8941\n",
       "   0.7436\n",
       "  -2.3875\n",
       "  -2.0274\n",
       "   1.4991\n",
       "   0.5112\n",
       "  -4.2517\n",
       "  -2.5338\n",
       "  -2.3767\n",
       "   1.9084\n",
       "  -0.5565\n",
       "   0.8925\n",
       "   1.8642\n",
       "   1.3887\n",
       "   2.4487\n",
       "  -4.3084\n",
       "  -3.7770\n",
       "  -0.8971\n",
       "  -4.0341\n",
       "  -2.9637\n",
       "   1.0998\n",
       "   3.4243\n",
       "  -0.2320\n",
       "  -3.2893\n",
       "  -1.5041\n",
       "  -2.6277\n",
       "  -2.0219\n",
       "  -2.2357\n",
       "  -2.2933\n",
       "   3.9372\n",
       "  -1.8948\n",
       "  -1.6997\n",
       "   0.7079\n",
       "  -0.9887\n",
       "  -2.1491\n",
       "  -1.3578\n",
       "  -2.4245\n",
       "  -0.3642\n",
       "  -4.7836\n",
       "  -1.8435\n",
       " [torch.cuda.FloatTensor of size 640 (GPU 0)], Parameter containing:\n",
       " -7.0615e-02  3.2277e-02  5.6453e-02  ...  -2.6387e-02 -4.0923e-01 -3.3449e-01\n",
       " -9.5614e-02 -1.1264e-01 -1.8380e-01  ...   6.3628e-01  2.1316e-01  2.2173e-01\n",
       "  3.5631e-02 -1.4437e-01 -1.7353e-01  ...   3.2773e-02  9.9059e-02 -3.4335e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  3.5808e-02  3.3251e-01 -1.2092e-01  ...   5.6435e-01 -3.3065e-01 -7.2584e-01\n",
       " -2.1099e-02 -4.6097e-01 -1.4307e-01  ...  -3.0913e-01 -3.0478e-03 -2.2976e-01\n",
       " -1.2346e-02 -5.6129e-01 -3.0025e-01  ...   7.8145e-02  3.4092e-01  2.2109e-01\n",
       " [torch.cuda.FloatTensor of size 1920x640 (GPU 0)], Parameter containing:\n",
       "  3.4125e-01 -1.4520e-01 -3.8754e-01  ...   1.1414e-01  1.7396e-01 -1.6926e-01\n",
       "  2.5837e-01  1.5592e-01  1.0332e-01  ...  -1.1518e-01  2.2575e-01  1.6473e-01\n",
       " -5.4255e-02  1.0465e-01  9.0217e-02  ...  -2.3464e-01  1.3105e-01 -1.0494e-01\n",
       "                 ...                   ⋱                   ...                \n",
       " -4.2899e-01  7.4367e-01 -5.5740e-01  ...  -6.7430e-01  7.6331e-01  4.3812e-01\n",
       " -1.4817e-01 -6.7403e-01  8.5435e-02  ...  -4.4526e-01 -3.4255e-01 -2.5332e-01\n",
       "  7.7031e-02  6.1914e-02 -1.0800e-02  ...   3.2144e-02 -2.7340e-02  2.9648e-01\n",
       " [torch.cuda.FloatTensor of size 1920x640 (GPU 0)], Parameter containing:\n",
       " -3.3816e-01\n",
       " -1.5172e-01\n",
       " -4.6110e-01\n",
       "      ⋮     \n",
       "  3.7861e-01\n",
       " -2.1231e-02\n",
       "  1.9076e-01\n",
       " [torch.cuda.FloatTensor of size 1920 (GPU 0)], Parameter containing:\n",
       " -0.3045\n",
       " -0.1016\n",
       " -0.4891\n",
       "    ⋮   \n",
       " -0.1147\n",
       "  0.1558\n",
       " -0.0322\n",
       " [torch.cuda.FloatTensor of size 1920 (GPU 0)], Parameter containing:\n",
       "  7.0603e-02  2.5024e-02 -1.0472e+00  ...   1.0483e-01 -2.2564e-01 -2.1447e-01\n",
       "  6.7596e-02  2.8734e-02 -1.0267e+00  ...   9.6166e-02 -2.6013e-01 -1.9035e-01\n",
       "  4.5309e-02  3.7634e-02 -1.0348e+00  ...   1.2616e-01 -2.4437e-01 -2.3179e-01\n",
       "                 ...                   ⋱                   ...                \n",
       " -1.1320e-01  1.3031e-02  2.3070e-01  ...   2.2895e-02 -9.8163e-02  9.9686e-02\n",
       " -1.0510e-01 -4.5805e-02 -1.8763e-01  ...  -8.2396e-03 -1.9755e-01  8.9396e-02\n",
       " -1.5417e-02 -1.5446e-01 -8.9648e-01  ...  -3.6871e-01  2.7306e-01 -1.6853e-01\n",
       " [torch.cuda.FloatTensor of size 32x640 (GPU 0)], Parameter containing:\n",
       " -3.5073\n",
       " -3.4447\n",
       " -3.5401\n",
       "  0.5060\n",
       " -0.8919\n",
       "  0.1233\n",
       " -0.2009\n",
       " -0.0367\n",
       "  0.0320\n",
       "  0.3577\n",
       " -0.0815\n",
       " -0.3525\n",
       "  0.1692\n",
       "  0.1101\n",
       " -0.3682\n",
       " -0.1943\n",
       "  0.1232\n",
       " -0.1591\n",
       "  0.2317\n",
       "  0.3872\n",
       " -0.1301\n",
       " -0.1758\n",
       "  0.1500\n",
       "  0.2446\n",
       "  0.2413\n",
       "  0.0529\n",
       " -0.0734\n",
       " -0.0825\n",
       " -0.3490\n",
       " -0.2259\n",
       " -0.6671\n",
       " -0.7304\n",
       " [torch.cuda.FloatTensor of size 32 (GPU 0)]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in decoder_rnn.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_parameters_value = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231040  10% (  1m 54s)   0.563   |   0.00: European -> european (✓) \n",
      "232040  20% (  2m 31s)   0.573   |   0.52: Nxe -> n e e (✗: n x e) (forcing)\n",
      "233040  30% (   3m 8s)   0.597   |   0.00: many -> many (✓) \n",
      "234040  40% (  3m 43s)   0.590   |   0.00: M. -> m (✓) (forcing)\n",
      "235040  50% (  4m 19s)   0.490   |   0.00: tax -> tax (✓) \n",
      "236040  60% (  4m 53s)   0.503   |   0.00: done -> done (✓) \n",
      "237040  70% (  5m 30s)   0.506   |   0.00: Museum -> museum (✓) \n",
      "238040  80% (   6m 8s)   0.550   |   0.00: CSE -> c s e (✓) (forcing)\n",
      "239040  90% (  6m 42s)   0.499   |   0.00: i -> i (✓) \n",
      "240040 100% (  7m 23s)   0.585   |   3.94: 22 13 7 2 42-14 464 -> thtee tof tot tofte tof four tou tou tou t tou t tot tou tou t tot toft tote  t th  (✗: two two sil one three sil seven sil two sil four two sil one four sil four six four) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=10000, lr=0.001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madison        => madisonsdosadiossand || madison \n",
      "                  ['\"', 'feature', 'detail', 'report', 'for', ':', 'etlan', '(', '<SAMPLE>', 'county', ',', 'virginia', ')', '\"', '.']\n",
      "&              => anddodreddreddredy o || and \n",
      "                  ['recorded', 'at', 'bonnie', 'wee', 'studios', ',', 'scotland', ';', 'windmill', 'lane', ',', 'dublin', ';', 'conway', ',', 'westlake', '<SAMPLE>', 'track', 'records', ',', 'los', 'angeles', 'california', '.']\n",
      "behaviour      => behaviory threen hte || behavior \n",
      "                  ['in', 'guido', ',', 'lefevre', 'and', 'lydgate', \"troilus'\", 'killer', \"'s\", '<SAMPLE>', 'is', 'very', 'different', ',', 'shorn', 'of', 'any', 'honour', '.']\n",
      "&              => anddred onedrddredyo || and \n",
      "                  ['science', '<SAMPLE>', 'justice', ',', '36', '(', '1', ')', ',', '51', '-', '54', '.']\n",
      "Veach          => veach seniactyscerct || veach \n",
      "                  ['<SAMPLE>', 'had', '128', 'rbis', '(', '4th', 'in', 'the', 'al', ')', 'and', 'hit', '.', '338', '(', '9th', 'in', 'the', 'al', ')', 'in', '1921', '.']\n",
      "N.             => n y nine dotercone d || n \n",
      "                  ['shomura', ',', 't', '.', ';', 'ezaki', ',', '<SAMPLE>', ';', 'tsuruoka', ',', 't', '.', ';', 'niwa', ',', 't', '.', ';', 'akita', ',', 'e', '.', ';', 'niida', ',', 't', '.', ';', 'studies', 'on', 'antibiotic', 'sf', '-', '733', ',', 'a', 'new', 'antibiotic', '.']\n",
      "pp             => p p c ter yone onend || p p \n",
      "                  ['arumugam', '1997', ',', '<SAMPLE>', '.']\n",
      "and            => andad'sacondrdredred || and \n",
      "                  ['the', 'garden', 'spans', 'three', 'quarters', 'of', 'an', 'acre', '<SAMPLE>', 'contains', 'tropical', 'and', 'native', 'trees', '.']\n",
      "6              => sixty seixdrendredre || six \n",
      "                  ['\"', 'ipv', '<SAMPLE>', 'traffic', 'over', 'vpn', 'connections', '\"', '.']\n",
      "MSU            => m s u c o's y c ond  || m s u \n",
      "                  ['\"', 'gamble', 'to', 'retire', 'as', '<SAMPLE>', 'president', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.40% (      40/   10000)\n",
      "250040   5% (   9m 0s)   0.637   |   0.00: & -> and (✓) \n",
      "260040  10% ( 14m 41s)   0.555   |   0.00: centre -> center (✓) \n",
      "270040  15% ( 19m 42s)   0.565   |   0.00: Originally -> originally (✓) (forcing)\n",
      "280040  20% ( 24m 24s)   0.528   |   0.00: NJ -> n j (✓) \n",
      "290040  25% ( 29m 13s)   0.532   |   2.95: 1813 -> eighteenn thirty  (✗: eighteen thirteen) \n",
      "Saved model to data/models/whole_gen_13_only_chars_and_attn/300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.70% (      70/   10000)\n",
      "300040  30% ( 37m 32s)   0.578   |   0.00: & -> and (✓) \n",
      "310040  35% ( 42m 24s)   0.525   |   5.94: dimanche.ch -> dimanqh w dch ncoct h w (✗: d i m a n c h e dot c h) \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-74ab6beeac73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-b943d87a23ec>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-1a40fbf99d54>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars_after\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Use own prediction as next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdecoded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_path = 'data/models/whole_gen_13_only_chars_and_attn/10_'\n",
    "#state_dict_path = 'data/models/whole_gen_13_only_chars_and_attn/250000_'\n",
    "\n",
    "decoder_rnn.load_state_dict(torch.load(state_dict_path + 'DecoderRNN'))\n",
    "encoder_rnn.load_state_dict(torch.load(state_dict_path + 'EncoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=400000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data.groupby('class')['class'].count()\n",
    "len(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_data_randomize_long():\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter, balanced_data_randomize_freq\n",
    "    \n",
    "    bal_data = pd.concat([v.sample(min(2000, len(v))) for k, v in balanced_data_classes_select])\n",
    "    long_data = sample_data[sample_data['before'].str.len()>8].sample(4000)\n",
    "    elec_data = sample_data[sample_data['class']=='ELECTRONIC']\n",
    "    let_long_data = sample_data[(sample_data['class'] == 'LETTERS') & (sample_data['before'].str.len() > 5)]\n",
    "    balanced_data = pd.concat([bal_data, long_data, elec_data, let_long_data])#.drop_duplicates()\n",
    "    balanced_data = balanced_data[~balanced_data.index.duplicated(keep='first')]\n",
    "    \n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_randomize_freq = balanced_data_length * 0.5\n",
    "    balanced_data_accessed_counter = 0\n",
    "\n",
    "balanced_data_randomize = balanced_data_randomize_long\n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    #ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    #ax.set_xticklabels([''] + input_sentence + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    \n",
    "    input_sentence = input_sentence + ['<EOS>']\n",
    "    #inp_arr = [\"{}\\n{}\".format(input_sentence[i], input_sentence[-1-i]) for i in range(len(input_sentence))]\n",
    "    inp_arr = input_sentence\n",
    "    ax.set_xticklabels([''] + inp_arr, rotation=0)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def debug_eval_sample_show_attention():\n",
    "    \n",
    "    sample_row = balanced_data_sample_row()\n",
    "    #sample_row = balanced_data[balanced_data['before'].str.len()>15].sample(1).iloc[0]\n",
    "    sample = sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence'].split(' ')\n",
    "\n",
    "    output, decoded_output, decoder_attns_arr, sample = test_model_single_sample(None, \n",
    "                                                            return_more=True, sample=sample)\n",
    "    print('input:  ', sample[0])\n",
    "    print('output: ', decoded_output)\n",
    "    print('target:   ', ' '.join([words_after_common[w] for w in sample[1][:-1]]))\n",
    "\n",
    "    attns = np.array([arr.data[0].cpu().numpy() for arr in decoder_attns_arr])\n",
    "\n",
    "    debug_show_attention(list(sample[0]), decoded_output, attns)\n",
    "    #plt.matshow(attns)\n",
    "\n",
    "debug_eval_sample_show_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data.groupby('class')['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_in_categories(iter_len = 1000):\n",
    "    wrong_preds = {}\n",
    "    for cat in categories_all:\n",
    "        tmp_data = sample_data[sample_data['class'] == cat].sample(iter_len)\n",
    "        correct_n = 0\n",
    "        wrong_preds_arr = []\n",
    "\n",
    "        for _ in range(iter_len):\n",
    "            sample_row = tmp_data.iloc[_]\n",
    "            sample = sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence']\n",
    "\n",
    "            output, t1, sample_target, t2 = test_model_single_sample(None, sample=sample)\n",
    "            if output == sample_target:\n",
    "                correct_n += 1\n",
    "            else:\n",
    "                wrong_preds_arr.append([sample_target, output])\n",
    "\n",
    "        print(\"{:>10}: {:>5d}/{:>5d} ({:>4.0%})\".format(cat, correct_n, iter_len, correct_n/iter_len))\n",
    "        wrong_preds[cat] = wrong_preds_arr\n",
    "    return wrong_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_preds = test_in_categories(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_preds['LETTERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With training longer words\n",
    "wrong_preds = test_in_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
