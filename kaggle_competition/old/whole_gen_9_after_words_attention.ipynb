{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'whole_gen_9_after_words_attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data = pickle.load(open(\"data/en_train_fixed_4_sentences.pkl\", \"rb\" ))\n",
    "all_data = pickle.load(open(\"data/en_train_fixed_5_manual.pkl\", \"rb\" ))\n",
    "# all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 654333,  (dropped rows: 9263859)\n"
     ]
    }
   ],
   "source": [
    "sample_data = all_data.copy()\n",
    "sample_data = sample_data[sample_data['class'] != 'NOT_CHANGED']\n",
    "sample_data = sample_data[sample_data['class'] != 'MANUAL']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(sample_data), len(all_data)-len(sample_data)))\n",
    "sample_data = sample_data.reset_index(drop=True)\n",
    "del(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ELECTRONIC', 'LETTERS', 'NUMBERS', 'PLAIN', 'VERBATIM']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "categories_all = sorted(sample_data[\"class\"].unique())\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', 'two', 'twenty']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_after_common = pickle.load(open(\"data/en_features/words_after_ext.pkl\", 'rb'))\n",
    "words_after_index = dict((c, i) for i, c in enumerate(words_after_common))\n",
    "words_after_common[0:7]\n",
    "len(words_after_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#onehot_sos = torch.zeros(1, 1, len(words_after_index))\n",
    "#onehot_sos[0, 0, words_after_index[SOS_TOKEN]] = 1\n",
    "#onehot_sos.size()\n",
    "#del(onehot_sos)\n",
    "\n",
    "sos_tensor = torch.LongTensor([words_after_index[SOS_TOKEN]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data_classes_select = list(sample_data.groupby('class'))\n",
    "\n",
    "balanced_data_accessed_counter = 0 \n",
    "balanced_data_length = 0\n",
    "def balanced_data_randomize(max_len=20000):\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter\n",
    "    balanced_data = pd.concat([v.sample(min(max_len, len(v))) for k, v in balanced_data_classes_select])\n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_accessed_counter = 0\n",
    "\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_accessed_counter\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_accessed_counter += 1\n",
    "    if balanced_data_accessed_counter/balanced_data_length > 0.2:\n",
    "        balanced_data_randomize()\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample\n",
    "    \n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC     4964\n",
       "LETTERS       20000\n",
       "NUMBERS       20000\n",
       "PLAIN         20000\n",
       "VERBATIM      11741\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data.groupby('class')['class'].count()\n",
    "#sample_data.groupby('class')['class'].count()\n",
    "balanced_data.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id                                                70756\n",
       "token_id                                                       8\n",
       "class                                                    LETTERS\n",
       "before                                                      przy\n",
       "after                                                    p r z y\n",
       "class_org                                                LETTERS\n",
       "a_word_ind                                  [24, 35, 105, 86, 0]\n",
       "sentence       jana bytnara \" rudego \" w mielcuszkola podstaw...\n",
       "Name: 61468, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBERS : 1855 -> eighteen fifty five <EOS> [40, 38, 14, 0]\n",
      "in <SAMPLE> kazakhs displaced from their nomadic territory appeared in verniy .\n",
      "torch.Size([1, 5, 104])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sentence_id = sample_row['class']\n",
    "    \n",
    "    return sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence']\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    s_aft_str = ' '.join([words_after_common[i] for i in s_aft])\n",
    "    print(s_class, ':', s_bef, '->', s_aft_str, s_aft)\n",
    "    print(s_sentence)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 µs ± 1.88 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('3123-5703-6077-8121-9900')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471808</th>\n",
       "      <td>540725</td>\n",
       "      <td>2</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.anglicantheologicalreview.org/read/...</td>\n",
       "      <td>h t t p colon slash slash w w w dot a n g l i ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>abstract : &lt;SAMPLE&gt; , david .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518135</th>\n",
       "      <td>592477</td>\n",
       "      <td>4</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.centerforpubliceducation.org/Main-M...</td>\n",
       "      <td>h t t p colon slash slash w w w dot c e n t e ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>retrieved 2 july 2009 , from &lt;SAMPLE&gt; , e . b ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id       class  \\\n",
       "471808       540725         2  ELECTRONIC   \n",
       "518135       592477         4  ELECTRONIC   \n",
       "\n",
       "                                                   before  \\\n",
       "471808  http://www.anglicantheologicalreview.org/read/...   \n",
       "518135  http://www.centerforpubliceducation.org/Main-M...   \n",
       "\n",
       "                                                    after   class_org  \\\n",
       "471808  h t t p colon slash slash w w w dot a n g l i ...  ELECTRONIC   \n",
       "518135  h t t p colon slash slash w w w dot c e n t e ...  ELECTRONIC   \n",
       "\n",
       "                                               a_word_ind  \\\n",
       "471808  [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "518135  [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "\n",
       "                                                 sentence  \n",
       "471808                      abstract : <SAMPLE> , david .  \n",
       "518135  retrieved 2 july 2009 , from <SAMPLE> , e . b ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>426722</th>\n",
       "      <td>491251</td>\n",
       "      <td>1</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>StoneDeadline.comDeadline.comDeadline.comDeadl...</td>\n",
       "      <td>s t o n e d e a d l i n e dot c o m d e a d l ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[17, 30, 25, 29, 28, 26, 28, 22, 26, 42, 31, 2...</td>\n",
       "      <td>deadline . comdeadline . comvarietydeadline . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231443</th>\n",
       "      <td>265794</td>\n",
       "      <td>3</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>2000Myspace.comTempleofschlock.blogpsot.comBil...</td>\n",
       "      <td>t w o o o o m y s p a c e dot c o m t e m p l ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[30, 52, 25, 25, 25, 25, 32, 86, 17, 24, 22, 2...</td>\n",
       "      <td>7th edn , &lt;SAMPLE&gt; vol 89 # 8 ( 26 february 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id       class  \\\n",
       "426722       491251         1  ELECTRONIC   \n",
       "231443       265794         3  ELECTRONIC   \n",
       "\n",
       "                                                   before  \\\n",
       "426722  StoneDeadline.comDeadline.comDeadline.comDeadl...   \n",
       "231443  2000Myspace.comTempleofschlock.blogpsot.comBil...   \n",
       "\n",
       "                                                    after   class_org  \\\n",
       "426722  s t o n e d e a d l i n e dot c o m d e a d l ...  ELECTRONIC   \n",
       "231443  t w o o o o m y s p a c e dot c o m t e m p l ...  ELECTRONIC   \n",
       "\n",
       "                                               a_word_ind  \\\n",
       "426722  [17, 30, 25, 29, 28, 26, 28, 22, 26, 42, 31, 2...   \n",
       "231443  [30, 52, 25, 25, 25, 25, 32, 86, 17, 24, 22, 2...   \n",
       "\n",
       "                                                 sentence  \n",
       "426722  deadline . comdeadline . comvarietydeadline . ...  \n",
       "231443  7th edn , <SAMPLE> vol 89 # 8 ( 26 february 19...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = sample_data[sample_data['before'].str.len()>50]\n",
    "len(tmp)\n",
    "tmp.sample(2)\n",
    "tmp[~tmp['before'].str.contains('/')].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ATTENTION_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(50, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 128, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "        self.hidden_size = words_hidden_size + chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        hidden_states_cat = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.hidden_size)).cuda()\n",
    "        for ei in range(min(MAX_ATTENTION_LENGTH, len(string_tensor[0]))):\n",
    "            hidden_states_cat[ei] = torch.cat((output_words, all_outputs_chars[0, ei].view(1,-1)), 1)\n",
    "                \n",
    "        output = torch.cat((output_words[0], output_chars[0]), 0)\n",
    "\n",
    "        #output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #return output, all_outputs_chars\n",
    "        return output, hidden_states_cat\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))\n",
    "    \n",
    "    \n",
    "encoder_rnn = EncoderRNN(words_input_size=wv_vecs.shape[-1], chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=256,\n",
    "                         words_layers=1, chars_layers=1).cuda()\n",
    "encoder_rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. M.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A. M.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 384])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_bef)\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t), s_bef\n",
    "    \n",
    "(encoder_output, encoder_outputs), tmp = test_encoder_single_sample()\n",
    "tmp\n",
    "encoder_output.size()\n",
    "encoder_outputs.size()\n",
    "torch.eq(encoder_output, encoder_outputs[len(tmp)]).data.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): GRU(384, 384, batch_first=True)\n",
       "  (lin_out): Linear (384 -> 1351)\n",
       "  (embedding): Embedding(1351, 384)\n",
       "  (attn): Linear (768 -> 50)\n",
       "  (attn_combine): Linear (768 -> 384)\n",
       "  (dropout): Dropout (p = 0.1)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 1, 384]), torch.Size([1, 50])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p=0.1, max_length=MAX_ATTENTION_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.max_length = max_length\n",
    "        self.attn = nn.Linear(hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    #def forward(self, char, hidden):\n",
    "    def forward(self, last_input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(last_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #IPython.core.debugger.Pdb().set_trace()\n",
    "        #attn_weights = F.softmax(self.attn(torch.cat((char[0], hidden[0]), 1)))\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.lin_out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        if self.n_layers > 1:\n",
    "            hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "            res = torch.cat((input_var, hid_var), 0)\n",
    "            return res\n",
    "        else:\n",
    "            return input_var\n",
    "        \n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(words_after_common), hidden_size=encoder_output.size()[0], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn\n",
    "\n",
    "tmp_outputs = Variable(torch.zeros(MAX_ATTENTION_LENGTH, encoder_output.size()[0])).cuda()\n",
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_hiddens, tmp_outputs)\n",
    "[v.size() for v in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  1.2000  2.4000  3.6000  4.8000\n",
       "[torch.FloatTensor of size 1x1x4]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(torch.FloatTensor([1,0.1,0.1]).view(1,1,-1), torch.arange(0, 12).view(1,3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 1, 384]), torch.Size([1, 50])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 242\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "s's\n"
     ]
    }
   ],
   "source": [
    "tmp_outputs = Variable(torch.zeros(MAX_ATTENTION_LENGTH, encoder_output.size()[0])).cuda()\n",
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_hiddens, tmp_outputs)\n",
    "[v.size() for v in tmp]\n",
    "print(tmp[0].topk(1)[1])\n",
    "print(words_after_common[tmp[0].topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"fifths s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's\",\n",
       " \"fifths s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's\",\n",
       " 'i e p m a',\n",
       " ('IEPMA',\n",
       "  [31, 28, 24, 32, 22, 0],\n",
       "  'LETTERS',\n",
       "  'the association was reconstituted in penticton , british columbia in 1992 as the <SAMPLE> , with dudley gordon as its first president .'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None, return_more=False):\n",
    "    decoder_rnn.eval()\n",
    "    encoder_rnn.eval()\n",
    "    \n",
    "    s_bef, s_aft, s_class, s_sentence = sample = get_random_sample()\n",
    "        \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    #encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_outputs[0].view(1,1,-1))\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    decoder_attns_arr = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attns_arr.append(decoder_attns)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        decoder_intput = torch.LongTensor([word_index])\n",
    "        #decoder_input = torch.zeros(1, 1, len(words_after_index))\n",
    "        #decoder_input[0, 0, word_index] = 1\n",
    "        #decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    sample_target = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "    \n",
    "    if return_more:\n",
    "        return output, decoded_output, decoder_attns_arr, sample\n",
    "    \n",
    "    return output, output, sample_target, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              => fifths s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's || [104, 0] \n",
      "                  isbn <SAMPLE> - 471 - 08364 - x . koszinowski , j . ( 1980 ) .\n",
      "www.hopenow.tv => fifths s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's s's || [52, 52, 52, 74, 45, 25, 24, 28, 29, 25, 52, 74, 30, 54, 0] \n",
      "                  hopenow . tv is available on cable in the roanoke lynchburg market , and via free live streaming from their website : <SAMPLE> .\n"
     ]
    }
   ],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', s_sentence, ))\n",
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/     100)\n",
      "CPU times: user 2.36 s, sys: 52 ms, total: 2.42 s\n",
      "Wall time: 2.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    #encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_outputs[0].view(1,1,-1))\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "    \n",
    "    ###\n",
    "    target_arr = s_aft\n",
    "    #target_arr = after_sentence_to_word_indexes(s_aft, include_eos=True)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        #decoder_target_i = chars_after_index[target_arr[i]]\n",
    "        decoder_target_i = target_arr[i]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word_index = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_intput = torch.LongTensor([word_index])\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            s_aft_sentence = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "            correct = '✓' if result == s_aft_sentence else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/whole_gen_9_after_words_attention\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   5.606   |   6.88: & -> <EOS> (✗: [55, 0]) (forcing)\n",
      "Saved model to data/models/whole_gen_9_after_words_attention/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (  0m 52s)   5.503   |   6.51: & -> <EOS> (✗: [55, 0]) (forcing)\n",
      "    27  54% (  0m 52s)   5.270   |   6.83: 28 December 2013 -> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> (✗: [11, 6, 80, 12, 65, 6, 49, 0]) (forcing)\n",
      "    36  72% (  0m 52s)   5.202   |   1.47: WPCR- ->  (✗: [52, 24, 21, 35, 0]) \n",
      "    45  90% (  0m 52s)   4.904   |   3.25: & ->  (✗: [55, 0]) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (  0m 10s)   3.042   |   1.65: - ->  (✗: [57, 0]) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 21s)   2.793   |   4.83: programme -> nineteen (✗: [139, 0]) \n",
      "  3000  22% (  0m 40s)   2.474   |   4.09: 10,000 -> the twenty twenty (✗: [44, 8, 0]) (forcing)\n",
      "  4000  33% (   1m 1s)   2.318   |   2.25: mr -> to (✗: [117, 0]) \n",
      "  5000  44% (  1m 22s)   2.278   |   3.80: 11 -> p (✗: [48, 0]) \n",
      "  6000  56% (  1m 44s)   2.098   |   6.25: November 4, 1996 -> h i t a a m (✗: [69, 77, 7, 23, 20, 0]) (forcing)\n",
      "  7000  67% (   2m 5s)   2.010   |   0.02: & -> and (✓) (forcing)\n",
      "  8000  78% (  2m 27s)   1.973   |   1.46: 2009 -> nineteen thousand <EOS> (✗: [5, 8, 15, 0]) (forcing)\n",
      "  9000  89% (  2m 50s)   1.969   |   2.94: programmes -> center (✗: [184, 0]) (forcing)\n",
      " 10000 100% (  3m 12s)   1.955   |   0.01: & -> and (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (  3m 51s)   1.164   |   0.00: & -> and (✓) (forcing)\n",
      " 30000  22% (   8m 2s)   0.874   |   0.78: 60 -> sixty (✓) \n",
      " 40000  33% ( 11m 56s)   0.689   |   0.00: - -> to (✓) (forcing)\n",
      " 50000  44% ( 15m 26s)   0.609   |   0.03: ISBN -> i s b n (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_9_after_words_attention/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 70.12% (    7012/   10000)\n",
      " 60000  56% (  20m 9s)   0.490   |   0.41: INI -> i n i (✓) \n",
      " 70000  67% (  24m 4s)   0.465   |   1.80: 287.1 -> two point point seven seventy <EOS> (✗: [5, 10, 27, 18, 46, 9, 0]) (forcing)\n",
      " 80000  78% (  28m 4s)   0.552   |   0.01: UCI -> u c i (✓) \n",
      " 90000  89% ( 31m 53s)   0.484   |   0.44: 1566 -> sixteen sixty six (✗: [51, 39, 20, 0]) \n",
      "100000 100% ( 35m 44s)   0.441   |   1.97: MySQL -> m s s l (✗: [32, 86, 17, 111, 42, 0]) \n",
      "Saved model to data/models/whole_gen_9_after_words_attention/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 78.00% (    7800/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111889   5% (  3m 30s)   0.411   |   0.94: Gsp.ro -> g p p dot o (✗: [53, 17, 24, 74, 35, 25, 0]) \n",
      "121889  10% (  7m 15s)   0.416   |   0.01: # -> number (✓) \n",
      "131889  15% ( 10m 59s)   0.423   |   0.00: S. H. -> s h (✓) (forcing)\n",
      "141889  20% ( 14m 44s)   0.463   |   0.10: CSSI -> c s s i (✓) (forcing)\n",
      "Saved model to data/models/whole_gen_9_after_words_attention/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 82.65% (    8265/   10000)\n",
      "151889  25% ( 19m 40s)   0.366   |   2.91: http://www.humboldt.edu/celt/tips/micro-lectures_just-in-time_teaching_for_critical_topics_and_skills/Center -> h t slash colon colon w w w w w w dot s dot o s o s s o s o o s o s s o s e s s e s e o s e s s e s e s s e s e e s e e s e s e e s e e e e s e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e (✗: [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74, 45, 43, 32, 36, 25, 42, 26, 30, 74, 28, 26, 43, 101, 21, 28, 42, 30, 101, 30, 31, 24, 17, 101, 32, 31, 21, 35, 25, 115, 42, 28, 21, 30, 43, 35, 28, 17, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 60, 43, 17, 30, 115, 31, 29, 115, 30, 31, 32, 28, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 30, 28, 22, 21, 45, 31, 29, 53, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 37, 25, 35, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 21, 35, 31, 30, 31, 21, 22, 42, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 30, 25, 24, 31, 21, 17, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 22, 29, 26, 43, 29, 26, 28, 35, 17, 21, 25, 35, 28, 17, 59, 31, 42, 42, 17, 101, 21, 28, 29, 30, 28, 35, 0]) \n",
      "161889  30% ( 23m 13s)   0.355   |   2.34: 1986 -> nineteen eighty six (✗: [9, 8, 15, 10, 27, 20, 0]) \n",
      "171889  35% ( 26m 46s)   0.405   |   0.00: vol -> volume (✓) \n",
      "181889  40% ( 30m 23s)   0.340   |   0.08: 8th -> eighth (✓) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-17229cbbb125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-fc5438e8eb84>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m                              \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-8fe5bd18af36>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.3, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We were using wrong sample data before this. From this point on we have removed 'manual'-class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data[sample_data['before'] == 'σ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   Jeuneafrique.com\n",
      "output:  ['j', 'e', 'u', 'u', 'u', 'e', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'dot', 'c', 'o']\n",
      "target:    j e u n e a f r i q u e dot c o m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAFeCAYAAAAc31myAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngMEUFAAAwGD/D3l75Kee4V0sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87248d7dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def debug_show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    #ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_xticklabels([''] + input_sentence + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def debug_eval_sample_show_attention():\n",
    "    output, decoded_output, decoder_attns_arr, sample = test_model_single_sample(None, return_more=True)\n",
    "    print('input:  ', sample[0])\n",
    "    print('output: ', decoded_output)\n",
    "    print('target:   ', ' '.join([words_after_common[w] for w in sample[1][:-1]]))\n",
    "\n",
    "    attns = np.array([arr.data[0].cpu().numpy() for arr in decoder_attns_arr])\n",
    "\n",
    "    debug_show_attention(list(sample[0]), decoded_output, attns)\n",
    "    #plt.matshow(attns)\n",
    "    \n",
    "debug_eval_sample_show_attention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [r/sum(r) for r in attns]\n",
    "plt.matshow(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(attns, 1)\n",
    "[attns[k,v] for k, v in enumerate(np.argmax(attns, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
