{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'numbers_gen_4_mod_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"data/en_train_fixed_1.pkl\", \"rb\" ))\n",
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8375412</th>\n",
       "      <td>633983</td>\n",
       "      <td>1</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>PLAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633878</th>\n",
       "      <td>204173</td>\n",
       "      <td>4</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>Ё</td>\n",
       "      <td>Ё</td>\n",
       "      <td>VERBATIM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id        class before after class_org\n",
       "8375412       633983         1  NOT_CHANGED     on    on     PLAIN\n",
       "2633878       204173         4  NOT_CHANGED      Ё     Ё  VERBATIM"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT_CHANGED' 'NUMBERS' 'LETTERS' 'PLAIN' 'VERBATIM' 'ELECTRONIC']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', '.', ',', 'the', '\"', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 448172,  (dropped rows: 9470020)\n"
     ]
    }
   ],
   "source": [
    "number_data = all_data[all_data['class'] == 'NUMBERS']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(number_data), len(all_data)-len(number_data)))\n",
    "number_data = number_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data = number_data\n",
    "\n",
    "balanced_data_length = len(balanced_data)\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id           63001\n",
       "token_id                  5\n",
       "class               NUMBERS\n",
       "before                 39th\n",
       "after          thirty ninth\n",
       "class_org           ORDINAL\n",
       "Name: 38710, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = list(set(list(number_data['after'])))\n",
    "arr = [s.split(' ') for s in arr]\n",
    "arr = np.concatenate(arr)\n",
    "arr = list(set(arr))\n",
    "number_words = [EOS_TOKEN, SOS_TOKEN, UNKNOWN_WORD_TOKEN, NUMBER_WORD_TOKEN, SAMPLE_WORD_TOKEN] + arr\n",
    "number_words_index = dict((c, i) for i, c in enumerate(number_words))\n",
    "len(number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ed77d58eaf12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumber_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'number_words' is not defined"
     ]
    }
   ],
   "source": [
    "number_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 511])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def number_words_to_tensor(words, include_eos=True):\n",
    "    return words_to_tensor(words, words_lookup_index=number_words_index, include_eos=include_eos)\n",
    "number_words_to_tensor(['one', 'first']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words_index['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(number_words, open('data/models/numbers_gen_4_mod_data_1/number_words.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 511])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words_onehot_sos = number_words_to_tensor([SOS_TOKEN], include_eos=False)\n",
    "#number_words_onehot_sos = Variable(torch.from_numpy(number_words_onehot_sos)).cuda()\n",
    "number_words_onehot_sos.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBERS : 978-0-8112-1878-8 -> nine seven eight sil o sil eight one one two sil one eight seven eight sil eight\n",
      "Katherine Silver ; 2010 ) ISBN <SAMPLE> ( New Directions ) The Seamstress and the Wind ( trans .\n",
      "['Katherine', 'Silver', ';', '2010', ')', 'ISBN', '<SAMPLE>', '(', 'New', 'Directions', ')', 'The', 'Seamstress', 'and', 'the', 'Wind', '(', 'trans', '.']\n",
      "torch.Size([1, 20, 8192])\n",
      "torch.Size([1, 18, 104])\n",
      "torch.Size([1, 18, 511])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = list(rows.before)\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = SAMPLE_WORD_TOKEN\n",
    "    \n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], befores\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_class, ':', s_bef, '->', s_aft)\n",
    "    print(' '.join(s_sentence))\n",
    "    print(s_sentence)\n",
    "    print(words_to_tensor(list(s_sentence), common_words_index).shape)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "    print(number_words_to_tensor(s_aft.split(' ')).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(8192, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_rnn = EncoderRNN(words_input_size=len(common_words), chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=128,\n",
    "                         words_layers=2, chars_layers=2).cuda()\n",
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t)\n",
    "    \n",
    "encoder_output = test_encoder_single_sample()\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): GRU(511, 256, batch_first=True)\n",
       "  (lin_out): Linear (256 -> 511)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                         # LSTM would require own hidden included\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(number_words), hidden_size=encoder_output.size()[-1], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 511])\n",
      "Variable containing:\n",
      " 53\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "steph\n"
     ]
    }
   ],
   "source": [
    "tmp_a, tmp_b = decoder_rnn(Variable(number_words_onehot_sos).cuda(), encoder_output.view(1,1,-1))\n",
    "print(tmp_a.size())\n",
    "print(tmp_a.topk(1)[1])\n",
    "print(number_words[tmp_a.topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('steph and and and and and and and and and and and and and and and and and and and',\n",
       " 'steph and and and and and and and and and and and and and and and and and and and',\n",
       " 'one thousand milligrams',\n",
       " ('1000 mg',\n",
       "  'one thousand milligrams',\n",
       "  'NUMBERS',\n",
       "  ['Dosage',\n",
       "   'of',\n",
       "   '<SAMPLE>',\n",
       "   '/day',\n",
       "   'will',\n",
       "   'produce',\n",
       "   'a',\n",
       "   '25%',\n",
       "   'decrease',\n",
       "   'in',\n",
       "   'performance',\n",
       "   ',',\n",
       "   'on',\n",
       "   'top',\n",
       "   'of',\n",
       "   'the',\n",
       "   'reduction',\n",
       "   'due',\n",
       "   'to',\n",
       "   'high',\n",
       "   'altitude',\n",
       "   'exposure',\n",
       "   '.']))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None):\n",
    "    s_bef, s_aft, s_class, s_sentence = sample = get_random_sample()\n",
    "        \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    return output, output, s_aft, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', ' '.join(s_sentence), ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1931           => steph and and and and and and and and and and and and and and and and and and and || nineteen thirty one \n",
      "                  He was a member of the US Davis Cup teams in 1930 and <SAMPLE> but did not play any matches .\n",
      "26             => steph and and and and and and and and and and and and and and and and and and and || twenty six \n",
      "                  Challenger landed at Edwards Air Force Base , California , on August 6, 1985 , at 12:45 : <SAMPLE> pm PDT .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/   10000)\n",
      "CPU times: user 11min 4s, sys: 7 s, total: 11min 11s\n",
      "Wall time: 2min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    target_arr = s_aft.split(' ') + [EOS_TOKEN]\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "\n",
    "        decoder_target_i = number_words_index[target_arr[i]]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            correct = '✓' if result == s_aft else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/numbers_gen_4_mod_data_1\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   6.244   |   6.26: 2 -> percent fiftieths (✗: two) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (  2m 19s)   6.239   |   6.23: 171,000 -> percent percent percent euros nine nine (✗: one hundred seventy one thousand) (forcing)\n",
      "    27  54% (  2m 19s)   6.230   |   6.19: May 10, 2015 -> percent percent nine nine nine (✗: may tenth twenty fifteen) (forcing)\n",
      "    36  72% (  2m 19s)   6.222   |   6.19: 2000 -> percent nine percent (✗: two thousand) \n",
      "    45  90% (  2m 20s)   6.214   |   6.19: July 9, 2008 -> percent nine percent nine percent nine (✗: july ninth two thousand eight) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (   0m 7s)   2.958   |   5.07: 1840s -> nineteen <EOS> (✗: eighteen forties) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 14s)   2.611   |   1.38: II -> two (✓) (forcing)\n",
      "  3000  22% (  0m 30s)   2.511   |   2.29: 50 -> one (✗: fifty) (forcing)\n",
      "  4000  33% (  0m 44s)   2.477   |   2.23: 12 May 2009 -> the twenty of of twenty thousand <EOS> (✗: the twelfth of may two thousand nine) (forcing)\n",
      "  5000  44% (  0m 59s)   2.325   |   2.40: 21 June 2008 -> the twenty of of twenty twenty <EOS> <EOS> (✗: the twenty first of june two thousand eight) (forcing)\n",
      "  6000  56% (  1m 14s)   2.386   |   1.33: 2004 -> two thousand <EOS> (✗: two thousand four) (forcing)\n",
      "  7000  67% (  1m 33s)   2.329   |   2.30: 809 -> two hundred <EOS> (✗: eight hundred nine) (forcing)\n",
      "  8000  78% (  1m 52s)   2.186   |   1.74: 1,002 -> two thousand <EOS> (✗: one thousand two) (forcing)\n",
      "  9000  89% (  2m 11s)   2.160   |   2.85: 2 June 1581 -> the twenty of of <EOS> <EOS> <EOS> (✗: the second of june fifteen eighty one) (forcing)\n",
      " 10000 100% (  2m 30s)   2.046   |   1.18: 1958 -> nineteen ninety eight (✗: nineteen fifty eight) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (  3m 21s)   1.195   |   1.96: 2010-08-07 -> the twenty of of twenty two thousand (✗: the seventh of august twenty ten) \n",
      " 30000  22% (  6m 51s)   0.728   |   0.02: 2004 -> two thousand four (✓) (forcing)\n",
      " 40000  33% ( 10m 22s)   0.551   |   0.03: 20th -> twentieth (✓) \n",
      " 50000  44% ( 13m 42s)   0.401   |   0.44: 7th -> seventh (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 66.70% (    6670/   10000)\n",
      " 60000  56% ( 18m 25s)   0.384   |   0.00: 10 -> ten (✓) (forcing)\n",
      " 70000  67% ( 21m 54s)   0.294   |   1.44: 6,712 -> six thousand seven hundred thirty two (✗: six thousand seven hundred twelve) \n",
      " 80000  78% ( 25m 27s)   0.289   |   0.01: 4th -> fourth (✓) (forcing)\n",
      " 90000  89% ( 28m 54s)   0.275   |   0.00: 1981 -> nineteen eighty one (✓) \n",
      "100000 100% (  32m 9s)   0.265   |   1.30: 2.7% -> two point seven seven (✗: two point seven percent) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 84.67% (    8467/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000   3% (  3m 16s)   0.217   |   1.70: 8247 -> eight hundred twenty seven (✗: eight thousand two hundred forty seven) \n",
      "120000   7% (  6m 32s)   0.241   |   0.20: 5pm -> five p m (✓) \n",
      "130000  10% (  9m 48s)   0.230   |   0.24: 120,000 -> one hundred twenty thousand (✓) \n",
      "140000  13% (  13m 4s)   0.215   |   0.00: 1971 -> nineteen seventy one (✓) (forcing)\n",
      "150000  17% ( 16m 21s)   0.215   |   0.03: Oct 2011 -> october twenty eleven (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 87.55% (    8755/   10000)\n",
      "160000  20% ( 20m 46s)   0.202   |   0.00: 1969 -> nineteen sixty nine (✓) (forcing)\n",
      "170000  23% (  24m 4s)   0.168   |   0.01: 1994 -> nineteen ninety four (✓) (forcing)\n",
      "180000  27% ( 27m 19s)   0.150   |   0.00: 2002 -> two thousand two (✓) \n",
      "190000  30% ( 30m 36s)   0.147   |   0.01: 283 -> two hundred eighty three (✓) \n",
      "200000  33% ( 33m 53s)   0.154   |   0.01: 1959 -> nineteen fifty nine (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 90.79% (    9079/   10000)\n",
      "210000  37% ( 38m 12s)   0.146   |   0.01: 100 -> one hundred (✓) \n",
      "220000  40% ( 41m 28s)   0.117   |   0.00: 88 -> eighty eight (✓) \n",
      "230000  43% ( 44m 46s)   0.144   |   0.64: 11 -> one (✗: eleven) (forcing)\n",
      "240000  47% (  48m 4s)   0.137   |   0.00: August 23, 2007 -> august twenty third two thousand seven (✓) \n",
      "250000  50% ( 51m 23s)   0.188   |   0.00: 548 -> five hundred forty eight (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 91.31% (    9131/   10000)\n",
      "260000  53% ( 55m 48s)   0.119   |   0.00: 24 March 1998 -> the twenty fourth of march nineteen ninety eight (✓) (forcing)\n",
      "270000  57% (  59m 4s)   0.133   |   0.12: .154 -> point one five four (✓) (forcing)\n",
      "280000  60% ( 62m 23s)   0.093   |   0.00: 1882 -> eighteen eighty two (✓) (forcing)\n",
      "290000  63% ( 65m 42s)   0.156   |   0.00: 2nd -> second (✓) \n",
      "300000  67% (  69m 0s)   0.123   |   0.02: 2,543 -> two thousand five hundred forty three (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.60% (    9260/   10000)\n",
      "310000  70% ( 73m 26s)   0.115   |   0.38: 9.15 -> nine point five five (✗: nine point one five) (forcing)\n",
      "320000  73% ( 76m 43s)   0.091   |   0.00: 1915 -> nineteen fifteen (✓) \n",
      "330000  77% ( 79m 59s)   0.107   |   0.00: 16 -> sixteen (✓) (forcing)\n",
      "340000  80% ( 83m 17s)   0.106   |   0.00: 1941 -> nineteen forty one (✓) (forcing)\n",
      "350000  83% ( 86m 32s)   0.128   |   0.00: 1992 -> nineteen ninety two (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.50% (    9350/   10000)\n",
      "360000  87% ( 90m 56s)   0.106   |   0.00: 4 -> four (✓) \n",
      "370000  90% ( 94m 14s)   0.142   |   0.00: 40 -> forty (✓) (forcing)\n",
      "380000  93% ( 97m 31s)   0.128   |   0.00: 2013 -> twenty thirteen (✓) (forcing)\n",
      "390000  97% (100m 48s)   0.126   |   3.15: 1970 -> one nine seven (✗: nineteen seventy) \n",
      "400000 100% (104m 10s)   0.108   |   0.00: 1919 -> nineteen nineteen (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.67% (    9367/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,111.7        => two thousand one hundred seventeen point seven || two thousand one hundred eleven point seven \n",
      "                  On January 1, 1971 , the City of Saskatoon annexed the airport and surrounding lands totalling <SAMPLE> acres .\n",
      "2012-09-30     => the thirtieth of september twenty ten || the thirtieth of september twenty twelve \n",
      "                  Banco Bradesco S / A . <SAMPLE> .\n",
      "2015           => twenty fifteen fifteen || twenty fifteen \n",
      "                  Tim Buford , \" Georgia - <SAMPLE> edition , Bradt Travel Guides \" , p 128 .\n",
      "7/15/08        => march twenty eighth || july fifteenth o eight \n",
      "                  Accessed on <SAMPLE> .\n",
      "6:00 am        => six hundred b c m || six a m \n",
      "                  It is simulcasted in aksyon tv , as well in Radyo 5 92.3 News FM from 5:30 - <SAMPLE> .\n",
      "April 1700     => april seventeen seventy || april seventeen hundred \n",
      "                  Generally inhospitable conditions thwarted the effort , and it was abandoned in <SAMPLE> .\n",
      "0300095937     => o three nine three three four || o three o o o nine five nine three seven \n",
      "                  ISBN <SAMPLE> \" Re opening of Holy Trinity Church , Micklegate \" .\n",
      "1332           => thirteen thirty three || thirteen thirty two \n",
      "                  Lower Mac William and Viscounts of Mayo , <SAMPLE> - 1649 , in A New History of Ireland IX , pp .\n",
      "109s           => one nine hundred || one hundred nines \n",
      "                  The battle with the Bf <SAMPLE> took place over Eleusis and Tanagra airfields .\n",
      "0-313-34526-0  => o sil three one three sil three three sil one five sil o || o sil three one three sil three four five two six sil o \n",
      "                  ISBN <SAMPLE> , ISBN 978-0-313-34526-5 .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410000   3% (  3m 16s)   0.125   |   0.00: February 26, 2013 -> february twenty sixth twenty thirteen (✓) \n",
      "420000   7% (  6m 34s)   0.103   |   0.00: 6 -> six (✓) \n",
      "430000  10% (  9m 52s)   0.120   |   0.00: June 2009 -> june two thousand nine (✓) \n",
      "440000  13% (  13m 3s)   0.110   |   0.00: 100 -> one hundred (✓) \n",
      "450000  17% ( 15m 31s)   0.126   |   0.00: 2002 -> two thousand two (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.76% (    9376/   10000)\n",
      "460000  20% ( 18m 50s)   0.105   |   0.01: 1843 -> eighteen forty three (✓) \n",
      "470000  23% ( 21m 19s)   0.118   |   0.00: 1990 -> nineteen ninety (✓) \n",
      "480000  27% ( 23m 47s)   0.116   |   0.00: 117 -> one hundred seventeen (✓) \n",
      "490000  30% ( 26m 15s)   0.099   |   0.00: 2001 -> two thousand one (✓) \n",
      "500000  33% ( 28m 42s)   0.085   |   0.00: 1 -> one (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.43% (    9343/   10000)\n",
      "510000  37% (  32m 1s)   0.140   |   0.00: 2000 -> two thousand (✓) \n",
      "520000  40% ( 34m 29s)   0.114   |   0.00: 1963 -> nineteen sixty three (✓) \n",
      "530000  43% ( 36m 57s)   0.120   |   0.00: 12 June 1944 -> the twelfth of june nineteen forty four (✓) \n",
      "540000  47% ( 39m 26s)   0.106   |   0.01: 2011 -> twenty eleven (✓) \n",
      "550000  50% ( 41m 54s)   0.121   |   0.00: 51 -> fifty one (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.28% (    9428/   10000)\n",
      "560000  53% ( 45m 13s)   0.115   |   0.00: 1999 -> nineteen ninety nine (✓) \n",
      "570000  57% ( 47m 41s)   0.101   |   0.00: 30 -> thirty (✓) \n",
      "580000  60% ( 50m 10s)   0.125   |   5.55: 08-30-09 -> the seventh of september o (✗: august thirtieth o nine) \n",
      "590000  63% ( 52m 38s)   0.089   |   0.00: July 21, 1952 -> july twenty first nineteen fifty two (✓) (forcing)\n",
      "600000  67% (  55m 6s)   0.118   |   0.00: 5 -> five (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.95% (    9395/   10000)\n",
      "610000  70% ( 58m 24s)   0.088   |   0.03: 1557 -> fifteen fifty seven (✓) \n",
      "620000  73% ( 60m 53s)   0.103   |   0.00: 3 -> three (✓) \n",
      "630000  77% ( 63m 22s)   0.071   |   0.00: 409 -> four hundred nine (✓) \n",
      "640000  80% ( 65m 50s)   0.093   |   0.00: 5 -> five (✓) \n",
      "650000  83% ( 68m 18s)   0.117   |   0.00: 7 -> seven (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.75% (    9475/   10000)\n",
      "660000  87% ( 71m 37s)   0.094   |   0.00: 208 -> two hundred eight (✓) \n",
      "670000  90% (  74m 6s)   0.126   |   0.01: $1,000 -> one thousand dollars (✓) \n",
      "680000  93% ( 76m 35s)   0.094   |   0.00: 2013 -> twenty thirteen (✓) \n",
      "690000  97% (  79m 4s)   0.122   |   0.00: 105 -> one hundred five (✓) \n",
      "700000 100% ( 81m 32s)   0.094   |   0.00: 1918 -> nineteen eighteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.90% (    9390/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20             => two o          || twenty \n",
      "                  Brut y Tywysogion , Peniarth MS 20 , 223 ; Peniarth MS <SAMPLE> Tr , 117 .\n",
      "2¾             => twenty five    || two and three quarters \n",
      "                  The girls' and younger boys' race is <SAMPLE> miles while the senior boys' is 4 miles .\n",
      "9801           => nine thousand eight || nine eight o one \n",
      "                  Later , other versions were developed for the MSX turbo R , NEC PC- <SAMPLE> , NEC PC- Engine , and Sega Game Gear in 1991 - 1994 .\n",
      "1939           => one thousand nine hundred thirty nine || nineteen thirty nine \n",
      "                  Summer time was used in 1917 - 1919 , 1921 and <SAMPLE> - 1967 .\n",
      "39370          => three three three seven o || three nine three seven o \n",
      "                  PLoS ONE , 7 ( 7 ) : e <SAMPLE> .\n",
      "1996           => one nine six   || nineteen ninety six \n",
      "                  Pollution prevention : Since <SAMPLE> , Unisys has reduced hazardous waste generation by approximately 95% .\n",
      "351.165 km/h   => three hundred sixty one point five five five || three hundred fifty one point one six five kilometers per hour \n",
      "                  Mario Andretti blistered the track at an unofficial track record of 218.204 mph ( <SAMPLE> ) .\n",
      "XII            => twelve         || the twelfth \n",
      "                  Karl <SAMPLE> : s syster .\n",
      "1997-10-06     => the sixth of june nineteen ninety nine || the sixth of october nineteen ninety seven \n",
      "                  The station was first known as KKRR , starting on <SAMPLE> .\n",
      "6.85 km2       => six point eight five kilometers || six point eight five square kilometers \n",
      "                  The municipality covers an area of <SAMPLE> .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710000   3% (  2m 27s)   0.105   |   0.00: 2007 -> two thousand seven (✓) (forcing)\n",
      "720000   7% (  4m 56s)   0.097   |   0.00: March 18, 2008 -> march eighteenth two thousand eight (✓) (forcing)\n",
      "730000  10% (  7m 25s)   0.064   |   0.00: 22 August 2013 -> the twenty second of august twenty thirteen (✓) \n",
      "740000  13% (  9m 54s)   0.054   |   0.00: 600 -> six hundred (✓) (forcing)\n",
      "750000  17% ( 12m 24s)   0.073   |   0.00: 2015 -> twenty fifteen (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.44% (    9544/   10000)\n",
      "760000  20% ( 15m 42s)   0.093   |   1.96: 647986- -> six seven eight six six six six (✗: six four seven nine eight six) (forcing)\n",
      "770000  23% ( 18m 11s)   0.078   |   0.00: 147 -> one hundred forty seven (✓) (forcing)\n",
      "780000  27% ( 20m 40s)   0.060   |   0.00: December 12, 1953 -> december twelfth nineteen fifty three (✓) (forcing)\n",
      "790000  30% (  23m 9s)   0.043   |   0.02: September 3 -> september third (✓) (forcing)\n",
      "800000  33% ( 25m 37s)   0.067   |   0.05: $1.1 million -> one point one million dollars (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.07% (    9607/   10000)\n",
      "810000  37% ( 28m 57s)   0.110   |   0.00: 9 -> nine (✓) (forcing)\n",
      "820000  40% ( 31m 26s)   0.075   |   0.00: 17 May 1882 -> the seventeenth of may eighteen eighty two (✓) (forcing)\n",
      "830000  43% ( 33m 55s)   0.050   |   0.00: 1980 -> nineteen eighty (✓) \n",
      "840000  47% ( 36m 24s)   0.064   |   0.00: 3 December 1973 -> the third of december nineteen seventy three (✓) \n",
      "850000  50% ( 38m 54s)   0.066   |   0.00: 2012 -> twenty twelve (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.87% (    9587/   10000)\n",
      "860000  53% ( 42m 15s)   0.072   |   0.00: 5 December 2009 -> the fifth of december two thousand nine (✓) (forcing)\n",
      "870000  57% ( 44m 44s)   0.046   |   0.00: 0% -> zero percent (✓) \n",
      "880000  60% ( 47m 13s)   0.064   |   0.00: 2014 -> twenty fourteen (✓) (forcing)\n",
      "890000  63% ( 49m 42s)   0.068   |   0.00: 1999 -> nineteen ninety nine (✓) (forcing)\n",
      "900000  67% ( 52m 11s)   0.041   |   0.00: 19 -> nineteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.32% (    9632/   10000)\n",
      "910000  70% ( 55m 31s)   0.056   |   0.00: 47 -> forty seven (✓) (forcing)\n",
      "920000  73% ( 57m 59s)   0.069   |   0.00: January 20, 1920 -> january twentieth nineteen twenty (✓) (forcing)\n",
      "930000  77% ( 60m 28s)   0.066   |   0.00: 2011 -> twenty eleven (✓) (forcing)\n",
      "940000  80% ( 62m 57s)   0.058   |   0.00: 43 -> forty three (✓) \n",
      "950000  83% ( 65m 26s)   0.059   |   0.00: 23 July 2015 -> the twenty third of july twenty fifteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.55% (    9655/   10000)\n",
      "960000  87% ( 68m 45s)   0.073   |   0.00: 2003 -> two thousand three (✓) \n",
      "970000  90% ( 71m 14s)   0.072   |   0.00: 2009 -> two thousand nine (✓) \n",
      "980000  93% ( 73m 44s)   0.069   |   0.00: 20 -> twenty (✓) (forcing)\n",
      "990000  97% ( 76m 12s)   0.046   |   0.00: 4 July 2014 -> the fourth of july twenty fourteen (✓) (forcing)\n",
      "1000000 100% ( 78m 41s)   0.054   |   0.00: 44 -> forty four (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.86% (    9686/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-8147-5123-7  => o sil eight one one sil sil one one seven seven sil three || o sil eight one four seven sil five one two three sil seven \n",
      "                  ISBN <SAMPLE> Patterson , John ( 2007 ) .\n",
      "0-8128-1537-8  => o sil eight one one sil sil one eight eight five sil eight || o sil eight one two eight sil one five three seven sil eight \n",
      "                  Sampson , Anthony \" The Sovereign State of ITT \" , Stein and Day , 1973 , ISBN <SAMPLE> Smith , J. Richard .\n",
      "10             => ten            || one o \n",
      "                  These are 07 BUJUMBURA 479 , 10 WINDHOEK 7 , 07 BUJUMBURA 515 , 09 STATE 15113 , 09 STOCKHOLM 194 , <SAMPLE> SANAA 5 , and 10 CARACAS 107 .\n",
      "04.01PM IST    => o four sil p m || four o one p m i s t \n",
      "                  TNN May 28, 2013 , <SAMPLE> ( 2013-05-28 ) .\n",
      "1-893777-00-6  => one sil eight seven seven seven sil sil seven seven six sil six || one sil eight nine three seven seven seven sil o o sil six \n",
      "                  ISBN <SAMPLE> ( series ) .\n",
      "48825404       => forty eight million two hundred twenty eight thousand four hundred four four || forty eight million eight hundred twenty five thousand four hundred four \n",
      "                  OCLC <SAMPLE> ; ASIN B 000 FMPZ 801924 edition — Queenstown : Mount Lyell Tourist Association .\n",
      "978-1578592135 => nine seven eight sil one five one five three five one three one five five || nine seven eight sil one five seven eight five nine two one three five \n",
      "                  ISBN <SAMPLE> Luckhurst , Roger .\n",
      "1911           => nineteen eleven || one thousand nine hundred eleven \n",
      "                  In 1910 - <SAMPLE> , they were taken over by the Bruner Mond company , alkali makers .\n",
      "1974's         => nineteen seventy four || nineteen seventy fours \n",
      "                  Love Unlimited ( by the vocal group Love Unlimited ) and <SAMPLE> Rhapsody in White by Love Unlimited Orchestra .\n",
      "1948           => nineteen forty eight || one thousand nine hundred forty eight \n",
      "                  The Merger : <SAMPLE> - 1949 ( online page ) ( online history book ) , Giving & Sharing .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010000  10% (  2m 27s)   0.104   |   0.00: 230 -> two hundred thirty (✓) \n",
      "1020000  20% (  4m 57s)   0.124   |   0.00: 13 January 1958 -> the thirteenth of january nineteen fifty eight (✓) \n",
      "1030000  30% (  7m 26s)   0.093   |   0.00: 754 -> seven hundred fifty four (✓) \n",
      "1040000  40% (  9m 55s)   0.106   |   0.00: 6 -> six (✓) \n",
      "1050000  50% ( 12m 25s)   0.120   |   0.00: 2008-12-08 -> the eighth of december two thousand eight (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.37% (    9437/   10000)\n",
      "1060000  60% ( 15m 44s)   0.099   |   0.00: 1996 -> nineteen ninety six (✓) \n",
      "1070000  70% ( 18m 13s)   0.119   |   0.00: 1st -> first (✓) \n",
      "1080000  80% ( 20m 42s)   0.070   |   0.00: 1990s -> nineteen nineties (✓) \n",
      "1090000  90% ( 23m 11s)   0.123   |   0.01: March 4, 1901 -> march fourth nineteen o one (✓) \n",
      "1100000 100% ( 25m 40s)   0.113   |   0.00: 2006 -> two thousand six (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.49% (    9449/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180's          => one nineties   || one eighties \n",
      "                  Neither player played at their best , both averaging below 90 despite hitting 8 <SAMPLE> between them .\n",
      "1983-84 CHL    => one nine four three sil sil sil sil three sil three three || one nine eight three sil eight four sil c h l \n",
      "                  He won the <SAMPLE> Championship ( Adams Cup ) as a member of the Tulsa Oilers team coached by Tom Webster .\n",
      "1065           => ten sixty six  || ten sixty five \n",
      "                  3 d <SAMPLE> - 67 , 283 Cal .\n",
      "-96.14         => minus one six six two four three four || minus ninety six point one four \n",
      "                  The elevation is 735 feet ( 224 m ) and the coordinates are latitude 35.653 and longitude <SAMPLE> .\n",
      "469,000        => four hundred sixty nine thousand o || four hundred sixty nine thousand \n",
      "                  When the second episode aired , the viewer count dipped to <SAMPLE> together , live and on timeshift .\n",
      "83/100         => eighty three one thousand || eighty three one hundredths \n",
      "                  As of October 9 , the game has a score of <SAMPLE> on Metacritic .\n",
      "214.21 km2     => twenty hundred one point two one square kilometers per || two hundred fourteen point two one square kilometers \n",
      "                  Karimpur I CD Block has an area of <SAMPLE> .\n",
      "169440         => one hundred sixty four thousand four hundred forty || one hundred sixty nine thousand four hundred forty \n",
      "                  The ship had the UK Official Number <SAMPLE> and the Code Letters GKJW .\n",
      "1947           => nineteen thousand seven || nineteen forty seven \n",
      "                  The screenplay by E. Y. Harburg and Fred Saidy is based on their <SAMPLE> stage musical of the same name .\n",
      "139.10         => one hundred thirty nine point one one || one hundred thirty nine point one o \n",
      "                  Time <SAMPLE> ( 1992 ) : 50 .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110000   3% (  2m 29s)   0.078   |   0.00: 2007 -> two thousand seven (✓) \n",
      "1120000   7% (  4m 58s)   0.064   |   0.85: 305.90 -> three hundred five point five o o (✗: three hundred five point nine o) \n",
      "1130000  10% (  7m 27s)   0.081   |   0.00: 1 -> one (✓) \n",
      "1140000  13% (  9m 57s)   0.076   |   0.00: 2014 -> twenty fourteen (✓) \n",
      "1150000  17% ( 12m 25s)   0.059   |   0.00: August 1965 -> august nineteen sixty five (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.29% (    9629/   10000)\n",
      "1160000  20% ( 15m 43s)   0.064   |   0.00: 1986 -> nineteen eighty six (✓) \n",
      "1170000  23% ( 18m 11s)   0.070   |   0.00: 100 -> one hundred (✓) \n",
      "1180000  27% ( 20m 39s)   0.079   |   0.00: 2012 -> twenty twelve (✓) \n",
      "1190000  30% (  23m 8s)   0.056   |   0.01: 5,100 -> five thousand one hundred (✓) \n",
      "1200000  33% ( 25m 37s)   0.091   |   0.00: August 1863 -> august eighteen sixty three (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.53% (    9653/   10000)\n",
      "1210000  37% ( 28m 55s)   0.068   |   0.00: 1940s -> nineteen forties (✓) \n",
      "1220000  40% ( 31m 24s)   0.076   |   0.00: 128 -> one hundred twenty eight (✓) \n",
      "1230000  43% ( 33m 52s)   0.053   |   1.22: 978-1402238666 -> nine seven eight sil one sil four two two six six six six six six (✗: nine seven eight sil one four o two two three eight six six six) \n",
      "1240000  47% ( 36m 20s)   0.063   |   0.00: 14 April -> the fourteenth of april (✓) \n",
      "1250000  50% ( 38m 48s)   0.080   |   0.00: 2009 -> two thousand nine (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.40% (    9640/   10000)\n",
      "1260000  53% (  42m 8s)   0.074   |   0.00: 1970 -> nineteen seventy (✓) \n",
      "1270000  57% ( 44m 37s)   0.079   |   0.00: 32 -> thirty two (✓) \n",
      "1280000  60% (  47m 4s)   0.079   |   0.00: 220 -> two hundred twenty (✓) \n",
      "1290000  63% ( 49m 34s)   0.053   |   0.00: April 9, 2010 -> april ninth twenty ten (✓) \n",
      "1300000  67% (  52m 2s)   0.053   |   0.01: 158 -> one hundred fifty eight (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.41% (    9641/   10000)\n",
      "1310000  70% ( 55m 20s)   0.071   |   0.01: 1947 -> nineteen forty seven (✓) \n",
      "1320000  73% ( 57m 50s)   0.066   |   0.00: 125th -> one hundred twenty fifth (✓) \n",
      "1330000  77% ( 60m 17s)   0.052   |   0.00: 100 -> one hundred (✓) \n",
      "1340000  80% ( 62m 46s)   0.058   |   0.00: 8.9 km2 -> eight point nine square kilometers (✓) \n",
      "1350000  83% ( 65m 15s)   0.073   |   0.00: October 2011 -> october twenty eleven (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.45% (    9645/   10000)\n",
      "1360000  87% ( 68m 33s)   0.065   |   0.00: 1892 -> eighteen ninety two (✓) \n",
      "1370000  90% (  71m 3s)   0.082   |   0.01: 2003 -> two o o three (✓) \n",
      "1380000  93% ( 73m 30s)   0.062   |   0.00: 1906 -> nineteen o six (✓) \n",
      "1390000  97% (  76m 0s)   0.065   |   0.00: 2000 -> two thousand (✓) \n",
      "1400000 100% ( 78m 28s)   0.057   |   0.00: 3:00 p.m. -> three p m (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.30% (    9630/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61             => sixty one      || six one \n",
      "                  S- <SAMPLE> L Non amphibious civil transport version .\n",
      "1,251,160      => one million two hundred one thousand thousand hundred sixty sixty || one million two hundred fifty one thousand one hundred sixty \n",
      "                  He was elected to the Parliament representing Tehran with 729,965 ( 58.3% ) out of <SAMPLE> votes .\n",
      "III            => three          || the third \n",
      "                  Charles <SAMPLE> recovered these possessions in the Treaty of Paris ( 1763 ) , but ceded Florida to the British .\n",
      "Saturday, 5 March 1904 => fifth the fifteenth of august nineteen o four || saturday the fifth of march nineteen o four \n",
      "                  The four Third Round matches were scheduled for <SAMPLE> .\n",
      "1111           => eleven eleven  || one one one one \n",
      "                  Oregon State Capitol , R HMC- <SAMPLE> ( Salem Public Library ) Corning , Howard M. Dictionary of Oregon History .\n",
      "472            => four hundred seventy two || four seven two \n",
      "                  Maruoka escaped , and in 1977 , led the hijacking of Japan Airlines flight <SAMPLE> .\n",
      "1096           => ten ninety six || one o nine six \n",
      "                  \" LST- <SAMPLE> st Clair County \" .\n",
      "35(4) 614-35   => three sil five four four four sil sil four four sil five five || three five sil four sil six one four sil three five \n",
      "                  Systematic Entomology <SAMPLE> .\n",
      ".1563          => point one five five three || point one five six three \n",
      "                  Morris Wynn ( b <SAMPLE> , d . 1609 ) Robert WynnHe was the Custos Rotulorum of Caernarvonshire bef .\n",
      "0-295-96156-2  => o sil two nine five sil sil one nine six six sil sil two || o sil two nine five sil nine six one five six sil two \n",
      "                  ISBN <SAMPLE> Kangas , M. , & Cumming , W. ( 2005 ) .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410000   5% (  2m 36s)   0.037   |   0.00: August 3, 2013 -> august third twenty thirteen (✓) \n",
      "1420000  10% (  5m 11s)   0.050   |   0.54: $671,382 -> six hundred eighty three thousand one hundred seventy two dollars (✗: six hundred seventy one thousand three hundred eighty two dollars) \n",
      "1430000  15% (  7m 46s)   0.028   |   0.00: 1988 -> nineteen eighty eight (✓) \n",
      "1440000  20% ( 10m 32s)   0.067   |   0.00: 1656 -> sixteen fifty six (✓) \n",
      "1450000  25% ( 13m 10s)   0.056   |   0.00: 1874 -> eighteen seventy four (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.43% (    9643/   10000)\n",
      "1460000  30% ( 16m 36s)   0.051   |   0.00: 1999 -> nineteen ninety nine (✓) \n",
      "1470000  35% (  20m 1s)   0.056   |   0.00: 2013 -> twenty thirteen (✓) \n",
      "1480000  40% ( 23m 40s)   0.071   |   0.00: 1883 -> eighteen eighty three (✓) \n",
      "1490000  45% (  27m 4s)   0.057   |   0.00: 6 -> six (✓) \n",
      "1500000  50% ( 30m 43s)   0.054   |   0.00: 69.6% -> sixty nine point six percent (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.31% (    9631/   10000)\n",
      "1510000  55% ( 35m 29s)   0.051   |   0.00: 30 -> thirty (✓) \n",
      "1520000  60% (  39m 6s)   0.067   |   0.00: 981 -> nine hundred eighty one (✓) \n",
      "1530000  65% ( 42m 40s)   0.071   |   0.00: 14 -> fourteen (✓) \n",
      "1540000  70% ( 46m 14s)   0.045   |   0.00: 6 -> six (✓) \n",
      "1550000  75% ( 49m 44s)   0.071   |   0.00: 2 -> two (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.82% (    9682/   10000)\n",
      "1560000  80% ( 54m 28s)   0.082   |   0.15: I -> the first (✓) \n",
      "1570000  85% (  58m 3s)   0.056   |   0.09: 212 AD. -> two twelve a d (✓) \n",
      "1580000  90% ( 61m 34s)   0.052   |   0.00: 1996 -> nineteen ninety six (✓) \n",
      "1590000  95% ( 64m 56s)   0.058   |   0.00: 37 -> thirty seven (✓) \n",
      "1600000 100% ( 68m 20s)   0.064   |   0.00: Mar 25, 2010 -> march twenty fifth twenty ten (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.78% (    9678/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945           => one thousand nine hundred forty five || nineteen forty five \n",
      "                  Marxism in Britain : dissent , decline and re emergence <SAMPLE> - c .2000 .\n",
      "9781139917148  => nine trillion seven hundred eighty one billion hundred hundred one million one hundred one one one one one one one || nine trillion seven hundred eighty one billion one hundred thirty nine million nine hundred seventeen thousand one hundred forty eight \n",
      "                  ISBN 1139917145 , <SAMPLE> .\n",
      "3-8331-1253-0  => three sil eight three three one sil one three three three sil o || three sil eight three three one sil one two five three sil o \n",
      "                  ISBN <SAMPLE> \" Taxon : Mandevilla laxa ( Ruiz & Pav .\n",
      "V's            => the            || the fifth's \n",
      "                  The story is set in 1911 London at the time of George <SAMPLE> coronation .\n",
      "£6.829m        => six point two nine nine pounds || six point eight two nine million pounds \n",
      "                  Frank is the 29th largest PR consultancy in the UK with a fee income of <SAMPLE> .\n",
      "425/2008       => four hundred twenty five thousand thousand || four hundred twenty five two thousand eighths \n",
      "                  8 ( 3 ) , archived from the original on 13 September 2010 , inserted by the Parks and Trees ( Amendment ) Regulations 2008 ( S <SAMPLE> ) .\n",
      "143            => one hundred forty three || one four three \n",
      "                  Other riders winning races on the Honda 2 RC <SAMPLE> that year were : Luigi Taveri , Jim Redman , Kunimitsu Takahashi and Mike Hailwood .\n",
      "8,991 kWh      => eight thousand nine hundred ninety nine nine || eight thousand nine hundred ninety one kilo watt hours \n",
      "                  Spanish electricity usage constituted 88% of the EU 15 average ( EU 15 : 7,409 kWh / person ) , and 73% of the OECD average ( <SAMPLE> / person ) .\n",
      "100            => one hundred    || one o o \n",
      "                  Since 10 BASE - T and <SAMPLE> BASE - TX use pairs 2 and 3 , these two pairs must be swapped in the cable .\n",
      "25             => twenty five    || two five \n",
      "                  \" Factsheets : North American B- <SAMPLE> A \" .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610000   5% (  3m 28s)   0.041   |   0.00: 1966 -> nineteen sixty six (✓) \n",
      "1620000  10% (  6m 59s)   0.051   |   0.33: 132502 -> one hundred thirty two thousand five hundred (✗: one hundred thirty two thousand five hundred two) \n",
      "1630000  15% ( 10m 32s)   0.050   |   0.00: 31 December 2010 -> the thirty first of december twenty ten (✓) \n",
      "1640000  20% (  14m 6s)   0.058   |   0.00: 1 -> one (✓) \n",
      "1650000  25% ( 17m 43s)   0.050   |   0.00: 1999 -> nineteen ninety nine (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.69% (    9669/   10000)\n",
      "1660000  30% ( 22m 41s)   0.046   |   0.00: October 7, 2013 -> october seventh twenty thirteen (✓) \n",
      "1670000  35% ( 26m 17s)   0.053   |   0.00: October 30, 2005 -> october thirtieth two thousand five (✓) \n",
      "1680000  40% ( 29m 58s)   0.066   |   0.00: 2 -> two (✓) \n",
      "1690000  45% ( 33m 40s)   0.055   |   0.00: 2001 -> two thousand one (✓) \n",
      "1700000  50% ( 37m 11s)   0.050   |   0.00: 1586 -> fifteen eighty six (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.87% (    9687/   10000)\n",
      "1710000  55% ( 41m 38s)   0.047   |   0.00: 3 -> three (✓) \n",
      "1720000  60% (  45m 7s)   0.064   |   0.00: 2012-03-25 -> the twenty fifth of march twenty twelve (✓) \n",
      "1730000  65% ( 48m 40s)   0.067   |   0.02: II -> two (✓) \n",
      "1740000  70% (  52m 9s)   0.043   |   0.00: 200 -> two hundred (✓) \n",
      "1750000  75% ( 55m 42s)   0.065   |   0.00: 1995 -> nineteen ninety five (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.78% (    9678/   10000)\n",
      "1760000  80% ( 60m 27s)   0.039   |   0.00: 195 -> one hundred ninety five (✓) \n",
      "1770000  85% ( 63m 55s)   0.041   |   0.00: 65 -> sixty five (✓) \n",
      "1780000  90% ( 67m 23s)   0.039   |   0.00: 31 August -> the thirty first of august (✓) \n",
      "1790000  95% (  71m 0s)   0.074   |   0.00: 1987 -> nineteen eighty seven (✓) \n",
      "1800000 100% ( 74m 35s)   0.064   |   0.00: 29 -> twenty nine (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.95% (    9695/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "II             => two second     || two \n",
      "                  Ruanui <SAMPLE> lived there with his four sons Tarauaua , Tuwhenuaroa , Koromaiterangi and Tangaroatupo .\n",
      "978-3-85630-019-7 => nine seven eight sil three sil eight o three sil sil one six five sil sil seven || nine seven eight sil three sil eight five six three o sil o one nine sil seven \n",
      "                  Auflage 1985 , Daimon Verlag , ISBN <SAMPLE> Religioser Wahn und schwarze Magie , d . trag .\n",
      "978-0-9723410-1-1 => nine seven eight sil o sil seven o o sil sil sil one one one sil one || nine seven eight sil o sil nine seven two three four one o sil one sil one \n",
      "                  ISBN <SAMPLE> Finley , Ruth Elbright .\n",
      "Rs 535cr       => five hundred thirty five hundred || five hundred thirty five crore rupees \n",
      "                  \" NHAI cancels <SAMPLE> project for 4 - lane road in Coimbatore \" .\n",
      "5343           => five thousand three hundred forty three || five three four three \n",
      "                  It was developed by dr Bruce Wienke ( NAUI # <SAMPLE> L ) in 1988 at Los Alamos National Laboratories .\n",
      "54600          => fifty six thousand four hundred || five four six o o \n",
      "                  \" HP <SAMPLE> B Oscilloscope Easter Eggs \" .\n",
      "200913         => two hundred thousand thousand hundred hundred || two hundred thousand nine hundred thirteen \n",
      "                  27 July <SAMPLE> Parties Agree to Jointly Organize Protest Rallies .\n",
      "1903           => one o nine three || one nine o three \n",
      "                  Edwards ( <SAMPLE> a ) , p . 306 .\n",
      "25.549 km²     => twenty five point four five nine square kilometers || twenty five point five four nine square kilometers \n",
      "                  The village lies at an altitude of 150 metres and covers an area of <SAMPLE> .\n",
      "10 Million Dollar => the million first of || ten million dollars \n",
      "                  His professional career began in 1991 in the television film The <SAMPLE> Getaway ( 1991 ) as John Murray .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(7*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810000   2% (  5m 37s)   0.033   |   0.00: 1832 -> eighteen thirty two (✓) \n",
      "1820000   4% (  9m 39s)   0.059   |   0.00: 86.2% -> eighty six point two percent (✓) \n",
      "1830000   6% ( 13m 26s)   0.059   |   0.00: 2004 -> two thousand four (✓) \n",
      "1840000   8% ( 16m 49s)   0.041   |   0.00: 1993 -> nineteen ninety three (✓) \n",
      "1850000  10% ( 20m 12s)   0.053   |   0.00: July 19, 2014 -> july nineteenth twenty fourteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.52% (    9652/   10000)\n",
      "1860000  12% ( 24m 40s)   0.062   |   0.00: 2005 -> two thousand five (✓) \n",
      "1870000  14% (  28m 0s)   0.054   |   0.00: July 28 -> july twenty eighth (✓) \n",
      "1880000  16% ( 31m 21s)   0.072   |   0.00: 22 July -> the twenty second of july (✓) \n",
      "1890000  18% ( 34m 41s)   0.067   |   0.00: 124 -> one hundred twenty four (✓) \n",
      "1900000  20% ( 37m 57s)   0.044   |   0.00: 30 March 2013 -> the thirtieth of march twenty thirteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.87% (    9687/   10000)\n",
      "1910000  22% ( 42m 25s)   0.046   |   0.00: 95.3 -> ninety five point three (✓) \n",
      "1920000  24% ( 45m 47s)   0.049   |   0.00: 6 -> six (✓) \n",
      "1930000  26% (  49m 7s)   0.079   |   0.00: 33 -> thirty three (✓) \n",
      "1940000  28% ( 52m 26s)   0.043   |   0.00: 2006 -> two thousand six (✓) \n",
      "1950000  30% ( 55m 44s)   0.055   |   0.00: 10 December 2015 -> the tenth of december twenty fifteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/1950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.86% (    9686/   10000)\n",
      "1960000  32% (  60m 9s)   0.059   |   0.00: 20 -> twenty (✓) \n",
      "1970000  34% ( 63m 30s)   0.065   |   1.66: 1019285 -> one million nineteen eighty thousand two hundred eighty five (✗: one million nineteen thousand two hundred eighty five) \n",
      "1980000  36% ( 66m 52s)   0.041   |   0.00: 1287 -> twelve eighty seven (✓) \n",
      "1990000  38% ( 70m 12s)   0.048   |   0.00: 175 -> one hundred seventy five (✓) \n",
      "2000000  40% ( 73m 32s)   0.057   |   0.00: 14% -> fourteen percent (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/2000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 97.17% (    9717/   10000)\n",
      "2010000  42% ( 77m 55s)   0.046   |   0.00: 16 -> sixteen (✓) \n",
      "2020000  44% ( 81m 17s)   0.037   |   0.00: 2000 -> two thousand (✓) \n",
      "2030000  46% ( 84m 35s)   0.044   |   0.00: 21 June 2010 -> the twenty first of june twenty ten (✓) \n",
      "2040000  48% ( 87m 56s)   0.044   |   0.00: January 2000 -> january two thousand (✓) \n",
      "2050000  50% ( 91m 16s)   0.053   |   0.01: 130's -> one thirties (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/2050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.92% (    9692/   10000)\n",
      "2060000  52% ( 95m 44s)   0.070   |   0.00: 1944 -> nineteen forty four (✓) \n",
      "2070000  54% (  99m 0s)   0.049   |   0.00: 1831 -> eighteen thirty one (✓) \n",
      "2080000  56% (102m 21s)   0.054   |   0.05: 05 -> five (✓) \n",
      "2090000  58% (105m 43s)   0.080   |   0.00: 2.0 -> two point zero (✓) \n",
      "2100000  60% ( 109m 2s)   0.059   |   0.00: 2014 -> twenty fourteen (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/2100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.90% (    9690/   10000)\n",
      "2110000  62% (113m 30s)   0.067   |   0.00: April 17, 2000 -> april seventeenth two thousand (✓) \n",
      "2120000  64% (116m 49s)   0.055   |   0.00: 99.39% -> ninety nine point three nine percent (✓) \n",
      "2130000  66% ( 120m 7s)   0.041   |   0.00: 84 -> eighty four (✓) \n",
      "2140000  68% (123m 28s)   0.057   |   0.00: 3% -> three percent (✓) \n",
      "2150000  70% (130m 29s)   0.048   |   0.00: 1954 -> nineteen fifty four (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/2150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.83% (    9683/   10000)\n",
      "2160000  72% (134m 56s)   0.080   |   0.03: 11735 -> eleven thousand seven hundred thirty five (✓) \n",
      "2170000  74% (138m 16s)   0.053   |   0.00: 1985 -> nineteen eighty five (✓) \n",
      "2180000  76% (141m 32s)   0.050   |   0.00: 22 -> twenty two (✓) \n",
      "2190000  78% (144m 50s)   0.069   |   0.00: 1915 -> nineteen fifteen (✓) \n",
      "2200000  80% (148m 11s)   0.059   |   0.00: 31 August 2011 -> the thirty first of august twenty eleven (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/2200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.97% (    9697/   10000)\n",
      "2210000  82% (152m 37s)   0.042   |   0.01: 18 -> eighteen (✓) \n",
      "2220000  84% (155m 57s)   0.064   |   0.00: November 12, 2009 -> november twelfth two thousand nine (✓) \n",
      "2230000  86% (159m 19s)   0.041   |   0.00: 12 January 1777 -> the twelfth of january seventeen seventy seven (✓) \n",
      "2240000  88% (162m 36s)   0.054   |   0.00: July 28, 2014 -> july twenty eighth twenty fourteen (✓) \n",
      "2250000  90% (165m 56s)   0.051   |   0.00: 35 -> thirty five (✓) \n",
      "Saved model to data/models/numbers_gen_4_mod_data_1/2250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 97.04% (    9704/   10000)\n",
      "2260000  92% (170m 23s)   0.046   |   0.00: April 15, 2010 -> april fifteenth twenty ten (✓) \n",
      "2270000  94% (174m 16s)   0.032   |   0.00: September 1647 -> september sixteen forty seven (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_local_wrong_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ae80356ad3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_local_wrong_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print_local_wrong_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=500000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
