{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import bcolz\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "print(\"Pytorch: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_org = pd.read_csv('data/en_train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616107</th>\n",
       "      <td>49226</td>\n",
       "      <td>17</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684691</th>\n",
       "      <td>54634</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965529</th>\n",
       "      <td>76612</td>\n",
       "      <td>7</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id    class before after\n",
       "616107        49226        17  LETTERS    NaN   n a\n",
       "684691        54634         1    PLAIN    NaN   NaN\n",
       "965529        76612         7    PLAIN    NaN   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_org[pd.isnull(all_data_org['before'])][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 9918390,  (dropped none rows: 51)\n",
      "Data rows: 9840282,  (dropped rows: 78159)\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data_org.dropna()\n",
    "print(\"Data rows: {},  (dropped none rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data[all_data['class'] != 'VERBATIM']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we dropped VERBATIM class. Thats because it had so many weird characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS           522\n",
       "CARDINAL       133744\n",
       "DATE           258348\n",
       "DECIMAL          9821\n",
       "DIGIT            5442\n",
       "ELECTRONIC       5162\n",
       "FRACTION         1196\n",
       "LETTERS        152790\n",
       "MEASURE         14783\n",
       "MONEY            6128\n",
       "ORDINAL         12703\n",
       "PLAIN         7353647\n",
       "PUNCT         1880507\n",
       "TELEPHONE        4024\n",
       "TIME             1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_classes = list(all_data.groupby('class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%time\n",
    "data_balanced_classes = pd.concat([v.sample(min(10000, len(v))) for k, v in all_data_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_balance_randomize_classes(max_len=10000):\n",
    "    global data_balanced_classes\n",
    "    data_balanced_classes = pd.concat([v.sample(min(max_len, len(v))) for k, v in all_data_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 368 ms, sys: 0 ns, total: 368 ms\n",
      "Wall time: 367 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_balance_randomize_classes(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS         522\n",
       "CARDINAL      10000\n",
       "DATE          10000\n",
       "DECIMAL        9821\n",
       "DIGIT          5442\n",
       "ELECTRONIC     5162\n",
       "FRACTION       1196\n",
       "LETTERS       10000\n",
       "MEASURE       10000\n",
       "MONEY          6128\n",
       "ORDINAL       10000\n",
       "PLAIN         10000\n",
       "PUNCT         10000\n",
       "TELEPHONE      4024\n",
       "TIME           1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4209257</th>\n",
       "      <td>325591</td>\n",
       "      <td>11</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>3</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500854</th>\n",
       "      <td>118701</td>\n",
       "      <td>15</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379218</th>\n",
       "      <td>30697</td>\n",
       "      <td>8</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>$130M</td>\n",
       "      <td>one hundred thirty million dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377259</th>\n",
       "      <td>638971</td>\n",
       "      <td>3</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>5</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8356962</th>\n",
       "      <td>637474</td>\n",
       "      <td>9</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090182</th>\n",
       "      <td>86914</td>\n",
       "      <td>4</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>ABC-</td>\n",
       "      <td>a b c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848332</th>\n",
       "      <td>298248</td>\n",
       "      <td>3</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010146</th>\n",
       "      <td>536386</td>\n",
       "      <td>7</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>7th</td>\n",
       "      <td>seventh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984492</th>\n",
       "      <td>232473</td>\n",
       "      <td>1</td>\n",
       "      <td>TELEPHONE</td>\n",
       "      <td>0-679-75096-7</td>\n",
       "      <td>o sil six seven nine sil seven five o nine six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7899195</th>\n",
       "      <td>603269</td>\n",
       "      <td>0</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>5</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id      class         before  \\\n",
       "4209257       325591        11   CARDINAL              3   \n",
       "1500854       118701        15      PUNCT              .   \n",
       "379218         30697         8      MONEY          $130M   \n",
       "8377259       638971         3   CARDINAL              5   \n",
       "8356962       637474         9      PUNCT              .   \n",
       "1090182        86914         4    LETTERS           ABC-   \n",
       "3848332       298248         3      PUNCT              (   \n",
       "7010146       536386         7    ORDINAL            7th   \n",
       "2984492       232473         1  TELEPHONE  0-679-75096-7   \n",
       "7899195       603269         0   CARDINAL              5   \n",
       "\n",
       "                                                     after  \n",
       "4209257                                              three  \n",
       "1500854                                                  .  \n",
       "379218                  one hundred thirty million dollars  \n",
       "8377259                                               five  \n",
       "8356962                                                  .  \n",
       "1090182                                              a b c  \n",
       "3848332                                                  (  \n",
       "7010146                                            seventh  \n",
       "2984492  o sil six seven nine sil seven five o nine six...  \n",
       "7899195                                               five  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(name):\n",
    "    with open(name, 'r') as f: lines = [line.split() for line in f]\n",
    "    words = [d[0] for d in lines]\n",
    "    vecs = np.stack(np.array(d[1:], dtype=np.float32) for d in lines)\n",
    "    wordidx = {o:i for i,o in enumerate(words)}\n",
    "    return vecs, words, wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', \"'s\", 'asdf', '-', 'testaaa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_tokeniser(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' - ')\n",
    "    sent = re_punc.sub(r\" \\1 \", sent)\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()\n",
    "simple_tokeniser(\"asdf's   asdf   -testaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arr = [simple_tokeniser(s_)[0] for s_ in list(all_data.sample(1000)['before'])]\n",
    "[s in wv_idx for s in arr].count(True) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDRESS : B03003 -> b o three o o three : (1, 15, 50) <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = data_balanced_classes.iloc[random.randint(1, len(data_balanced_classes)-1)]\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    #rows = all_data[all_data['sentence_id']==sample_row['sentence_id']]\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = rows.before.values\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = '*****'\n",
    "    str_list = simple_tokeniser(' '.join(befores))\n",
    "    \n",
    "    word_vect = np.zeros((1, len(str_list), wv_vecs.shape[1]), dtype=np.float32)\n",
    "    # var = np.zeros((1, len(str_list), wv_vecs.shape[1]+1))\n",
    "    for i, w in enumerate(str_list):\n",
    "        if w=='*****':\n",
    "            word_vect[0][i] = np.zeros((1, wv_vecs.shape[1]))\n",
    "        else:\n",
    "            try:\n",
    "                word_vect[0][i] = wv_vecs[wv_idx[w]]\n",
    "            except KeyError:\n",
    "                word_vect[0][i] = np.random.rand(1, wv_vecs.shape[1])\n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], word_vect\n",
    "            \n",
    "# get_random_sample()\n",
    "s_bef, s_aft, s_class, s_word_v = get_random_sample()\n",
    "print(s_class, ':', s_bef, '->', s_aft, ':', s_word_v.shape, type(s_word_v[0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 µs ± 38.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories and Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAIN' 'PUNCT' 'DATE' 'LETTERS' 'CARDINAL' 'DECIMAL' 'MEASURE' 'MONEY'\n",
      " 'ORDINAL' 'TIME' 'ELECTRONIC' 'DIGIT' 'FRACTION' 'TELEPHONE' 'ADDRESS']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~¡£¥ª«²³µº»¼½¾¿éɒʻˈΩμ—€⅓⅔⅛⅝⅞\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "letters_all = sorted(list(set(''.join(all_data['before']))))\n",
    "letters_index = dict((c, i) for i, c in enumerate(letters_all))\n",
    "letters_n = len(letters_all)\n",
    "print(''.join(letters_all))\n",
    "print(len(letters_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 112])\n"
     ]
    }
   ],
   "source": [
    "def string_to_tensor(line):\n",
    "    tensor = torch.zeros(1, len(line), letters_n)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[0, li, letters_index[letter]] = 1\n",
    "    return tensor\n",
    "print(string_to_tensor('wordup').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_WORDS_CHARS_CLASS(nn.Module):\n",
    "    def __init__(self, wordvect_size, letters_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN_WORDS_CHARS_CLASS, self).__init__()\n",
    "\n",
    "        self.train_iterations = 0\n",
    "        self.train_history = []\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(wordvect_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(letters_size, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        #self.lin_1 = nn.Linear(hidden_size*2, 1024)\n",
    "        self.lin_output = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #output = self.lin_1(output)\n",
    "        output = self.lin_output(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(1, self.n_layers, self.hidden_size))\n",
    "        var1_2 = Variable(torch.zeros(1, self.n_layers, self.hidden_size))\n",
    "        var2_1 = Variable(torch.zeros(1, self.n_layers, self.hidden_size))\n",
    "        var2_2 = Variable(torch.zeros(1, self.n_layers, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "            var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))\n",
    "\n",
    "use_cuda = False\n",
    "s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "s_string = string_to_tensor(s_bef)\n",
    "model_tmp = RNN_WORDS_CHARS_CLASS(wordvect_size=s_word_vs.shape[-1], letters_size=letters_n,\n",
    "                              hidden_size=128, output_size=len(categories_all))\n",
    "print('Word vect size:', s_word_vs.shape, '. String vector size:', s_string.size())\n",
    "output = model_tmp(Variable(torch.from_numpy(s_word_vs)), Variable(s_string))\n",
    "print('Output:', output.size())\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp = use_cuda\n",
    "use_cuda = True\n",
    "model_tmp.cuda()\n",
    "output = model_tmp(Variable(torch.from_numpy(s_word_vs)).cuda(), Variable(s_string).cuda())\n",
    "use_cuda = tmp\n",
    "type(output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1)\n",
    "    category_i = top_i[0][0]\n",
    "    return categories_all[category_i], category_i\n",
    "\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_accuracy(model, n_sample=10000):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    for iteration in range(n_sample):\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "        s_string = Variable(string_to_tensor(s_bef))\n",
    "        s_word_vs = Variable(torch.from_numpy(s_word_vs))\n",
    "        if use_cuda:\n",
    "            s_word_vs = s_word_vs.cuda()\n",
    "            s_string = s_string.cuda()\n",
    "        output = model(s_word_vs, s_string)\n",
    "        if s_class == category_from_output(output)[0]:\n",
    "            n_correct += 1\n",
    "\n",
    "    print(\"Accuracy: {:>4.2%} ({:>8d}/{:>8d})\".format(\n",
    "            n_correct/n_sample, n_correct, n_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "test_model_accuracy(model_tmp.cuda(), n_sample=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, category, word_vectors, string, loss_function, optimizer):\n",
    "    category_tensor = Variable(torch.LongTensor([categories_index[category]]))\n",
    "    word_vectors = Variable(torch.from_numpy(word_vectors))\n",
    "    string = Variable(string_to_tensor(string))\n",
    "    if use_cuda:\n",
    "        category_tensor = category_tensor.cuda()\n",
    "        word_vectors = word_vectors.cuda()\n",
    "        string = string.cuda()\n",
    "\n",
    "    output = model(word_vectors, string)\n",
    "    loss = loss_function(output, category_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "train(model, s_class, s_word_vs, s_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, print_every=20000, plot_every=1000):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model.train_iterations += 1\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "\n",
    "        output, loss = train(model, s_class, s_word_vs, s_bef, loss_function, optimizer)\n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            guess, guess_i = category_from_output(output)\n",
    "            correct = '✓' if guess == s_class else \"✗ ({})\".format(s_class)\n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} {}\".format(\n",
    "                model.train_iterations, iteration/n_iters, timeSince(start),\n",
    "                current_loss/current_loss_iter, loss,\n",
    "                s_bef, guess, correct ))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model.train_history.append((current_loss / plot_every, lr))\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "    \n",
    "    test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN_WORDS_CHARS_CLASS(wordvect_size=s_word_vs.shape[-1], letters_size=letters_n,\n",
    "                              hidden_size=128, output_size=len(categories_all)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=10000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(len(categories_all), len(categories_all))\n",
    "n_confusion = 100000\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "    word_vectors = Variable(torch.from_numpy(s_word_vs))\n",
    "    string = Variable(string_to_tensor(s_bef))\n",
    "    if use_cuda:\n",
    "        word_vectors = word_vectors.cuda()\n",
    "        string = string.cuda()\n",
    "    output = model(word_vects, string)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = categories_index[s_class]\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(len(categories_all)):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + list(categories_all), rotation=90)\n",
    "ax.set_yticklabels([''] + list(categories_all))\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_losses = [arr[0] for arr in model.train_history]\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
