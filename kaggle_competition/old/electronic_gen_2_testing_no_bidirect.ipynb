{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'electronic_gen_2_testing_no_bidirect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data = pickle.load(open(\"data/en_train_fixed_4_sentences.pkl\", \"rb\" ))\n",
    "all_data = pickle.load(open(\"data/en_train_fixed_5_manual.pkl\", \"rb\" ))\n",
    "# all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 4964,  (dropped rows: 9913228)\n"
     ]
    }
   ],
   "source": [
    "sample_data = all_data.copy()\n",
    "sample_data = sample_data[sample_data['class'] == 'ELECTRONIC']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(sample_data), len(all_data)-len(sample_data)))\n",
    "sample_data = sample_data.reset_index(drop=True)\n",
    "del(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ELECTRONIC']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "categories_all = sorted(sample_data[\"class\"].unique())\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', 'two', 'twenty']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_after_common = pickle.load(open(\"data/en_features/words_after_ext.pkl\", 'rb'))\n",
    "words_after_index = dict((c, i) for i, c in enumerate(words_after_common))\n",
    "words_after_common[0:7]\n",
    "len(words_after_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1351])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_tensor = torch.zeros(1, 1, len(words_after_index))\n",
    "sos_tensor[0, 0, words_after_index[SOS_TOKEN]] = 1\n",
    "sos_tensor.size()\n",
    "#del(onehot_sos)\n",
    "\n",
    "# sos_tensor = torch.LongTensor([words_after_index[SOS_TOKEN]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data_classes_select = list(sample_data.groupby('class'))\n",
    "\n",
    "balanced_data_accessed_counter = 0 \n",
    "balanced_data_randomize_freq = False\n",
    "balanced_data_length = 0\n",
    "\n",
    "def balanced_data_randomize_org(max_len=20000):\n",
    "    global balanced_data, balanced_data_length, balanced_data_accessed_counter, balanced_data_randomize_freq\n",
    "    balanced_data = pd.concat([v.sample(min(max_len, len(v))) for k, v in balanced_data_classes_select])\n",
    "    balanced_data_length = len(balanced_data)\n",
    "    balanced_data_randomize_freq = balanced_data_length * 0.2\n",
    "    balanced_data_accessed_counter = 0\n",
    "balanced_data_randomize = balanced_data_randomize_org\n",
    "\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_accessed_counter\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_accessed_counter += 1\n",
    "    #if balanced_data_randomize_freq and balanced_data_accessed_counter > balanced_data_randomize_freq:\n",
    "    #    balanced_data_randomize()\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample\n",
    "    \n",
    "balanced_data_randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4964"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ELECTRONIC    4964\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data.groupby('class')['class'].count()\n",
    "#sample_data.groupby('class')['class'].count()\n",
    "balanced_data.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id                                               182347\n",
       "token_id                                                      12\n",
       "class                                                 ELECTRONIC\n",
       "before                                               NYTimes.com\n",
       "after                                    n y t i m e s dot c o m\n",
       "class_org                                             ELECTRONIC\n",
       "a_word_ind       [29, 86, 30, 31, 32, 28, 17, 74, 21, 25, 32, 0]\n",
       "sentence       \" caroline azar — about this person — movies &...\n",
       "Name: 1130, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRONIC : http://llt.msu.edu/vol10num1/fitzdebski/default.html -> h t t p colon slash slash l l t dot m s u dot e d u slash v o l u m e t e n n u m o n e slash f i t z d e b s k i slash d e f a u l t dot h t m l <EOS> [45, 30, 30, 24, 129, 101, 101, 42, 42, 30, 74, 32, 17, 43, 74, 28, 26, 43, 101, 54, 25, 42, 43, 32, 28, 30, 28, 29, 29, 43, 32, 25, 29, 28, 101, 37, 31, 30, 105, 26, 28, 36, 17, 59, 31, 101, 26, 28, 37, 22, 43, 42, 30, 74, 45, 30, 32, 42, 0]\n",
      "retrieved 10 august 2006 from <SAMPLE> .\n",
      "torch.Size([1, 53, 104])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()   \n",
    "    return sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence']\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    s_aft_str = ' '.join([words_after_common[i] for i in s_aft])\n",
    "    print(s_class, ':', s_bef, '->', s_aft_str, s_aft)\n",
    "    print(s_sentence)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 µs ± 1.53 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ATTENTION_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>506099</td>\n",
       "      <td>18</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.landofthebrave.info/colonial-times....</td>\n",
       "      <td>h t t p colon slash slash w w w dot l a n d o ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>p . h . spectre and d . larkin , wooden shp ( ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>132576</td>\n",
       "      <td>24</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>http://www.kickspeed.com.au/Improve-measure-ki...</td>\n",
       "      <td>h t t p colon slash slash w w w dot k i c k s ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...</td>\n",
       "      <td>hewitt ( 2006 ) , p . 42 hewitt ( 2006 ) , p ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_id  token_id       class  \\\n",
       "3351       506099        18  ELECTRONIC   \n",
       "787        132576        24  ELECTRONIC   \n",
       "\n",
       "                                                 before  \\\n",
       "3351  http://www.landofthebrave.info/colonial-times....   \n",
       "787   http://www.kickspeed.com.au/Improve-measure-ki...   \n",
       "\n",
       "                                                  after   class_org  \\\n",
       "3351  h t t p colon slash slash w w w dot l a n d o ...  ELECTRONIC   \n",
       "787   h t t p colon slash slash w w w dot k i c k s ...  ELECTRONIC   \n",
       "\n",
       "                                             a_word_ind  \\\n",
       "3351  [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "787   [45, 30, 30, 24, 129, 101, 101, 52, 52, 52, 74...   \n",
       "\n",
       "                                               sentence  \n",
       "3351  p . h . spectre and d . larkin , wooden shp ( ...  \n",
       "787   hewitt ( 2006 ) , p . 42 hewitt ( 2006 ) , p ....  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "      <th>a_word_ind</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>339237</td>\n",
       "      <td>6</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>Resultsnola.combestofneworleans.comthehullabal...</td>\n",
       "      <td>r e s u l t s n o l a dot c o m b e s t o f n ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[35, 28, 17, 43, 42, 30, 17, 29, 25, 42, 22, 7...</td>\n",
       "      <td>broadway to vegas february 15 , 2004 newslibra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>313820</td>\n",
       "      <td>0</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>Findagrave.comFindagrave.comFindagrave.comFind...</td>\n",
       "      <td>f i n d a g r a v e dot c o m f i n d a g r a ...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>[37, 31, 29, 26, 22, 53, 35, 22, 54, 28, 74, 2...</td>\n",
       "      <td>&lt;SAMPLE&gt; \" hibbard , spencer , bartlett &amp; co \" .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_id  token_id       class  \\\n",
       "2187       339237         6  ELECTRONIC   \n",
       "2021       313820         0  ELECTRONIC   \n",
       "\n",
       "                                                 before  \\\n",
       "2187  Resultsnola.combestofneworleans.comthehullabal...   \n",
       "2021  Findagrave.comFindagrave.comFindagrave.comFind...   \n",
       "\n",
       "                                                  after   class_org  \\\n",
       "2187  r e s u l t s n o l a dot c o m b e s t o f n ...  ELECTRONIC   \n",
       "2021  f i n d a g r a v e dot c o m f i n d a g r a ...  ELECTRONIC   \n",
       "\n",
       "                                             a_word_ind  \\\n",
       "2187  [35, 28, 17, 43, 42, 30, 17, 29, 25, 42, 22, 7...   \n",
       "2021  [37, 31, 29, 26, 22, 53, 35, 22, 54, 28, 74, 2...   \n",
       "\n",
       "                                               sentence  \n",
       "2187  broadway to vegas february 15 , 2004 newslibra...  \n",
       "2021   <SAMPLE> \" hibbard , spencer , bartlett & co \" .  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = sample_data[sample_data['before'].str.len()>MAX_ATTENTION_LENGTH]\n",
    "len(tmp)\n",
    "tmp.sample(2)\n",
    "tmp[~tmp['before'].str.contains('/')].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(50, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 256, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "        self.hidden_size = words_hidden_size + chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size, chars_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                                #batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        hidden_states_cat = Variable(torch.zeros(MAX_ATTENTION_LENGTH, self.hidden_size)).cuda()\n",
    "        for ei in range(min(MAX_ATTENTION_LENGTH, len(string_tensor[0]))):\n",
    "            hidden_states_cat[ei] = torch.cat((output_words, all_outputs_chars[0, ei].view(1,-1)), 1)\n",
    "                \n",
    "        output = torch.cat((output_words[0], output_chars[0]), 0)\n",
    "\n",
    "        #output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #return output, all_outputs_chars\n",
    "        return output, hidden_states_cat\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(1 * self.chars_layers, 1, self.chars_hidden_size))\n",
    "        var2_2 = Variable(torch.zeros(1 * self.chars_layers, 1, self.chars_hidden_size))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))\n",
    "    \n",
    "    \n",
    "encoder_rnn = EncoderRNN(words_input_size=wv_vecs.shape[-1], chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=256,\n",
    "                         words_layers=1, chars_layers=1).cuda()\n",
    "encoder_rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports-reference.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sports-reference.com'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 384])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_bef)\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t), s_bef\n",
    "    \n",
    "(encoder_output, encoder_outputs), tmp = test_encoder_single_sample()\n",
    "tmp\n",
    "encoder_output.size()\n",
    "encoder_outputs.size()\n",
    "torch.eq(encoder_output, encoder_outputs[len(tmp)]).data.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): GRU(384, 384, batch_first=True)\n",
       "  (lin_out): Linear (384 -> 1351)\n",
       "  (emb_lin): Linear (1351 -> 384)\n",
       "  (attn): Linear (768 -> 100)\n",
       "  (attn_combine): Linear (768 -> 384)\n",
       "  (dropout): Dropout (p = 0.1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 1, 384]), torch.Size([1, 100])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p=0.1, max_length=MAX_ATTENTION_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        #self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.emb_lin = nn.Linear(input_size, hidden_size)\n",
    "        self.max_length = max_length\n",
    "        self.attn = nn.Linear(hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    #def forward(self, char, hidden):\n",
    "    def forward(self, last_input, hidden, encoder_outputs):\n",
    "        #embedded = self.embedding(last_input).view(1, 1, -1)\n",
    "        embedded = self.emb_lin(last_input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #IPython.core.debugger.Pdb().set_trace()\n",
    "        #attn_weights = F.softmax(self.attn(torch.cat((char[0], hidden[0]), 1)))\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.lin_out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        if self.n_layers > 1:\n",
    "            hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "            res = torch.cat((input_var, hid_var), 0)\n",
    "            return res\n",
    "        else:\n",
    "            return input_var\n",
    "        \n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(words_after_common), hidden_size=encoder_output.size()[0], n_layers=1)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn\n",
    "\n",
    "tmp_outputs = Variable(torch.zeros(MAX_ATTENTION_LENGTH, encoder_output.size()[0])).cuda()\n",
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_hiddens, tmp_outputs)\n",
    "[v.size() for v in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  1.2000  2.4000  3.6000  4.8000\n",
       "[torch.FloatTensor of size 1x1x4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(torch.FloatTensor([1,0.1,0.1]).view(1,1,-1), torch.arange(0, 12).view(1,3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1351]), torch.Size([1, 1, 384]), torch.Size([1, 100])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 14\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "five\n"
     ]
    }
   ],
   "source": [
    "tmp_outputs = Variable(torch.zeros(MAX_ATTENTION_LENGTH, encoder_output.size()[0])).cuda()\n",
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp = decoder_rnn(Variable(sos_tensor).cuda(), tmp_hiddens, tmp_outputs)\n",
    "[v.size() for v in tmp]\n",
    "print(tmp[0].topk(1)[1])\n",
    "print(words_after_common[tmp[0].topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('five kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor',\n",
       " 'five kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor',\n",
       " 'v i p e r e x c h a n g e dot c o m',\n",
       " ('ViperExchange.com',\n",
       "  [54, 31, 24, 28, 35, 28, 97, 21, 45, 22, 29, 53, 28, 74, 21, 25, 32, 0],\n",
       "  'ELECTRONIC',\n",
       "  '\" track issue knocks <SAMPLE> from winning contention at laguna seca \" .'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None, return_more=False, sample=False):\n",
    "    decoder_rnn.eval()\n",
    "    encoder_rnn.eval()\n",
    "    \n",
    "    if not sample:\n",
    "        sample = get_random_sample()\n",
    "    s_bef, s_aft, s_class, s_sentence = sample\n",
    "        \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    #encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_outputs[0].view(1,1,-1))\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    decoder_attns_arr = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attns_arr.append(decoder_attns)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        #decoder_input = Variable(torch.LongTensor([word_index])).cuda()\n",
    "        \n",
    "        decoder_input = torch.zeros(1, 1, len(words_after_index))\n",
    "        decoder_input[0, 0, word_index] = 1\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    sample_target = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "    \n",
    "    if return_more:\n",
    "        return output, decoded_output, decoder_attns_arr, sample\n",
    "    \n",
    "    return output, output, sample_target, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes.com    => five five kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor || [29, 86, 30, 31, 32, 28, 17, 74, 21, 25, 32, 0] \n",
      "                  \" gale storm , 87 , is dead ; earned television fame for her wholesome roles \" , <SAMPLE> , june 29 , 2009 ; accessed december 14 , 2015 .\n",
      "Amazon.de      => five kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor || [22, 32, 22, 105, 25, 29, 74, 26, 28, 0] \n",
      "                  <SAMPLE> ( in german ) .\n"
     ]
    }
   ],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', s_sentence, ))\n",
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/     100)\n",
      "CPU times: user 2.34 s, sys: 20 ms, total: 2.36 s\n",
      "Wall time: 2.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample, n_sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = Variable(words_to_word_vectors_tensor(list(s_sentence), wv_vecs, wv_idx)).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output, encoder_outputs = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    #encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_outputs[0].view(1,1,-1))\n",
    "    decoder_input = Variable(sos_tensor).cuda()\n",
    "    \n",
    "    ###\n",
    "    target_arr = s_aft\n",
    "    #target_arr = after_sentence_to_word_indexes(s_aft, include_eos=True)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden, decoder_attns = decoder_rnn(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        #decoder_target_i = chars_after_index[target_arr[i]]\n",
    "        decoder_target_i = target_arr[i]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = words_after_common[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word_index = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_input = torch.zeros(1, 1, len(words_after_index))\n",
    "        decoder_input[0, 0, word_index] = 1\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        #decoder_input = Variable(torch.LongTensor([word_index])).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            s_aft_sentence = ' '.join([words_after_common[w] for w in s_aft][:-1])\n",
    "            correct = '✓' if result == s_aft_sentence else \"✗: {}\".format(s_aft_sentence)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/electronic_gen_2_testing_no_bidirect\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   7.195   |   7.18: //www.zirox.co.in/resources.htm -> five kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor kronor (✗: s l a s h s l a s h w w w dot z i r o x dot c o dot i n s l a s h r e s o u r c e s dot h t m) (forcing)\n",
      "Saved model to data/models/electronic_gen_2_testing_no_bidirect/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (   4m 0s)   6.950   |   7.14: Newspaperarchive.com -> w kronor kronor <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> (✗: n e w s p a p e r a r c h i v e dot c o m) (forcing)\n",
      "    27  54% (   4m 1s)   6.365   |   7.11: forbes.com -> w w <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> (✗: f o r b e s dot c o m) (forcing)\n",
      "    36  72% (   4m 1s)   5.966   |   1.33: TODAY.comAssess -> w e (✗: t o d a y dot c o m a s s e s s) \n",
      "    45  90% (   4m 2s)   5.785   |   0.59: Leakymails.blogspot.com -> e (✗: l e a k y m a i l s dot b l o g s p o t dot c o m) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (  0m 37s)   3.203   |   3.19: http://articles.economictimes.indiatimes.com/2011-12-17/news/30528703_1_indian-tea-industry-tea-production-tea-estateshttp://www.thehindu.com/todays-paper/tp-features/tp-metroplus/kadak-chai/article4250743.ece -> h e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e (✗: h t t p colon slash slash a r t i c l e s dot e c o n o m i c t i m e s dot i n d i a t i m e s dot com slash t w e n t y e l e v e n dash t w e l v e dash s e v e n t e e n slash n e w s slash t h r e e o f i v e t w o e i g h t s e v e n o t h r e e u n d e r s c o r e o n e u n d e r s c o r e i n d i a n dash t e a dash i n d u s t r y dash t e a dash p r o d u c t i o n dash t e a dash e s t a t e s h t t p colon slash slash w w w dot t h e h i n d u dot com slash t o d a y s dash p a p e r slash t p dash f e a t u r e s slash t p dash m e t r o p l u s slash k a d a k dash c h a i slash a r t i c l e f o u r t w o f i v e o s e v e n f o u r t h r e e dot e c e) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11000  11% ( 13m 49s)   2.107   |   2.03: thisiswarrington.co.uk -> t h e t h a h a r d a t d r a r a o o m u k (✗: t h i s i s w a r r i n g t o n dot c o dot u k) (forcing)\n",
      " 21000  22% ( 27m 51s)   1.984   |   2.99: http://www.uscg.mil/hq/cg5/cg5214/survivalequip.asppicture -> h t t p colon slash slash w w w dot f o r t s dot c e n i slash e slash e slash s o l e e t t r a o e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e (✗: h t t p colon slash slash w w w dot u s c g dot m i l slash h q slash c g f i v e slash c g f i v e t w o o n e f o u r slash s u r v i v a l e q u i p dot a s p p i c t u r e) \n",
      " 31000  33% ( 42m 11s)   1.898   |   2.41: Soundtrack.net -> s e a r e e e e e e e dot e o (✗: s o u n d t r a c k dot n e t) \n",
      " 41000  44% ( 56m 47s)   1.832   |   1.30: wif.org -> w w w dot o r g (✗: w i f dot o r g) \n",
      "Saved model to data/models/electronic_gen_2_testing_no_bidirect/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 8.34% (     834/   10000)\n",
      " 51000  56% ( 74m 32s)   1.813   |   1.33: fas.org -> f f dot dot f r g (✗: f a s dot o r g) (forcing)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5bd73cf9ea7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-27807fc3707c>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(n_iters, lr, teacher_forcing_ratio, print_every, plot_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m                              \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                              \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                              max_length=40 )\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-95336e4ebf15>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function, use_teacher_forcing, max_length)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   nationalarchives.gov.ukCWGC\n",
      "output:  n i n i n e t i c n a s l a s h s e a s\n",
      "target:    nationalarchivesdotgovdotukcwgc\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAFeCAYAAAAoppiuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd6702abe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def debug_show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    #ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    #ax.set_xticklabels([''] + input_sentence + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    \n",
    "    input_sentence = input_sentence + ['<EOS>']\n",
    "    inp_arr = [\"{}\\n{}\".format(input_sentence[i], input_sentence[-1-i]) for i in range(len(input_sentence))]\n",
    "    ax.set_xticklabels([''] + inp_arr, rotation=0)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def debug_eval_sample_show_attention():\n",
    "    \n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sample_row = balanced_data[balanced_data['before'].str.len()>15].sample(1).iloc[0]\n",
    "    sample = sample_row['before'], sample_row['a_word_ind'], sample_row['class'], sample_row['sentence']\n",
    "\n",
    "    output, decoded_output, decoder_attns_arr, sample = test_model_single_sample(None, \n",
    "                                                            return_more=True, sample=sample)\n",
    "    print('input:  ', sample[0])\n",
    "    print('output: ', ' '.join(decoded_output))\n",
    "    print('target:   ', ''.join([words_after_common[w] for w in sample[1][:-1]]))\n",
    "\n",
    "    attns = np.array([arr.data[0].cpu().numpy() for arr in decoder_attns_arr])\n",
    "\n",
    "    debug_show_attention(list(sample[0]), decoded_output, attns)\n",
    "    #plt.matshow(attns)\n",
    "\n",
    "debug_eval_sample_show_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
