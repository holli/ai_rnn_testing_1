{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import bcolz\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "print(\"Pytorch: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_org = pd.read_csv('data/en_train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616107</th>\n",
       "      <td>49226</td>\n",
       "      <td>17</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684691</th>\n",
       "      <td>54634</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965529</th>\n",
       "      <td>76612</td>\n",
       "      <td>7</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id    class before after\n",
       "616107        49226        17  LETTERS    NaN   n a\n",
       "684691        54634         1    PLAIN    NaN   NaN\n",
       "965529        76612         7    PLAIN    NaN   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_org[pd.isnull(all_data_org['before'])][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 9918390,  (dropped none rows: 51)\n",
      "Data rows: 9840282,  (dropped rows: 78159)\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data_org.dropna()\n",
    "print(\"Data rows: {},  (dropped none rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data[all_data['class'] != 'VERBATIM']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we dropped VERBATIM class. Thats because it had so many weird characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS           522\n",
       "CARDINAL       133744\n",
       "DATE           258348\n",
       "DECIMAL          9821\n",
       "DIGIT            5442\n",
       "ELECTRONIC       5162\n",
       "FRACTION         1196\n",
       "LETTERS        152790\n",
       "MEASURE         14783\n",
       "MONEY            6128\n",
       "ORDINAL         12703\n",
       "PLAIN         7353647\n",
       "PUNCT         1880507\n",
       "TELEPHONE        4024\n",
       "TIME             1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_classes = list(all_data.groupby('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_balance_randomize_classes(max_len=10000):\n",
    "    global data_balanced_classes\n",
    "    data_balanced_classes = pd.concat([v.sample(min(max_len, len(v))) for k, v in all_data_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "data_balance_randomize_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS         522\n",
       "CARDINAL      10000\n",
       "DATE          10000\n",
       "DECIMAL        9821\n",
       "DIGIT          5442\n",
       "ELECTRONIC     5162\n",
       "FRACTION       1196\n",
       "LETTERS       10000\n",
       "MEASURE       10000\n",
       "MONEY          6128\n",
       "ORDINAL       10000\n",
       "PLAIN         10000\n",
       "PUNCT         10000\n",
       "TELEPHONE      4024\n",
       "TIME           1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9790516</th>\n",
       "      <td>744382</td>\n",
       "      <td>10</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>20th</td>\n",
       "      <td>twentieth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6350766</th>\n",
       "      <td>487015</td>\n",
       "      <td>12</td>\n",
       "      <td>DIGIT</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870172</th>\n",
       "      <td>525935</td>\n",
       "      <td>1</td>\n",
       "      <td>TELEPHONE</td>\n",
       "      <td>0-9549241-0</td>\n",
       "      <td>o sil nine five four nine two four one sil o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151317</th>\n",
       "      <td>12238</td>\n",
       "      <td>2</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>J.</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6944132</th>\n",
       "      <td>531470</td>\n",
       "      <td>8</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>metal</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8537951</th>\n",
       "      <td>650975</td>\n",
       "      <td>2</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>10</td>\n",
       "      <td>ten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151809</th>\n",
       "      <td>396853</td>\n",
       "      <td>4</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>P.</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500186</th>\n",
       "      <td>573249</td>\n",
       "      <td>4</td>\n",
       "      <td>DATE</td>\n",
       "      <td>2011</td>\n",
       "      <td>twenty eleven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988976</th>\n",
       "      <td>308901</td>\n",
       "      <td>9</td>\n",
       "      <td>MEASURE</td>\n",
       "      <td>35 km</td>\n",
       "      <td>thirty five kilometers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119710</th>\n",
       "      <td>619750</td>\n",
       "      <td>10</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>16th</td>\n",
       "      <td>sixteenth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id      class       before  \\\n",
       "9790516       744382        10    ORDINAL         20th   \n",
       "6350766       487015        12      DIGIT            2   \n",
       "6870172       525935         1  TELEPHONE  0-9549241-0   \n",
       "151317         12238         2    LETTERS           J.   \n",
       "6944132       531470         8      PLAIN        metal   \n",
       "8537951       650975         2   CARDINAL           10   \n",
       "5151809       396853         4    LETTERS           P.   \n",
       "7500186       573249         4       DATE         2011   \n",
       "3988976       308901         9    MEASURE        35 km   \n",
       "8119710       619750        10    ORDINAL         16th   \n",
       "\n",
       "                                                after  \n",
       "9790516                                     twentieth  \n",
       "6350766                                           two  \n",
       "6870172  o sil nine five four nine two four one sil o  \n",
       "151317                                              j  \n",
       "6944132                                         metal  \n",
       "8537951                                           ten  \n",
       "5151809                                             p  \n",
       "7500186                                 twenty eleven  \n",
       "3988976                        thirty five kilometers  \n",
       "8119710                                     sixteenth  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(name):\n",
    "    with open(name, 'r') as f: lines = [line.split() for line in f]\n",
    "    words = [d[0] for d in lines]\n",
    "    vecs = np.stack(np.array(d[1:], dtype=np.float32) for d in lines)\n",
    "    wordidx = {o:i for i,o in enumerate(words)}\n",
    "    return vecs, words, wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', \"'s\", 'asdf', '-', 'testaaa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_tokeniser(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' - ')\n",
    "    sent = re_punc.sub(r\" \\1 \", sent)\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()\n",
    "simple_tokeniser(\"asdf's   asdf   -testaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arr = [simple_tokeniser(s_)[0] for s_ in list(all_data.sample(1000)['before'])]\n",
    "[s in wv_idx for s in arr].count(True) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEASURE : 7\" -> seven inches : (1, 11, 50) <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = data_balanced_classes.iloc[random.randint(1, len(data_balanced_classes)-1)]\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    #rows = all_data[all_data['sentence_id']==sample_row['sentence_id']]\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = rows.before.values\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = '*****'\n",
    "    str_list = simple_tokeniser(' '.join(befores))\n",
    "    \n",
    "    word_vect = np.zeros((1, len(str_list), wv_vecs.shape[1]), dtype=np.float32)\n",
    "    # var = np.zeros((1, len(str_list), wv_vecs.shape[1]+1))\n",
    "    for i, w in enumerate(str_list):\n",
    "        if w=='*****':\n",
    "            word_vect[0][i] = np.zeros((1, wv_vecs.shape[1]))\n",
    "        else:\n",
    "            try:\n",
    "                word_vect[0][i] = wv_vecs[wv_idx[w]]\n",
    "            except KeyError:\n",
    "                word_vect[0][i] = np.random.rand(1, wv_vecs.shape[1])\n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], word_vect\n",
    "            \n",
    "# get_random_sample()\n",
    "s_bef, s_aft, s_class, s_word_v = get_random_sample()\n",
    "print(s_class, ':', s_bef, '->', s_aft, ':', s_word_v.shape, type(s_word_v[0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 µs ± 3.28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories and Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAIN' 'PUNCT' 'DATE' 'LETTERS' 'CARDINAL' 'DECIMAL' 'MEASURE' 'MONEY'\n",
      " 'ORDINAL' 'TIME' 'ELECTRONIC' 'DIGIT' 'FRACTION' 'TELEPHONE' 'ADDRESS']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~¡£¥ª«²³µº»¼½¾¿éɒʻˈΩμ—€⅓⅔⅛⅝⅞\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "letters_all = sorted(list(set(''.join(all_data['before']))))\n",
    "letters_index = dict((c, i) for i, c in enumerate(letters_all))\n",
    "letters_n = len(letters_all)\n",
    "print(''.join(letters_all))\n",
    "print(len(letters_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 112])\n"
     ]
    }
   ],
   "source": [
    "def string_to_tensor(line):\n",
    "    tensor = torch.zeros(1, len(line), letters_n)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[0, li, letters_index[letter]] = 1\n",
    "    return tensor\n",
    "print(string_to_tensor('wordup').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vect size: (1, 14, 50) . String vector size: torch.Size([1, 2, 112])\n",
      "Output: torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "class RNN_WORDS_CHARS_CLASS(nn.Module):\n",
    "    def __init__(self, wordvect_size, letters_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN_WORDS_CHARS_CLASS, self).__init__()\n",
    "\n",
    "        self.train_iterations = 0\n",
    "        self.train_history = []\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(wordvect_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(letters_size, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        #self.lin_1 = nn.Linear(hidden_size*2, 1024)\n",
    "        self.lin_output = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #output = self.lin_1(output)\n",
    "        output = self.lin_output(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        var1_2 = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        var2_1 = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        var2_2 = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "            var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))\n",
    "\n",
    "use_cuda = False\n",
    "s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "s_string = string_to_tensor(s_bef)\n",
    "model_tmp = RNN_WORDS_CHARS_CLASS(wordvect_size=s_word_vs.shape[-1], letters_size=letters_n,\n",
    "                              hidden_size=128, output_size=len(categories_all))\n",
    "print('Word vect size:', s_word_vs.shape, '. String vector size:', s_string.size())\n",
    "output = model_tmp(Variable(torch.from_numpy(s_word_vs)), Variable(s_string))\n",
    "print('Output:', output.size())\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp = use_cuda\n",
    "use_cuda = True\n",
    "model_tmp.cuda()\n",
    "output = model_tmp(Variable(torch.from_numpy(s_word_vs)).cuda(), Variable(s_string).cuda())\n",
    "use_cuda = tmp\n",
    "type(output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ORDINAL', 8)\n"
     ]
    }
   ],
   "source": [
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1)\n",
    "    category_i = top_i[0][0]\n",
    "    return categories_all[category_i], category_i\n",
    "\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_accuracy(model, n_sample=10000):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    for iteration in range(n_sample):\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "        s_string = Variable(string_to_tensor(s_bef))\n",
    "        s_word_vs = Variable(torch.from_numpy(s_word_vs))\n",
    "        if use_cuda:\n",
    "            s_word_vs = s_word_vs.cuda()\n",
    "            s_string = s_string.cuda()\n",
    "        output = model(s_word_vs, s_string)\n",
    "        if s_class == category_from_output(output)[0]:\n",
    "            n_correct += 1\n",
    "\n",
    "    print(\"Accuracy: {:>4.2%} ({:>8d}/{:>8d})\".format(\n",
    "            n_correct/n_sample, n_correct, n_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "test_model_accuracy(model_tmp.cuda(), n_sample=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, category, word_vectors, string, loss_function, optimizer):\n",
    "    category_tensor = Variable(torch.LongTensor([categories_index[category]]))\n",
    "    word_vectors = Variable(torch.from_numpy(word_vectors))\n",
    "    string = Variable(string_to_tensor(string))\n",
    "    if use_cuda:\n",
    "        category_tensor = category_tensor.cuda()\n",
    "        word_vectors = word_vectors.cuda()\n",
    "        string = string.cuda()\n",
    "\n",
    "    output = model(word_vectors, string)\n",
    "    loss = loss_function(output, category_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "train(model, s_class, s_word_vs, s_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, print_every=20000, plot_every=1000):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model.train_iterations += 1\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "\n",
    "        output, loss = train(model, s_class, s_word_vs, s_bef, loss_function, optimizer)\n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            guess, guess_i = category_from_output(output)\n",
    "            correct = '✓' if guess == s_class else \"✗ ({})\".format(s_class)\n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} {}\".format(\n",
    "                model.train_iterations, iteration/n_iters, timeSince(start),\n",
    "                current_loss/current_loss_iter, loss,\n",
    "                s_bef, guess, correct ))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model.train_history.append((current_loss / plot_every, lr))\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model.train_iterations % 25000 == 0:\n",
    "            data_balance_randomize_classes()\n",
    "    \n",
    "    test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN_WORDS_CHARS_CLASS(wordvect_size=s_word_vs.shape[-1], letters_size=letters_n,\n",
    "                              hidden_size=128, output_size=len(categories_all)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=10000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(len(categories_all), len(categories_all))\n",
    "n_confusion = 100000\n",
    "data_balance_randomize_classes(50000)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "    word_vectors = Variable(torch.from_numpy(s_word_vs))\n",
    "    string = Variable(string_to_tensor(s_bef))\n",
    "    if use_cuda:\n",
    "        word_vectors = word_vectors.cuda()\n",
    "        string = string.cuda()\n",
    "    output = model(word_vects, string)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = categories_index[s_class]\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(len(categories_all)):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + list(categories_all), rotation=90)\n",
    "ax.set_yticklabels([''] + list(categories_all))\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_2 = confusion.clone().numpy()\n",
    "for i in range(len(confusion_2)):\n",
    "    confusion_2[i,i]=0\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion_2)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + list(categories_all), rotation=90)\n",
    "ax.set_yticklabels([''] + list(categories_all))\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_losses = [arr[0] for arr in model.train_history]\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
