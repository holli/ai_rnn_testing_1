{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'numbers_gen_5_2_layer_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"data/en_train_fixed_1.pkl\", \"rb\" ))\n",
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>963880</th>\n",
       "      <td>76496</td>\n",
       "      <td>14</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948046</th>\n",
       "      <td>151964</td>\n",
       "      <td>4</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id        class before after class_org\n",
       "963880         76496        14  NOT_CHANGED      .     .     PUNCT\n",
       "1948046       151964         4  NOT_CHANGED      )     )     PUNCT"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT_CHANGED' 'NUMBERS' 'LETTERS' 'PLAIN' 'VERBATIM' 'ELECTRONIC']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', '.', ',', 'the', '\"', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 448172,  (dropped rows: 9470020)\n"
     ]
    }
   ],
   "source": [
    "number_data = all_data[all_data['class'] == 'NUMBERS']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(number_data), len(all_data)-len(number_data)))\n",
    "number_data = number_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data = number_data\n",
    "\n",
    "balanced_data_length = len(balanced_data)\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id      639889\n",
       "token_id             22\n",
       "class           NUMBERS\n",
       "before               12\n",
       "after            twelve\n",
       "class_org      CARDINAL\n",
       "Name: 383543, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = list(set(list(number_data['after'])))\n",
    "arr = [s.split(' ') for s in arr]\n",
    "arr = np.concatenate(arr)\n",
    "arr = sorted(list(set(arr)))\n",
    "number_words = [EOS_TOKEN, SOS_TOKEN, UNKNOWN_WORD_TOKEN, NUMBER_WORD_TOKEN, SAMPLE_WORD_TOKEN] + arr\n",
    "number_words_index = dict((c, i) for i, c in enumerate(number_words))\n",
    "len(number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 511])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def number_words_to_tensor(words, include_eos=True):\n",
    "    return words_to_tensor(words, words_lookup_index=number_words_index, include_eos=include_eos)\n",
    "number_words_to_tensor(['one', 'first']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words_index['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 511])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words_onehot_sos = number_words_to_tensor([SOS_TOKEN], include_eos=False)\n",
    "#number_words_onehot_sos = Variable(torch.from_numpy(number_words_onehot_sos)).cuda()\n",
    "number_words_onehot_sos.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBERS : 1195 -> eleven ninety five\n",
      "In <SAMPLE> Henricus le Scotte witnessed a charter by David , Earl of Strathearn .\n",
      "['In', '<SAMPLE>', 'Henricus', 'le', 'Scotte', 'witnessed', 'a', 'charter', 'by', 'David', ',', 'Earl', 'of', 'Strathearn', '.']\n",
      "torch.Size([1, 16, 8192])\n",
      "torch.Size([1, 5, 104])\n",
      "torch.Size([1, 4, 511])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = list(rows.before)\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = SAMPLE_WORD_TOKEN\n",
    "    \n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], befores\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_class, ':', s_bef, '->', s_aft)\n",
    "    print(' '.join(s_sentence))\n",
    "    print(s_sentence)\n",
    "    print(words_to_tensor(list(s_sentence), common_words_index).shape)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "    print(number_words_to_tensor(s_aft.split(' ')).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(8192, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_rnn = EncoderRNN(words_input_size=len(common_words), chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=128,\n",
    "                         words_layers=2, chars_layers=2).cuda()\n",
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t)\n",
    "    \n",
    "encoder_output = test_encoder_single_sample()\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): GRU(511, 256, num_layers=2, batch_first=True)\n",
       "  (lin_out): Linear (256 -> 511)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                         # LSTM would require own hidden included\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "        res = torch.cat((input_var, hid_var), 0)\n",
    "        return res\n",
    "        \n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(number_words), hidden_size=encoder_output.size()[-1], n_layers=2)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 511])\n",
      "torch.Size([2, 1, 256])\n",
      "Variable containing:\n",
      " 476\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "twos\n"
     ]
    }
   ],
   "source": [
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp_a, tmp_b = decoder_rnn(Variable(number_words_onehot_sos).cuda(), tmp_hiddens)\n",
    "print(tmp_a.size())\n",
    "print(tmp_b.size())\n",
    "print(tmp_a.topk(1)[1])\n",
    "print(number_words[tmp_a.topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 511])\n",
      "Variable containing:\n",
      " 304\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "nineteens\n"
     ]
    }
   ],
   "source": [
    "tmp_a, tmp_b = decoder_rnn(Variable(number_words_onehot_sos).cuda(), tmp_b)\n",
    "print(tmp_a.size())\n",
    "print(tmp_a.topk(1)[1])\n",
    "print(number_words[tmp_a.topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('minutes saint saint saint saint nineteens nineteens nineteens eighties eighties eighties somerville somerville nineteens nineteens eighties eighties somerville nineteens nineteens',\n",
       " 'minutes saint saint saint saint nineteens nineteens nineteens eighties eighties eighties somerville somerville nineteens nineteens eighties eighties somerville nineteens nineteens',\n",
       " 'the twenty sixth of june nineteen eleven',\n",
       " ('26 June 1911',\n",
       "  'the twenty sixth of june nineteen eleven',\n",
       "  'NUMBERS',\n",
       "  ['<SAMPLE>', '.']))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None):\n",
    "    s_bef, s_aft, s_class, s_sentence = sample = get_random_sample()\n",
    "        \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_output)\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    return output, output, s_aft, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', ' '.join(s_sentence), ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XI             => one            || the eleventh \n",
      "                  Gran cronica de Alfonso <SAMPLE> .\n",
      "45192          => four thousand seven hundred twenty two || forty five thousand one hundred ninety two \n",
      "                  Lieutenant Colonel John Cameron , TD ( <SAMPLE> ) , Royal Regiment of Artillery , Territorial Army ( now TARO ) .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/   10000)\n",
      "CPU times: user 11min 4s, sys: 7 s, total: 11min 11s\n",
      "Wall time: 2min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_output)\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    target_arr = s_aft.split(' ') + [EOS_TOKEN]\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "\n",
    "        decoder_target_i = number_words_index[target_arr[i]]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            correct = '✓' if result == s_aft else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/numbers_gen_5_2_layer_output\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   5.141   |   1.55: 350 ->  (✗: three hundred fifty) \n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (  0m 46s)   3.891   |   6.14: $16 million -> <EOS> <EOS> <EOS> (✗: sixteen million dollars) (forcing)\n",
      "    27  54% (  0m 46s)   3.968   |   6.20: 26 January 2013 -> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> (✗: the twenty sixth of january twenty thirteen) (forcing)\n",
      "    36  72% (  0m 46s)   4.314   |   3.09: 3 ->  (✗: three) \n",
      "    45  90% (  0m 46s)   4.176   |   6.11: March 21, 2006 -> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> (✗: march twenty first two thousand six) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (   0m 9s)   2.881   |   3.21: 50 -> nineteen (✗: fifty) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 20s)   2.622   |   3.01: 15 -> two (✗: fifteen) (forcing)\n",
      "  3000  22% (  0m 42s)   2.535   |   2.19: III -> two (✗: three) \n",
      "  4000  33% (   1m 3s)   2.393   |   1.55: 107 -> two hundred (✗: one hundred seven) \n",
      "  5000  44% (  1m 23s)   2.370   |   2.39: 90 -> two (✗: ninety) (forcing)\n",
      "  6000  56% (  1m 45s)   2.287   |   2.15: 266 -> two hundred <EOS> <EOS> (✗: two hundred sixty six) (forcing)\n",
      "  7000  67% (   2m 6s)   2.228   |   1.14: 1988 -> nineteen ninety (✗: nineteen eighty eight) \n",
      "  8000  78% (  2m 27s)   2.209   |   1.70: 2011 -> two thousand (✗: twenty eleven) (forcing)\n",
      "  9000  89% (  2m 48s)   2.093   |   3.16: January 20, 2012 -> the twenty of of twenty (✗: january twentieth twenty twelve) \n",
      " 10000 100% (  3m 10s)   2.094   |   1.19: 1 -> one (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (  3m 36s)   1.217   |   1.39: 1891 -> nineteen forty one (✗: eighteen ninety one) (forcing)\n",
      " 30000  22% (  7m 25s)   0.768   |   0.00: 5 -> five (✓) (forcing)\n",
      " 40000  33% ( 11m 19s)   0.501   |   4.13: I -> one <EOS> (✗: the first) (forcing)\n",
      " 50000  44% ( 15m 16s)   0.366   |   0.00: 2007 -> two thousand seven (✓) \n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 73.87% (    7387/   10000)\n",
      " 60000  56% ( 20m 21s)   0.320   |   0.03: 2015 -> twenty fifteen (✓) (forcing)\n",
      " 70000  67% ( 24m 47s)   0.272   |   0.00: December 2009 -> december two thousand nine (✓) \n",
      " 80000  78% ( 28m 32s)   0.219   |   0.00: 4 -> four (✓) (forcing)\n",
      " 90000  89% ( 32m 45s)   0.242   |   4.11: 1 kg -> eleven p (✗: one kilogram) (forcing)\n",
      "100000 100% ( 36m 51s)   0.177   |   0.02: 3.1 -> three point one (✓) \n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 88.56% (    8856/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000   3% (   3m 3s)   0.229   |   0.00: 3 -> three (✓) \n",
      "120000   7% (  5m 55s)   0.233   |   0.00: 133 -> one hundred thirty three (✓) (forcing)\n",
      "130000  10% (   9m 2s)   0.183   |   0.00: July 10, 2012 -> july tenth twenty twelve (✓) \n",
      "140000  13% ( 11m 49s)   0.142   |   0.00: 1894 -> eighteen ninety four (✓) \n",
      "150000  17% ( 14m 40s)   0.182   |   0.00: 1st -> first (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 90.55% (    9055/   10000)\n",
      "160000  20% ( 18m 29s)   0.148   |   0.00: 2 -> two (✓) (forcing)\n",
      "170000  23% ( 21m 21s)   0.169   |   0.00: 2004 -> two thousand four (✓) (forcing)\n",
      "180000  27% ( 24m 16s)   0.152   |   0.00: September 26, 2006 -> september twenty sixth two thousand six (✓) \n",
      "190000  30% ( 27m 15s)   0.174   |   0.01: 298 -> two hundred ninety eight (✓) \n",
      "200000  33% (  30m 7s)   0.139   |   0.00: 12th -> twelfth (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 91.71% (    9171/   10000)\n",
      "210000  37% ( 34m 27s)   0.130   |   0.00: 1853 -> eighteen fifty three (✓) (forcing)\n",
      "220000  40% ( 38m 10s)   0.148   |   0.00: 2005 -> two thousand five (✓) (forcing)\n",
      "230000  43% ( 41m 55s)   0.158   |   0.00: November 1997 -> november nineteen ninety seven (✓) \n",
      "240000  47% (  47m 5s)   0.143   |   0.03: 2007 -> two thousand seven (✓) \n",
      "250000  50% ( 51m 54s)   0.124   |   0.00: 2006 -> two thousand six (✓) \n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 91.95% (    9195/   10000)\n",
      "260000  53% ( 60m 15s)   0.142   |   0.00: Aug 1, 1966 -> august first nineteen sixty six (✓) \n",
      "270000  57% ( 65m 45s)   0.147   |   0.00: 6 -> six (✓) \n",
      "280000  60% ( 71m 59s)   0.138   |   0.00: 1984 -> nineteen eighty four (✓) (forcing)\n",
      "290000  63% ( 77m 58s)   0.131   |   0.00: 1988 -> nineteen eighty eight (✓) \n",
      "300000  67% (  84m 2s)   0.167   |   0.00: 1989 -> nineteen eighty nine (✓) \n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.05% (    9205/   10000)\n",
      "310000  70% ( 91m 46s)   0.131   |   0.65: 2012 -> two thousand one two (✗: two o one two) (forcing)\n",
      "320000  73% ( 97m 28s)   0.098   |   0.00: 2004 -> two thousand four (✓) \n",
      "330000  77% (102m 57s)   0.122   |   0.04: 2010-04-15 -> the fifteenth of april twenty ten (✓) (forcing)\n",
      "340000  80% (108m 17s)   0.188   |   0.44: 1965 -> nineteen nine nine hundred sixty five (✗: one thousand nine hundred sixty five) (forcing)\n",
      "350000  83% (113m 40s)   0.124   |   0.00: 1649 -> sixteen forty nine (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.49% (    9349/   10000)\n",
      "360000  87% (121m 13s)   0.094   |   0.00: 223 -> two hundred twenty three (✓) (forcing)\n",
      "370000  90% (126m 44s)   0.128   |   0.00: 1993 -> nineteen ninety three (✓) (forcing)\n",
      "380000  93% (132m 10s)   0.115   |   0.00: March 2011 -> march twenty eleven (✓) \n",
      "390000  97% (137m 31s)   0.139   |   0.03: 1911 -> nineteen eleven (✓) \n",
      "400000 100% ( 143m 6s)   0.104   |   0.00: 8 -> eight (✓) \n",
      "Saved model to data/models/numbers_gen_5_2_layer_output/400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.80% (    9280/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8543          => minus eight thousand four hundred forty three || minus eight thousand five hundred forty three \n",
      "                  The result was the identification of BAY 41 -2272 and BAY 41 <SAMPLE> .\n",
      "2010-07-08     => the eighth of april twenty eighth || the eighth of july twenty ten \n",
      "                  Sperling , Nicole ( <SAMPLE> ) .\n",
      "10930          => ten thousand nine hundred three || ten thousand nine hundred thirty \n",
      "                  Asteroid <SAMPLE> Jinyong ( 1998 CR2 ) is named after him .\n",
      "169-70 ISBN 1611493528 => one two sil six two sil six three sil three sil three || one six nine sil seven o sil i s b n sil one six one one four nine three five two eight \n",
      "                  <SAMPLE> Roger Moorhouse , The Devils' Alliance : Hitler's Pact with Stalin , 1939 - 1941 .\n",
      "60             => sixty          || six o \n",
      "                  Duncan Island ( 36 LA <SAMPLE> , 61 ) is a prehistoric archaeological site located in the Susquehanna River at Martic Township in Lancaster County , Pennsylvania .\n",
      "9781420941142  => nine trillion seven four one four four sil one two one one sil one || nine trillion seven hundred eighty one billion four hundred twenty million nine hundred forty one thousand one hundred forty two \n",
      "                  Prescott , W.H. , 2011 , The History of the Conquest of Peru , Digireads.com Publishing , ISBN <SAMPLE> \" Klimatafel von Tumbes , Prov .\n",
      "1986           => nineteen eighty six || one nine eight six \n",
      "                  Only three PRE <SAMPLE> triple doubles are included below .\n",
      "100yr          => one hundred megawatts || one hundred years \n",
      "                  \" Wilson County Centennial 1860 - 1960 \" By the Wilson county library , Centennial program handed out at The <SAMPLE> centennial celebration .\n",
      "385 million    => three hundred eighty five million dollars || three hundred eighty five million \n",
      "                  Sam F. \" Ferrovial enhances its services business with the acquisition of Enterprise for <SAMPLE> pounds ( 443 million euro ) \" .\n",
      "273            => two hundred seventy three || two seven three \n",
      "                  Gregory labelled it by 217 a and <SAMPLE> p ; Scrivener labelled it by 185 a and 255 p .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
