{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import bcolz\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "print(\"Pytorch: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_org = pd.read_csv('data/en_train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616107</th>\n",
       "      <td>49226</td>\n",
       "      <td>17</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684691</th>\n",
       "      <td>54634</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965529</th>\n",
       "      <td>76612</td>\n",
       "      <td>7</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id    class before after\n",
       "616107        49226        17  LETTERS    NaN   n a\n",
       "684691        54634         1    PLAIN    NaN   NaN\n",
       "965529        76612         7    PLAIN    NaN   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_org[pd.isnull(all_data_org['before'])][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 9918390,  (dropped none rows: 51)\n",
      "Data rows: 9840282,  (dropped rows: 78159)\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data_org.dropna()\n",
    "print(\"Data rows: {},  (dropped none rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data[all_data['class'] != 'VERBATIM']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we dropped VERBATIM class. Thats because it had so many weird characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS           522\n",
       "CARDINAL       133744\n",
       "DATE           258348\n",
       "DECIMAL          9821\n",
       "DIGIT            5442\n",
       "ELECTRONIC       5162\n",
       "FRACTION         1196\n",
       "LETTERS        152790\n",
       "MEASURE         14783\n",
       "MONEY            6128\n",
       "ORDINAL         12703\n",
       "PLAIN         7353647\n",
       "PUNCT         1880507\n",
       "TELEPHONE        4024\n",
       "TIME             1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_classes = list(all_data.groupby('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_balance_randomize_classes(max_len=10000):\n",
    "    global data_balanced_classes\n",
    "    data_balanced_classes = pd.concat([v.sample(min(max_len, len(v))) for k, v in all_data_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "data_balance_randomize_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS         522\n",
       "CARDINAL      10000\n",
       "DATE          10000\n",
       "DECIMAL        9821\n",
       "DIGIT          5442\n",
       "ELECTRONIC     5162\n",
       "FRACTION       1196\n",
       "LETTERS       10000\n",
       "MEASURE       10000\n",
       "MONEY          6128\n",
       "ORDINAL       10000\n",
       "PLAIN         10000\n",
       "PUNCT         10000\n",
       "TELEPHONE      4024\n",
       "TIME           1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4015740</th>\n",
       "      <td>310950</td>\n",
       "      <td>3</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501902</th>\n",
       "      <td>573374</td>\n",
       "      <td>9</td>\n",
       "      <td>DIGIT</td>\n",
       "      <td>24706</td>\n",
       "      <td>two four seven o six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7295163</th>\n",
       "      <td>557828</td>\n",
       "      <td>16</td>\n",
       "      <td>DECIMAL</td>\n",
       "      <td>2.5</td>\n",
       "      <td>two point five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579598</th>\n",
       "      <td>429216</td>\n",
       "      <td>10</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>2013</td>\n",
       "      <td>two thousand thirteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6556192</th>\n",
       "      <td>502427</td>\n",
       "      <td>2</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Hush</td>\n",
       "      <td>Hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81065</th>\n",
       "      <td>6558</td>\n",
       "      <td>4</td>\n",
       "      <td>DECIMAL</td>\n",
       "      <td>38 million</td>\n",
       "      <td>thirty eight million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402486</th>\n",
       "      <td>640852</td>\n",
       "      <td>7</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Sylvania</td>\n",
       "      <td>Sylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973606</th>\n",
       "      <td>231678</td>\n",
       "      <td>4</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1943</td>\n",
       "      <td>nineteen forty three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672866</th>\n",
       "      <td>54165</td>\n",
       "      <td>10</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302562</th>\n",
       "      <td>332666</td>\n",
       "      <td>5</td>\n",
       "      <td>TIME</td>\n",
       "      <td>7:18 p.m.</td>\n",
       "      <td>seven eighteen p m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id     class      before                  after\n",
       "4015740       310950         3     PUNCT           .                      .\n",
       "7501902       573374         9     DIGIT       24706   two four seven o six\n",
       "7295163       557828        16   DECIMAL         2.5         two point five\n",
       "5579598       429216        10  CARDINAL        2013  two thousand thirteen\n",
       "6556192       502427         2     PLAIN        Hush                   Hush\n",
       "81065           6558         4   DECIMAL  38 million   thirty eight million\n",
       "8402486       640852         7     PLAIN    Sylvania               Sylvania\n",
       "2973606       231678         4      DATE        1943   nineteen forty three\n",
       "672866         54165        10  CARDINAL           1                    one\n",
       "4302562       332666         5      TIME   7:18 p.m.     seven eighteen p m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(name):\n",
    "    with open(name, 'r') as f: lines = [line.split() for line in f]\n",
    "    words = [d[0] for d in lines]\n",
    "    vecs = np.stack(np.array(d[1:], dtype=np.float32) for d in lines)\n",
    "    wordidx = {o:i for i,o in enumerate(words)}\n",
    "    return vecs, words, wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', \"'s\", 'asdf', '-', 'testaaa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_tokeniser(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' - ')\n",
    "    sent = re_punc.sub(r\" \\1 \", sent)\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()\n",
    "simple_tokeniser(\"asdf's   asdf   -testaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arr = [simple_tokeniser(s_)[0] for s_ in list(all_data.sample(1000)['before'])]\n",
    "[s in wv_idx for s in arr].count(True) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = data_balanced_classes.iloc[random.randint(1, len(data_balanced_classes)-1)]\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    #rows = all_data[all_data['sentence_id']==sample_row['sentence_id']]\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = rows.before.values\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = '*****'\n",
    "    str_list = simple_tokeniser(' '.join(befores))\n",
    "    \n",
    "    word_vect = np.zeros((1, len(str_list), wv_vecs.shape[1]), dtype=np.float32)\n",
    "    # var = np.zeros((1, len(str_list), wv_vecs.shape[1]+1))\n",
    "    for i, w in enumerate(str_list):\n",
    "        if w=='*****':\n",
    "            word_vect[0][i] = np.zeros((1, wv_vecs.shape[1]))\n",
    "        else:\n",
    "            try:\n",
    "                word_vect[0][i] = wv_vecs[wv_idx[w]]\n",
    "            except KeyError:\n",
    "                word_vect[0][i] = np.random.rand(1, wv_vecs.shape[1])\n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], word_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "s_bef, s_aft, s_class, s_word_v = get_random_sample()\n",
    "print(s_class, ':', s_bef, '->', s_aft, '('+str(len(s_aft))+')', ':', s_word_v.shape, type(s_word_v[0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492 µs ± 3.73 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories and Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAIN' 'PUNCT' 'DATE' 'LETTERS' 'CARDINAL' 'DECIMAL' 'MEASURE' 'MONEY'\n",
      " 'ORDINAL' 'TIME' 'ELECTRONIC' 'DIGIT' 'FRACTION' 'TELEPHONE' 'ADDRESS']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "letters_before_all = sorted(list(set(''.join(all_data['before']))))\n",
    "print(len(letters_before_all))\n",
    "letters_after_all = sorted(list(set(''.join(all_data['after']))))\n",
    "print(len(letters_after_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EOS><SOS> !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~¡£¥ª«²³µº»¼½¾¿éɒʻˈΩμ—€⅓⅔⅛⅝⅞\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "letters_all = ['<EOS>', '<SOS>'] + sorted(list(set(letters_before_all + letters_after_all)))\n",
    "letters_all_index = dict((c, i) for i, c in enumerate(letters_all))\n",
    "print(''.join(letters_all))\n",
    "print(len(letters_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot_char(char):\n",
    "    tensor = torch.zeros(1, len(letters_all))\n",
    "    tensor[0, letters_all_index[char]] = 1\n",
    "    return tensor\n",
    "onehot_char('<EOS>')[0, 0] == 1\n",
    "\n",
    "sos_variable = Variable(onehot_char('<SOS>')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 114])\n",
      "torch.Size([1, 7, 114])\n"
     ]
    }
   ],
   "source": [
    "def string_to_tensor(line, include_eos=False):\n",
    "    tensor_length = len(line)+1 if include_eos else len(line)\n",
    "    tensor = torch.zeros(1, tensor_length, len(letters_all))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[0, li, letters_all_index[letter]] = 1\n",
    "    if include_eos:\n",
    "        tensor[0, -1, letters_all_index['<EOS>']] = 1\n",
    "    return tensor\n",
    "print(string_to_tensor('wordup').size())\n",
    "print(string_to_tensor('wordup', include_eos=True).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, wordvect_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        #self.train_iterations = 0\n",
    "        #self.train_history = []\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "        # self.output_size = output_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(wordvect_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # self.lin_output = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        # output = self.lin_output(output)\n",
    "        # output = F.log_softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        if use_cuda:\n",
    "            var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "            var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vect size: (1, 29, 50) . String vector size: torch.Size([1, 4, 114])\n",
      "Output: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder():\n",
    "    s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "    s_string = string_to_tensor(s_bef)\n",
    "    encoder_rnn = EncoderRNN(wordvect_size=s_word_vs.shape[-1], chars_input_size=len(letters_all),\n",
    "                                      words_hidden_size=64, chars_hidden_size=128,\n",
    "                                      words_layers=1, chars_layers=1).cuda()\n",
    "    print('Word vect size:', s_word_vs.shape, '. String vector size:', s_string.size())\n",
    "    output_encoded = encoder_rnn(Variable(torch.from_numpy(s_word_vs)).cuda(), Variable(s_string).cuda())\n",
    "    print('Output:', output_encoded.size())\n",
    "    return encoder_rnn, output_encoded;\n",
    "encoder_rnn, output_encoded = test_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, chars_input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(chars_input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                         # LSTM would require own hidden included\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, output_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -4.6277\n",
       " [torch.cuda.FloatTensor of size 1x1 (GPU 0)], Variable containing:\n",
       "  98\n",
       " [torch.cuda.LongTensor of size 1x1 (GPU 0)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_rnn = DecoderRNN(chars_input_size=len(letters_all), hidden_size=output_encoded.size()[-1],\n",
    "                         output_size=len(letters_all)).cuda()\n",
    "tmp_a, tmp_b = decoder_rnn(sos_variable.view(1,1,-1), output_encoded.view(1,1,-1))\n",
    "tmp_a.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 114]), torch.Size([1, 1, 192])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = Variable(string_to_tensor(letters_all[tmp_a.topk(1)[1].data[0][0]])).cuda()\n",
    "tmp = decoder_rnn(decoder_input, tmp_b)\n",
    "[t.size() for t in tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_word_vs, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    s_bef_string = string_to_tensor(s_bef, include_eos=False)\n",
    "    target_arr = list(s_aft) + ['<EOS>']\n",
    "\n",
    "    encoder_output = encoder(Variable(torch.from_numpy(s_word_vs)).cuda(), Variable(s_bef_string).cuda())\n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    decoder_hidden = encoder_output\n",
    "\n",
    "    decoder_input = sos_variable.view(1,1,-1).cuda()\n",
    "\n",
    "    decoded_chars_arr = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "        decoder_target = Variable(torch.LongTensor([letters_all_index[target_arr[i]]])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target)\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = letters_all[char_index] # Use own prediction as next char\n",
    "        decoded_chars_arr.append(char)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            char = target_arr[i] # replace input with right target char\n",
    "        else:\n",
    "            # use output char normally as input char\n",
    "            if char == '<EOS>':\n",
    "                print(\"BREAKINg eos\")\n",
    "                break\n",
    "\n",
    "        decoder_input = Variable(string_to_tensor(char)).cuda()\n",
    "        \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return((loss.data[0] / len(target_arr)), ''.join(decoded_chars_arr))\n",
    "#print(train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_train_iterations = 0\n",
    "model_train_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314 ms ± 2.58 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "data_balance_randomize_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=5000, plot_every=1000):\n",
    "    global model_train_iterations\n",
    "    global model_train_history\n",
    "    start = time.time()\n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_train_iterations += 1\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "        #print(s_class, ':', s_bef, '->', s_aft, '('+str(len(s_aft))+')')\n",
    "        loss, result = train(s_bef=s_bef, s_aft=s_aft, s_word_vs=s_word_vs,\n",
    "                             encoder=encoder_rnn, decoder=decoder_rnn,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "\n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} {} ({})\".format(\n",
    "                      model_train_iterations, iteration/n_iters, timeSince(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, teacher_forcing_str, result, s_aft))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_train_history.append((current_loss / plot_every, lr))\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_train_iterations % 25000 == 0:\n",
    "            data_balance_randomize_classes()\n",
    "            \n",
    "        if model_train_iterations % 25000 == 0:\n",
    "            save_model()\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     6  50% (   0m 0s)   4.700   |   4.67: 3 -> (forcing) .gngn (three)\n",
      "    11 100% (   0m 0s)   4.680   |   4.64: XII -> (forcing)   eeee (twelve)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=10, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3500  25% (  0m 14s)   2.149   |   3.13: 2/3 ->  three nine (two thirds)\n",
      "  4000  50% (  0m 29s)   2.160   |   2.52: 57 ->  o e t e t e (fifty seven)\n",
      "  4500  75% (  0m 42s)   2.168   |   2.95: 3 ->  six t (three)\n",
      "  5000 100% (  0m 57s)   2.151   |   2.78: 16118 ->  theeteen nineteen nin (one six one one eight)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=2000, teacher_forcing_ratio=0.5, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10000  20% (  2m 31s)   2.033   |   0.01: . ->  . (.)\n",
      " 15000  40% (   5m 1s)   2.028   |   1.50: 8/21 -> (forcing) eight eho ty fivt   (eight twenty firsts)\n",
      " 20000  60% (  7m 26s)   1.976   |   2.56: HDLs ->  a a a d (h d l's)\n",
      " 25000  80% (  9m 50s)   1.920   |   0.88: $47,500 -> (forcing) fouty fiven  h usand tive eundred tollars (forty seven thousand five hundred dollars)\n",
      "Saving: data/models/gen_1_rnn_25000\n",
      " 30000 100% ( 12m 13s)   1.796   |   0.92: 0.92% -> (forcing) foro point oineteho torcent (zero point nine two percent)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(30000-model_train_iterations), teacher_forcing_ratio=0.2, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35000  25% (  2m 29s)   1.547   |   7.01: - ->  — (-)\n",
      " 40000  50% (  4m 57s)   1.432   |   2.09: of ->  fo (of)\n",
      " 45000  75% (  7m 22s)   1.236   |   2.56: $325,000 ->  three hundred fifty thousand six hundred s (three hundred twenty five thousand dollars)\n",
      " 50000 100% (  9m 47s)   1.098   |   0.48: 1965 -> (forcing) nineteen fixty sive (nineteen sixty five)\n",
      "Saving: data/models/gen_1_rnn_50000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55000  10% (  2m 26s)   1.198   |   0.03: 2 -> (forcing) two (two)\n",
      " 60000  20% (  4m 53s)   1.159   |   1.01: 100.00 ->  one hundred twenty tw (one hundred point o o)\n",
      " 65000  30% (  7m 17s)   1.054   |   1.71: Idir ->  ind six (i d i r)\n",
      " 70000  40% (  9m 47s)   1.026   |   1.20: F. A. ->  f o (f a)\n",
      " 75000  50% ( 12m 18s)   0.954   |   0.46: 533.7 -> (forcing) five hundred thirty ehree point tiven (five hundred thirty three point seven)\n",
      "Saving: data/models/gen_1_rnn_75000\n",
      " 80000  60% ( 14m 44s)   0.894   |   0.55: 194 km2 -> (forcing) nie hundred sinety fiur pquare kilometers (one hundred ninety four square kilometers)\n",
      " 85000  70% ( 17m 10s)   0.862   |   0.41: 539 ->  five hundred thirty nine (five hundred thirty nine)\n",
      " 90000  80% ( 19m 35s)   0.818   |   2.77: Kevin ->  k e v (Kevin)\n",
      " 95000  90% (  22m 1s)   0.824   |   0.26: 50.1% -> (forcing) fifty point one percent (fifty point one percent)\n",
      "100000 100% ( 24m 32s)   0.813   |   0.00: \" ->  \" (\")\n",
      "Saving: data/models/gen_1_rnn_100000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105000  10% (  2m 25s)   0.831   |   0.00: . -> (forcing) . (.)\n",
      "110000  20% (  4m 52s)   0.770   |   0.07: 30.5 -> (forcing) thirty point five (thirty point five)\n",
      "115000  30% (  7m 15s)   0.794   |   1.47: February 1921 ->  february twenteenteennenenen (february nineteen twenty one)\n",
      "120000  40% (  9m 41s)   0.740   |   2.11: Six ->  S i (Six)\n",
      "125000  50% (  12m 7s)   0.759   |   0.05: I ->  the first (the first)\n",
      "Saving: data/models/gen_1_rnn_125000\n",
      "130000  60% ( 14m 37s)   0.731   |   0.00: — ->  — (—)\n",
      "135000  70% (  17m 3s)   0.756   |   0.50: 35.06 -> (forcing) thirty five point sneie (thirty five point o six)\n",
      "140000  80% ( 19m 23s)   0.725   |   0.16: 364 ->  three hundred sixty four (three hundred sixty four)\n",
      "145000  90% ( 21m 52s)   0.759   |   2.56: 85931 ->  eight five  ine e eeee eeeeeeeeeeeeeeeeeeeee (eighty five thousand nine hundred thirty one)\n",
      "150000 100% ( 24m 21s)   0.722   |   0.04: ISBN -> (forcing) i s b n (i s b n)\n",
      "Saving: data/models/gen_1_rnn_150000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155000  10% (  2m 22s)   0.599   |   0.35: £800,000 -> (forcing) eight hundred mhousand dounds (eight hundred thousand pounds)\n",
      "160000  20% (  4m 45s)   0.626   |   0.00: ) ->  ) ())\n",
      "165000  30% (  7m 11s)   0.563   |   1.55: 1000 GHz -> (forcing) one hhousand thrhs   h (one thousand gigahertz)\n",
      "170000  40% (  9m 35s)   0.560   |   1.76: VIII's ->  the sixthhht (the eighth's)\n",
      "175000  50% ( 11m 56s)   0.513   |   0.00: ) ->  ) ())\n",
      "Saving: data/models/gen_1_rnn_175000\n",
      "180000  60% ( 14m 21s)   0.519   |   0.42: 14 August -> (forcing) the fourthenth of jugust (the fourteenth of august)\n",
      "185000  70% ( 16m 47s)   0.560   |   0.01: to -> (forcing) to (to)\n",
      "190000  80% ( 19m 12s)   0.535   |   1.39: 8.00 PM ->  eight poi (eight p m)\n",
      "195000  90% ( 21m 38s)   0.531   |   1.24: .2005 ->  point two thousand (point two o o five)\n",
      "200000 100% (  24m 5s)   0.548   |   0.00: 5 -> (forcing) five (five)\n",
      "Saving: data/models/gen_1_rnn_200000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205000  10% (  2m 24s)   0.537   |   1.06: Government ->  Govermener (Government)\n",
      "210000  20% (  4m 47s)   0.545   |   0.87: QELP ->  p e e l (q e l p)\n",
      "215000  30% (  7m 11s)   0.532   |   0.37: 2010 ->  twenty ten (twenty ten)\n",
      "220000  40% (  9m 35s)   0.482   |   0.00: , ->  , (,)\n",
      "225000  50% ( 11m 58s)   0.525   |   0.50: 1,500th ->  one thousand five hundred f (one thousand five hundredth)\n",
      "Saving: data/models/gen_1_rnn_225000\n",
      "230000  60% ( 14m 25s)   0.503   |   1.14: Dii ->  dii i (d i i)\n",
      "235000  70% ( 16m 48s)   0.445   |   0.16: 10,500 ft -> (forcing) ten thousand five hundred feet (ten thousand five hundred feet)\n",
      "240000  80% ( 19m 11s)   0.474   |   0.42: same ->  same (same)\n",
      "245000  90% ( 21m 40s)   0.543   |   0.04: $18,000 ->  eighteen thousand dollars (eighteen thousand dollars)\n",
      "250000 100% (  24m 4s)   0.483   |   0.04: $60,000 ->  sixty thousand dollars (sixty thousand dollars)\n",
      "Saving: data/models/gen_1_rnn_250000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.2, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255000  10% (  2m 25s)   0.587   |   0.20: 68.5 percent -> (forcing) sixty eight point five percent (sixty eight point five percent)\n",
      "260000  20% (  4m 48s)   0.532   |   0.09: £2m ->  two million pounds (two million pounds)\n",
      "265000  30% (  7m 18s)   0.584   |   0.00: ) -> (forcing) ) ())\n",
      "270000  40% (  9m 47s)   0.554   |   0.20: 4-4-0 -> (forcing) four sil four sil o (four sil four sil o)\n",
      "275000  50% ( 12m 13s)   0.554   |   0.00: . -> (forcing) . (.)\n",
      "Saving: data/models/gen_1_rnn_275000\n",
      "280000  60% ( 14m 35s)   0.585   |   1.52: 0-521-29626-9 ->  o sil five two one two one sil o e                   (o sil five two one sil two nine six two six sil nine)\n",
      "285000  70% (  17m 5s)   0.563   |   1.61: BJTs ->  b b s s (b j t's)\n",
      "290000  80% ( 19m 30s)   0.525   |   1.27: $73,750 ->  seventy three thousand five hundred fifty  illasd  (seventy three thousand seven hundred fifty dollars)\n",
      "295000  90% ( 21m 59s)   0.534   |   2.68: DM 23 billion ->  thent  nint   ninte    nne e  e   (twenty three billion german marks)\n",
      "300000 100% ( 24m 24s)   0.504   |   0.06: $200,000 -> (forcing) two hundred thousand dollars (two hundred thousand dollars)\n",
      "Saving: data/models/gen_1_rnn_300000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.2, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305000  10% (  2m 27s)   0.400   |   0.93: 2027 -> (forcing) two ty teelveeseven (twenty twenty seven)\n",
      "310000  20% (  4m 49s)   0.414   |   0.69: 29 January 1622 -> (forcing) the twenty sinth of junu ry tixtyen thenty fhe (the twenty ninth of january sixteen twenty two)\n",
      "315000  30% (  7m 10s)   0.375   |   0.66: HQE ->  h e e (h q e)\n",
      "320000  40% (  9m 36s)   0.397   |   0.33: year -> (forcing) year (year)\n",
      "325000  50% ( 11m 56s)   0.393   |   0.00: 3rd -> (forcing) third (third)\n",
      "Saving: data/models/gen_1_rnn_325000\n",
      "330000  60% ( 14m 24s)   0.344   |   0.04: S. J. -> (forcing) s j (s j)\n",
      "335000  70% ( 16m 49s)   0.370   |   0.02: KM ->  k m (k m)\n",
      "340000  80% ( 19m 14s)   0.350   |   0.01: 2.23 ->  two point two three (two point two three)\n",
      "345000  90% ( 21m 43s)   0.358   |   0.04: are -> (forcing) are (are)\n",
      "350000 100% (  24m 5s)   0.379   |   1.72: $12,058 ->  twelve thousand five hundred dollar (twelve thousand fifty eight dollars)\n",
      "Saving: data/models/gen_1_rnn_350000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.4, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355000  10% (  2m 26s)   0.504   |   0.01: 40.3 ->  forty point three (forty point three)\n",
      "360000  20% (  4m 54s)   0.535   |   0.00: . ->  . (.)\n",
      "365000  30% (  7m 22s)   0.552   |   0.00: . ->  . (.)\n",
      "370000  40% (  9m 45s)   0.532   |   1.35: $42,344 ->  forty two thousand five hundred threty three dollar (forty two thousand three hundred forty four dollars)\n",
      "375000  50% ( 12m 16s)   0.503   |   0.00: :: ->  :: (::)\n",
      "Saving: data/models/gen_1_rnn_375000\n",
      "380000  60% ( 14m 46s)   0.532   |   0.61: 2,591 m ->  two thousand five hundred ninety six   eeee (two thousand five hundred ninety one meters)\n",
      "385000  70% ( 17m 14s)   0.496   |   0.82: 95 ->  nine fifeee (ninety five)\n",
      "390000  80% ( 19m 43s)   0.545   |   1.61: 0-15-100389-0 ->  o sil one five oie                                (o sil one five sil one o o three eight nine sil o)\n",
      "395000  90% ( 22m 10s)   0.543   |   0.00: 53% ->  fifty three percent (fifty three percent)\n",
      "400000 100% ( 24m 37s)   0.545   |   0.55: listing -> (forcing) listing (listing)\n",
      "Saving: data/models/gen_1_rnn_400000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405000  10% (  2m 26s)   0.427   |   0.00: ( ->  ( (()\n",
      "410000  20% (   5m 0s)   0.351   |   0.64: 0-374-52700-8 -> (forcing) o sil three seven four oil  ive  wo siven sisnsil oight (o sil three seven four sil five two seven o o sil eight)\n",
      "415000  30% (  7m 26s)   0.385   |   0.03: 200 m -> (forcing) two hundred meters (two hundred meters)\n",
      "420000  40% (  9m 48s)   0.348   |   0.02: 1960 -> (forcing) nineteen sixty (nineteen sixty)\n",
      "425000  50% ( 12m 15s)   0.417   |   1.44: officially ->  offocailll (officially)\n",
      "Saving: data/models/gen_1_rnn_425000\n",
      "430000  60% ( 14m 43s)   0.382   |   0.02: 87.0 ->  eighty seven point zero (eighty seven point zero)\n",
      "435000  70% (  17m 9s)   0.362   |   0.00: 5th ->  fifth (fifth)\n",
      "440000  80% ( 19m 32s)   0.365   |   0.00: on -> (forcing) on (on)\n",
      "445000  90% (  22m 0s)   0.357   |   0.02: 1980 ->  nineteen eighty (nineteen eighty)\n",
      "450000 100% ( 24m 29s)   0.343   |   0.79: 0.26 kg ->  zero point two six kilometer (zero point two six kilograms)\n",
      "Saving: data/models/gen_1_rnn_450000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.3, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455000  10% (  2m 26s)   0.394   |   0.17: $7,266 ->  seven thousand two hundred sixty six dollars (seven thousand two hundred sixty six dollars)\n",
      "460000  20% (  4m 53s)   0.439   |   0.78: 0 ->  z (o)\n",
      "465000  30% (  7m 19s)   0.396   |   0.00: \" ->  \" (\")\n",
      "470000  40% (  9m 47s)   0.324   |   1.51: Snorri ->  Sonrrr (Snorri)\n",
      "475000  50% ( 12m 13s)   0.427   |   0.07: Biolib.cz ->  b i o l i b dot c z (b i o l i b dot c z)\n",
      "Saving: data/models/gen_1_rnn_475000\n",
      "480000  60% ( 14m 37s)   0.360   |   0.01: 1.34 ->  one point three four (one point three four)\n",
      "485000  70% (  17m 4s)   0.405   |   0.85: Cyclingnews.com ->  c y c n i n c i n i d dot c o m (c y c l i n g n e w s dot c o m)\n",
      "490000  80% ( 19m 31s)   0.417   |   0.05: October 2009 ->  october two thousand nine (october two thousand nine)\n",
      "495000  90% ( 21m 57s)   0.341   |   0.11: 159 km ->  one hundred fifty nine kilometers (one hundred fifty nine kilometers)\n",
      "500000 100% ( 24m 29s)   0.381   |   0.06: $6,900 ->  six thousand nine hundred dollars (six thousand nine hundred dollars)\n",
      "Saving: data/models/gen_1_rnn_500000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505000  10% (  2m 32s)   0.377   |   0.02: £2,000 ->  two thousand pounds (two thousand pounds)\n",
      "510000  20% (  4m 53s)   0.359   |   0.00: 9 ->  nine (nine)\n",
      "515000  30% (  7m 15s)   0.354   |   0.71: gorge ->  gogge (gorge)\n",
      "520000  40% (  9m 42s)   0.359   |   0.01: $200 million ->  two hundred million dollars (two hundred million dollars)\n",
      "525000  50% (  12m 3s)   0.377   |   0.00: 95% ->  ninety five percent (ninety five percent)\n",
      "Saving: data/models/gen_1_rnn_525000\n",
      "530000  60% ( 14m 27s)   0.397   |   0.00: ! ->  ! (!)\n",
      "535000  70% (  17m 0s)   0.357   |   0.02: 1972 ->  nineteen seventy two (nineteen seventy two)\n",
      "540000  80% ( 19m 29s)   0.365   |   0.07: 2014 ->  twenty fourteen (twenty fourteen)\n",
      "545000  90% ( 21m 55s)   0.350   |   0.00: 19th ->  nineteenth (nineteenth)\n",
      "550000 100% ( 24m 23s)   0.395   |   0.01: 36.3 ->  thirty six point three (thirty six point three)\n",
      "Saving: data/models/gen_1_rnn_550000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555000  10% (  2m 22s)   0.345   |   0.55: $19,087 ->  nineteen thousand eighty sived llaasss (nineteen thousand eighty seven dollars)\n",
      "560000  20% (  4m 48s)   0.355   |   0.00: . ->  . (.)\n",
      "565000  30% (  7m 21s)   0.410   |   0.00: : ->  : (:)\n",
      "570000  40% (  9m 48s)   0.382   |   0.10: 1993 ->  nineteen ninety three (nineteen ninety three)\n",
      "575000  50% ( 12m 36s)   0.329   |   0.01: 9pm ->  nine p m (nine p m)\n",
      "Saving: data/models/gen_1_rnn_575000\n",
      "580000  60% ( 51m 43s)   0.356   |   0.01: 126 ->  one hundred twenty six (one hundred twenty six)\n",
      "585000  70% (  54m 7s)   0.363   |   0.13: FT.com ->  f t dot c o m (f t dot c o m)\n",
      "590000  80% ( 56m 30s)   0.360   |   0.00: . ->  . (.)\n",
      "595000  90% ( 58m 55s)   0.383   |   0.08: 12 June 2012 ->  the twelfth of june twenty twelve (the twelfth of june twenty twelve)\n",
      "600000 100% ( 61m 25s)   0.380   |   1.06: JAZU ->  j j j a (j a z u)\n",
      "Saving: data/models/gen_1_rnn_600000\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50000, teacher_forcing_ratio=0.0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, s_bef, s_word_vs, max_length=20):\n",
    "    s_string = string_to_tensor(s_bef, include_eos=True)\n",
    "\n",
    "    encoder_output = encoder_rnn(Variable(torch.from_numpy(s_word_vs)).cuda(), Variable(s_string).cuda())\n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    decoder_hidden = encoder_output\n",
    "    \n",
    "    decoder_input = sos_variable.view(1,1,-1).cuda()\n",
    "\n",
    "    decoded_chars = []\n",
    "    #for _ in range(max_length):\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = letters_all[char_index]\n",
    "        decoded_chars.append(char)\n",
    "        if char == '<EOS>':\n",
    "            break\n",
    "        \n",
    "        decoder_input = Variable(string_to_tensor(char)).cuda()\n",
    "\n",
    "    return ''.join(decoded_chars)\n",
    "\n",
    "#evaluate(encoder_rnn, decoder_rnn, s_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$18,865              -> eighteen thousand ei \n",
      "                     != eighteen thousand eight hundred sixty five dollars\n",
      "754-1985             -> seven five five fie  \n",
      "                     != seven five four sil one nine eight five\n",
      "Cornwall             -> Conrllll \n",
      "                     != Cornwall\n",
      "espn                 -> espnnnt \n",
      "                     != e s p n\n",
      "NUTV                 -> n u t t \n",
      "                     != n u t v\n",
      "23 August 2012       -> the twenty third of  \n",
      "                     != the twenty third of august twenty twelve\n",
      "298                  -> two hundred ni \n",
      "                     != two nine eight\n",
      "2.38                 -> two point three eigh \n",
      "                     != two point three eight\n",
      "$18,125              -> eighteen thousand tw \n",
      "                     != eighteen thousand one hundred twenty five dollars\n",
      "Accuracy: 55.00% (      11/      20)\n"
     ]
    }
   ],
   "source": [
    "def test_model_accuracy(n_sample=10000, print_wrongs=False):\n",
    "    data_balance_randomize_classes()\n",
    "    n_correct = 0\n",
    "    for iteration in range(n_sample):\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "        output = evaluate(encoder_rnn, decoder_rnn, s_bef, s_word_vs)\n",
    "        \n",
    "        output = output[0:len(s_aft)]\n",
    "        if s_aft == output:\n",
    "            n_correct += 1\n",
    "        else:\n",
    "            if print_wrongs:\n",
    "                print(\"{:<20} -> {} \\n{:<20} != {}\".format(s_bef, output, '', s_aft))\n",
    "                \n",
    "\n",
    "    print(\"Accuracy: {:>4.2%} ({:>8d}/{:>8d})\".format(\n",
    "            n_correct/n_sample, n_correct, n_sample))\n",
    "\n",
    "test_model_accuracy(20, print_wrongs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,003 m         -> one thousand thirty meter (one thousand three meters - MEASURE)\n",
      "500 lb          -> five hundred cennis  (five hundred pounds - MEASURE)\n",
      "07              -> o seven              (o seven    - DIGIT)\n",
      "Wug.za.net      -> z u g e dot e e e e e e (w u g dot z a dot n e t - ELECTRONIC)\n",
      "football        -> foobblll             (football   - PLAIN)\n",
      "1,300 ft        -> one thousand three hundred feet (one thousand three hundred feet - MEASURE)\n",
      "ESPN.com        -> e s p n dot c o m    (e s p n dot c o m - ELECTRONIC)\n",
      "connected       -> connentee            (connected  - PLAIN)\n",
      "2008            -> two thousand eight   (two thousand eight - DATE)\n",
      "1,153.1         -> one thousand one hundred tinety five ppont  nn (one thousand one hundred fifty three point one - DECIMAL)\n",
      "II              -> two second           (the second - ORDINAL)\n",
      ")               -> )                    ()          - PUNCT)\n",
      "$50,673         -> fifty thousand sex hundred seventy shree dollars (fifty thousand six hundred seventy three dollars - MONEY)\n",
      ".36             -> point three six      (point three six - DECIMAL)\n",
      "R&B             -> r and b              (r and b    - LETTERS)\n",
      "1,510 ft        -> one thousand five hundred fwenty s (one thousand five hundred ten feet - MEASURE)\n",
      "7 MB            -> seven milaretms      (seven megabytes - MEASURE)\n",
      "the             -> the                  (the        - PLAIN)\n",
      "1 million       -> one million          (one million - DECIMAL)\n",
      "40 percent      -> forty percent        (forty percent - MEASURE)\n",
      "1.55 km         -> one point five five kilometers (one point five five kilometers - MEASURE)\n",
      "Football        -> Foobabll             (Football   - PLAIN)\n",
      "Yoakam          -> Yokaam               (Yoakam     - PLAIN)\n",
      "(               -> (                    ((          - PUNCT)\n",
      "repair          -> repaai               (repair     - PLAIN)\n",
      "1               -> one                  (one        - CARDINAL)\n",
      "527-28479 U.S.  -> five two sil tilet nee                       (five two seven sil two eight four seven nine - TELEPHONE)\n",
      ".2005           -> point two o oivili   (point two o o five - DECIMAL)\n",
      "7th             -> seventh              (seventh    - ORDINAL)\n",
      "1.6             -> one point six        (one point six - DECIMAL)\n",
      "\n",
      "Clipped results because mistake in training\n"
     ]
    }
   ],
   "source": [
    "data_balance_randomize_classes(10000)\n",
    "for _ in range(30):\n",
    "    s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "    s_result = evaluate(encoder_rnn, decoder_rnn, s_bef, s_word_vs, max_length=50)   \n",
    "    \n",
    "    s_result = s_result[0:len(s_aft)]\n",
    "    \n",
    "    print(\"{:<15} -> {:<20} ({:<10} - {})\".format(\n",
    "        s_bef, s_result, s_aft, s_class))\n",
    "\n",
    "print()\n",
    "print(\"Clipped results because mistake in training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balance_randomize_classes(100000)\n",
    "data_balanced_classes.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_balanced_classes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_losses = [arr[0] for arr in model.train_history]\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_model_path = 'data/models/gen_1_rnn_' + str(model_train_iterations)\n",
    "    print(\"Saving:\", saved_model_path)\n",
    "    torch.save(decoder_rnn.state_dict(), saved_model_path+'_decoder')\n",
    "    torch.save(encoder_rnn.state_dict(), saved_model_path+'_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
