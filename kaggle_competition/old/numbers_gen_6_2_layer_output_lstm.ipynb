{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'numbers_gen_6_2_layer_output_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"data/en_train_fixed_1.pkl\", \"rb\" ))\n",
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>class_org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7313013</th>\n",
       "      <td>554751</td>\n",
       "      <td>11</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>;</td>\n",
       "      <td>;</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630324</th>\n",
       "      <td>354644</td>\n",
       "      <td>19</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>settlement</td>\n",
       "      <td>settlement</td>\n",
       "      <td>PLAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id        class      before       after class_org\n",
       "7313013       554751        11  NOT_CHANGED           ;           ;     PUNCT\n",
       "4630324       354644        19  NOT_CHANGED  settlement  settlement     PLAIN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT_CHANGED' 'NUMBERS' 'LETTERS' 'PLAIN' 'VERBATIM' 'ELECTRONIC']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS><EOS>☒ !\"#$%&'(),-./0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~£¥ª²³µº¼½¾éɒʻˈΩμ—€⅓⅔⅛\n"
     ]
    }
   ],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "print(''.join(chars_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['<EOS>', '<SOS>', '<UNK>', '<0000>', '<SAMPLE>', '.', ',', 'the', '\"', 'of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words, common_words_index = load_common_words_10k()\n",
    "len(common_words)\n",
    "common_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 448172,  (dropped rows: 9470020)\n"
     ]
    }
   ],
   "source": [
    "number_data = all_data[all_data['class'] == 'NUMBERS']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(number_data), len(all_data)-len(number_data)))\n",
    "number_data = number_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balanced_data = number_data\n",
    "\n",
    "balanced_data_length = len(balanced_data)\n",
    "def balanced_data_sample_row():\n",
    "    global balanced_data_last_sample\n",
    "    balanced_data_last_sample = balanced_data.iloc[random.randint(1, balanced_data_length-1)]\n",
    "    return balanced_data_last_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id                                 450963\n",
       "token_id                                         4\n",
       "class                                      NUMBERS\n",
       "before                              2 October 2011\n",
       "after          the second of october twenty eleven\n",
       "class_org                                     DATE\n",
       "Name: 268840, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " balanced_data_sample_row()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = list(set(list(number_data['after'])))\n",
    "arr = [s.split(' ') for s in arr]\n",
    "arr = np.concatenate(arr)\n",
    "arr = sorted(list(set(arr)))\n",
    "number_words = [EOS_TOKEN, SOS_TOKEN, UNKNOWN_WORD_TOKEN, NUMBER_WORD_TOKEN, SAMPLE_WORD_TOKEN] + arr\n",
    "number_words_index = dict((c, i) for i, c in enumerate(number_words))\n",
    "len(number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 511])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def number_words_to_tensor(words, include_eos=True):\n",
    "    return words_to_tensor(words, words_lookup_index=number_words_index, include_eos=include_eos)\n",
    "number_words_to_tensor(['one', 'first']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words_index['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 511])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words_onehot_sos = number_words_to_tensor([SOS_TOKEN], include_eos=False)\n",
    "#number_words_onehot_sos = Variable(torch.from_numpy(number_words_onehot_sos)).cuda()\n",
    "number_words_onehot_sos.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBERS : 725 -> seven hundred twenty five\n",
      "During the five day procedure <SAMPLE> landings were made while 94 pilots were qualified .\n",
      "['During', 'the', 'five', 'day', 'procedure', '<SAMPLE>', 'landings', 'were', 'made', 'while', '94', 'pilots', 'were', 'qualified', '.']\n",
      "torch.Size([1, 16, 8192])\n",
      "torch.Size([1, 4, 104])\n",
      "torch.Size([1, 5, 511])\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = balanced_data_sample_row()\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = list(rows.before)\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = SAMPLE_WORD_TOKEN\n",
    "    \n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], befores\n",
    "            \n",
    "def tmp():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    print(s_class, ':', s_bef, '->', s_aft)\n",
    "    print(' '.join(s_sentence))\n",
    "    print(s_sentence)\n",
    "    print(words_to_tensor(list(s_sentence), common_words_index).shape)\n",
    "    print(string_to_tensor(s_bef, chars_normal_index).shape)\n",
    "    print(number_words_to_tensor(s_aft.split(' ')).shape)\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN (\n",
       "  (rnn_words): LSTM(8192, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_rnn = EncoderRNN(words_input_size=len(common_words), chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=128,\n",
    "                         words_layers=2, chars_layers=2).cuda()\n",
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoder_single_sample():\n",
    "    s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    return encoder_rnn(words_t, string_t)\n",
    "    \n",
    "encoder_output = test_encoder_single_sample()\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN (\n",
       "  (rnn): LSTM(511, 256, num_layers=2, batch_first=True)\n",
       "  (lin_out): Linear (256 -> 511)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                         # LSTM would require own hidden included\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        hid_var_1 = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "        hid_var_2 = Variable(torch.zeros(self.n_layers, 1, self.hidden_size)).cuda()\n",
    "        res_1 = torch.cat((input_var, hid_var_1), 0)\n",
    "        return res_1, hid_var_2\n",
    "        \n",
    "\n",
    "decoder_rnn = DecoderRNN(input_size=len(number_words), hidden_size=encoder_output.size()[-1], n_layers=2)\n",
    "decoder_rnn = decoder_rnn.cuda()\n",
    "decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 1, 256]), torch.Size([2, 1, 256])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.size() for a in decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 511])\n",
      "[torch.Size([2, 1, 256]), torch.Size([2, 1, 256])]\n",
      "Variable containing:\n",
      " 462\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "toronto\n"
     ]
    }
   ],
   "source": [
    "tmp_hiddens = decoder_rnn.init_rest_hidden(encoder_output.view(1,1,-1))\n",
    "tmp_a, tmp_b = decoder_rnn(Variable(number_words_onehot_sos).cuda(), tmp_hiddens)\n",
    "print(tmp_a.size())\n",
    "print([a.size() for a in tmp_b])\n",
    "print(tmp_a.topk(1)[1])\n",
    "print(number_words[tmp_a.topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 511])\n",
      "Variable containing:\n",
      " 462\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "toronto\n"
     ]
    }
   ],
   "source": [
    "tmp_a, tmp_b = decoder_rnn(Variable(number_words_onehot_sos).cuda(), tmp_b)\n",
    "print(tmp_a.size())\n",
    "print(tmp_a.topk(1)[1])\n",
    "print(number_words[tmp_a.topk(1)[1].data[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto',\n",
       " 'toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto',\n",
       " 'twelve',\n",
       " ('12',\n",
       "  'twelve',\n",
       "  'NUMBERS',\n",
       "  ['Ynkepenee',\n",
       "   '12',\n",
       "   '30, ',\n",
       "   'Yngelpenne',\n",
       "   '12',\n",
       "   '35,',\n",
       "   ',',\n",
       "   'Ynkepenne',\n",
       "   '12',\n",
       "   '41, ',\n",
       "   'Ingelpenne',\n",
       "   '12',\n",
       "   '41,',\n",
       "   ',',\n",
       "   'Hingepenna',\n",
       "   '12',\n",
       "   '42, ',\n",
       "   'Ingepepenn',\n",
       "   '<SAMPLE>',\n",
       "   '42,',\n",
       "   ',',\n",
       "   'Ingelpenn',\n",
       "   '12',\n",
       "   '52, ',\n",
       "   'Enkepenne',\n",
       "   '12',\n",
       "   '82,',\n",
       "   ',',\n",
       "   'Inckepene',\n",
       "   '1292',\n",
       "   '.']))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_single_sample(model=None):\n",
    "    s_bef, s_aft, s_class, s_sentence = sample = get_random_sample()\n",
    "        \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_output)\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    max_length = 20\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    return output, output, s_aft, sample\n",
    "    \n",
    "tmp = test_model_single_sample(None)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_local_wrong_predictions(max_results=10):\n",
    "    arr = get_some_wrong_predictions(None, test_model_single_sample, max_iterations=10000, max_results=max_results)\n",
    "    for sample, predict, output in arr:\n",
    "        s_bef, s_aft, s_class, s_sentence = sample\n",
    "        print(\"{:<14} => {:<14} || {} \\n{:>17} {}\".format(s_bef, predict, s_aft, '', ' '.join(s_sentence), ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11             => toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto || eleven \n",
      "                  \" Dog Hears Senate Howl ; Blind Owner Sees to It , \" New York Times ( January 6, 1950 ) : <SAMPLE> .\n",
      "1662           => toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto toronto || sixteen sixty two \n",
      "                  \" Register of Salomon La Chair , Notary Public at New Amsterdam 1661 - <SAMPLE> \" .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (       0/   10000)\n",
      "CPU times: user 14min 15s, sys: 16.5 s, total: 14min 32s\n",
      "Wall time: 6min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_model_accuracy(encoder_rnn, test_model_single_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(s_bef, s_aft, s_sentence, encoder_optimizer, decoder_optimizer, loss_function,\n",
    "          use_teacher_forcing, max_length=20):\n",
    "    \n",
    "    words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = encoder_rnn(words_t, string_t)\n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = decoder_rnn.init_rest_hidden(encoder_output)\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    target_arr = s_aft.split(' ') + [EOS_TOKEN]\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    decoded_output = []\n",
    "    for i in range(len(target_arr)):\n",
    "        decoder_output, decoder_hidden = decoder_rnn(decoder_input, decoder_hidden)\n",
    "\n",
    "        decoder_target_i = number_words_index[target_arr[i]]\n",
    "        decoder_target_i = Variable(torch.LongTensor([decoder_target_i])).cuda()\n",
    "        loss += loss_function(decoder_output, decoder_target_i)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            word = target_arr[i] # replace input with right target\n",
    "        else:\n",
    "            # use output normally as input \n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "    if decoded_output[-1] == EOS_TOKEN:\n",
    "        decoded_output = decoded_output[:-1]\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ' '.join(decoded_output), (loss.data[0] / len(target_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, teacher_forcing_ratio=0.5,\n",
    "                     print_every=10000, plot_every=1000):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    decoder_rnn.train()\n",
    "    encoder_rnn.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_rnn.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model_training.iterations += 1\n",
    "        \n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        s_bef, s_aft, s_class, s_sentence = get_random_sample()\n",
    "        \n",
    "        result, loss = train(s_bef=s_bef, s_aft=s_aft, s_sentence=s_sentence,\n",
    "                             encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
    "                             loss_function=nn.NLLLoss(), use_teacher_forcing=use_teacher_forcing,\n",
    "                             max_length=40 )\n",
    "        \n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            teacher_forcing_str = \"\"\n",
    "            if use_teacher_forcing:\n",
    "                teacher_forcing_str = \"(forcing)\"\n",
    "            correct = '✓' if result == s_aft else \"✗: {}\".format(s_aft)\n",
    "            \n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} ({}) {}\".format(\n",
    "                      model_training.iterations, iteration/n_iters, time_since(start),\n",
    "                      current_loss/current_loss_iter, loss,\n",
    "                      s_bef, result, correct, teacher_forcing_str))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model_training.losses.append(current_loss / plot_every)\n",
    "            model_training.learning_rates.append(lr)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model_training.iterations % 50000 == 0 or model_training.iterations == 10:\n",
    "            model_training.save_models()\n",
    "            acc = test_model_accuracy(encoder_rnn, test_model_single_sample)\n",
    "            model_training.accuracy.append(acc)\n",
    "    \n",
    "    # test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path: data/models/numbers_gen_6_2_layer_output_lstm\n"
     ]
    }
   ],
   "source": [
    "model_training = ModelTraining(MODEL_SAVE_PATH, [encoder_rnn, decoder_rnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9  18% (   0m 0s)   6.233   |   6.21: 2 -> toronto toronto (✗: two) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/10_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 0.00% (       0/   10000)\n",
      "    18  36% (  6m 12s)   6.230   |   6.23: 2006 -> toronto toronto toronto toronto (✗: two thousand six) (forcing)\n",
      "    27  54% (  6m 12s)   6.226   |   6.21: March 22, 2011 -> two two two two two two (✗: march twenty second twenty eleven) (forcing)\n",
      "    36  72% (  6m 13s)   6.160   |   6.19: 1984 -> two two two (✗: nineteen eighty four) (forcing)\n",
      "    45  90% (  6m 13s)   6.085   |   6.18: 3 -> two two (✗: three) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=50, print_every=9, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   550  53% (  0m 14s)   2.853   |   3.90: December 2011 -> two <EOS> <EOS> (✗: december twenty eleven) (forcing)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=(1000-model_training.iterations), print_every=500, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000  11% (  0m 33s)   2.531   |   2.87: 89 -> two (✗: eighty nine) \n",
      "  3000  22% (   1m 7s)   2.575   |   2.23: 50 -> two (✗: fifty) \n",
      "  4000  33% (  1m 41s)   2.487   |   1.65: 5 -> two (✗: five) (forcing)\n",
      "  5000  44% (  2m 13s)   2.405   |   2.18: 29 August 2013 -> the twenty of of twenty twenty (✗: the twenty ninth of august twenty thirteen) \n",
      "  6000  56% (  2m 46s)   2.356   |   2.57: 33 -> one <EOS> (✗: thirty three) (forcing)\n",
      "  7000  67% (  3m 18s)   2.322   |   1.84: 3,666.3 -> the twenty thousand (✗: three thousand six hundred sixty six point three) \n",
      "  8000  78% (  3m 54s)   2.238   |   3.13: May 31, 2013 -> the twenty thousand <EOS> <EOS> (✗: may thirty first twenty thirteen) (forcing)\n",
      "  9000  89% (  4m 29s)   2.240   |   1.11: 1 -> one (✓) \n",
      " 10000 100% (   5m 4s)   2.203   |   1.15: II -> two (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=9000, lr=0.0001, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  11% (  5m 41s)   1.518   |   2.46: 18 November 1902 -> the twenty of of twenty twenty (✗: the eighteenth of november nineteen o two) \n",
      " 30000  22% ( 11m 27s)   1.200   |   0.02: 2 -> two (✓) \n",
      " 40000  33% ( 16m 52s)   0.918   |   0.18: 48 -> forty eight (✓) \n",
      " 50000  44% ( 22m 29s)   0.737   |   0.47: 247 -> two hundred forty nine (✗: two hundred forty seven) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/50000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 52.82% (    5282/   10000)\n",
      " 60000  56% ( 30m 15s)   0.572   |   1.50: 1454.5/mi² -> one hundred three hundred fifty point point five per square kilometers (✗: one thousand four hundred fifty four point five per square miles) (forcing)\n",
      " 70000  67% (  36m 4s)   0.515   |   0.72: 17 April 1973 -> the nineteenth of july nineteen ninety three (✗: the seventeenth of april nineteen seventy three) \n",
      " 80000  78% ( 41m 51s)   0.479   |   0.00: 14 -> fourteen (✓) (forcing)\n",
      " 90000  89% ( 47m 40s)   0.425   |   0.00: 4 -> four (✓) (forcing)\n",
      "100000 100% ( 53m 17s)   0.379   |   0.00: 2000 -> two thousand (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 67.15% (    6715/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=90000, print_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000   3% (  5m 42s)   0.353   |   0.00: 2008 -> two thousand eight (✓) \n",
      "120000   7% ( 10m 47s)   0.355   |   0.03: II -> two (✓) (forcing)\n",
      "130000  10% ( 14m 50s)   0.333   |   0.05: 5.3% -> five point three percent (✓) \n",
      "140000  13% ( 18m 54s)   0.333   |   0.00: 220 -> two hundred twenty (✓) (forcing)\n",
      "150000  17% ( 22m 55s)   0.298   |   0.02: 2006 -> two thousand six (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 72.76% (    7276/   10000)\n",
      "160000  20% ( 28m 16s)   0.273   |   0.01: 2013 -> twenty thirteen (✓) \n",
      "170000  23% ( 32m 20s)   0.298   |   0.29: September 16, 2010 -> september sixteenth twenty ten (✓) \n",
      "180000  27% ( 36m 23s)   0.216   |   0.01: 1982 -> nineteen eighty two (✓) (forcing)\n",
      "190000  30% ( 40m 27s)   0.234   |   0.48: 0.03 -> zero point five three (✗: zero point o three) (forcing)\n",
      "200000  33% ( 43m 21s)   0.265   |   0.00: 3 -> three (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 79.07% (    7907/   10000)\n",
      "210000  37% (  47m 4s)   0.224   |   0.02: April 25, 1990 -> april twenty fifth nineteen ninety (✓) (forcing)\n",
      "220000  40% ( 49m 53s)   0.210   |   0.00: 5 -> five (✓) \n",
      "230000  43% ( 52m 43s)   0.216   |   0.63: $59,809 -> fifty nine thousand eight hundred ninety dollars (✗: fifty nine thousand eight hundred nine dollars) (forcing)\n",
      "240000  47% ( 55m 32s)   0.180   |   0.43: 16 January 2013 -> the sixteenth of may twenty thirteen (✗: the sixteenth of january twenty thirteen) (forcing)\n",
      "250000  50% ( 58m 22s)   0.171   |   0.00: 80 -> eighty (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 82.70% (    8270/   10000)\n",
      "260000  53% (  62m 5s)   0.177   |   0.00: September 13, 2005 -> september thirteenth two thousand five (✓) (forcing)\n",
      "270000  57% ( 64m 53s)   0.201   |   0.01: 1970 -> nineteen seventy (✓) (forcing)\n",
      "280000  60% ( 67m 41s)   0.157   |   0.03: 1910 -> nineteen ten (✓) (forcing)\n",
      "290000  63% ( 70m 31s)   0.166   |   0.00: 1904 -> nineteen o four (✓) \n",
      "300000  67% ( 73m 20s)   0.143   |   0.00: 1962 -> nineteen sixty two (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 85.76% (    8576/   10000)\n",
      "310000  70% (  77m 5s)   0.152   |   0.00: 1996 -> nineteen ninety six (✓) (forcing)\n",
      "320000  73% ( 79m 54s)   0.165   |   0.05: .472 -> point four seven two (✓) \n",
      "330000  77% ( 82m 44s)   0.156   |   0.01: 1979 -> nineteen seventy nine (✓) \n",
      "340000  80% ( 85m 33s)   0.134   |   0.32: 2009-06-03 -> the third of march two thousand nine (✗: the third of june two thousand nine) (forcing)\n",
      "350000  83% ( 88m 24s)   0.120   |   0.00: 11 June 2014 -> the eleventh of june twenty fourteen (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 91.07% (    9107/   10000)\n",
      "360000  87% (  92m 8s)   0.123   |   0.05: VI -> the sixth (✓) \n",
      "370000  90% ( 94m 57s)   0.104   |   0.01: 2010 -> twenty ten (✓) \n",
      "380000  93% ( 97m 44s)   0.141   |   0.18: 5,100 m -> five thousand one hundred <EOS> (✗: five thousand one hundred meters) (forcing)\n",
      "390000  97% (100m 34s)   0.101   |   0.00: 1976 -> nineteen seventy six (✓) \n",
      "400000 100% (103m 24s)   0.133   |   0.00: October 3, 2011 -> october third twenty eleven (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 90.96% (    9096/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-06-21     => the twenty first of february twenty ten || the twenty first of june twenty ten \n",
      "                  Archived from the original on <SAMPLE> .\n",
      "XVII           => twenty         || seventeen \n",
      "                  \" Book IV , Chapter <SAMPLE> : Of Reason \" .\n",
      "1999           => nineteen ninety nine || one thousand nine hundred ninety nine \n",
      "                  Between <SAMPLE> - 2000 , Farouq , under the mandatory National Youth Service Corps ( NYSC ) Scheme served with the Nigeria National Assembly .\n",
      "$35.01         => thirty five thousand two hundred one dollars || thirty five dollars and one cent \n",
      "                  Shares sharply rose 19% by the end of the trading day to <SAMPLE> a share , up from $26 .\n",
      "Fri 29 Nov 2002 => friday the twenty first of february two thousand two || friday the twenty ninth of november two thousand two \n",
      "                  \" Official Results <SAMPLE> \" ( PDF ) .\n",
      "198 cm         => one hundred ninety eight feet || one hundred ninety eight centimeters \n",
      "                  In his teens , Prowse was 6 feet 6 inches ( <SAMPLE> ) tall , and developed an interest in bodybuilding .\n",
      "08-10844       => o sil o six four sil one o seven four || o eight sil one o eight four four \n",
      "                  <SAMPLE> ( Bkrtcy W.D. Washington ) \" ( PDF ) .\n",
      "11TH           => eleven feet    || eleventh \n",
      "                  Winner of the <SAMPLE> ARTS AWARD and first ever PRODUCT DESIGNER OF THE YEAR .\n",
      "26.7 mi        => twenty six point seven per per hour || twenty six point seven miles \n",
      "                  The third stage was a lengthy team time trial that stretched for 43 km ( <SAMPLE> ) between Lerici and Camaiore .\n",
      "1910 BC        => one ninety b c || nineteen ten b c \n",
      "                  Even afterward until at least <SAMPLE> commonly looked south instead of east .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410000   3% (  2m 47s)   0.115   |   0.00: 59% -> fifty nine percent (✓) \n",
      "420000   7% (  5m 36s)   0.100   |   0.00: 26 -> twenty six (✓) \n",
      "430000  10% (  8m 24s)   0.150   |   0.06: III -> three (✓) \n",
      "440000  13% ( 11m 14s)   0.129   |   0.00: 301 -> three hundred one (✓) \n",
      "450000  17% (  14m 2s)   0.149   |   0.00: 1.9 mi -> one point nine miles (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.24% (    9224/   10000)\n",
      "460000  20% ( 17m 46s)   0.128   |   0.00: 4 -> four (✓) \n",
      "470000  23% ( 20m 35s)   0.132   |   0.00: December 23, 2014 -> december twenty third twenty fourteen (✓) \n",
      "480000  27% ( 23m 24s)   0.129   |   0.00: 1659 -> sixteen fifty nine (✓) (forcing)\n",
      "490000  30% ( 26m 12s)   0.141   |   3.11: 0 -> zero (✗: o) \n",
      "500000  33% (  29m 2s)   0.123   |   0.00: 90 -> ninety (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 91.25% (    9125/   10000)\n",
      "510000  37% ( 32m 45s)   0.172   |   0.01: 577 -> five hundred seventy seven (✓) (forcing)\n",
      "520000  40% ( 35m 34s)   0.123   |   0.00: 2008 -> two thousand eight (✓) \n",
      "530000  43% ( 38m 23s)   0.097   |   0.00: 15 -> fifteen (✓) (forcing)\n",
      "540000  47% ( 41m 11s)   0.124   |   0.00: 1980s -> nineteen eighties (✓) \n",
      "550000  50% ( 43m 59s)   0.177   |   0.00: 3rd -> third (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 91.46% (    9146/   10000)\n",
      "560000  53% ( 47m 42s)   0.128   |   0.00: 14 June 2012 -> the fourteenth of june twenty twelve (✓) (forcing)\n",
      "570000  57% ( 50m 31s)   0.115   |   0.00: 12 January 2009 -> the twelfth of january two thousand nine (✓) \n",
      "580000  60% ( 53m 20s)   0.130   |   0.00: 64 -> sixty four (✓) \n",
      "590000  63% (  56m 9s)   0.107   |   0.02: 1500 m -> one thousand five hundred meters (✓) \n",
      "600000  67% ( 58m 58s)   0.091   |   0.06: 9,821 -> nine thousand eight hundred twenty one (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.97% (    9297/   10000)\n",
      "610000  70% ( 62m 41s)   0.127   |   0.00: 1.3 -> one point three (✓) \n",
      "620000  73% ( 65m 30s)   0.113   |   0.00: 7 -> seven (✓) (forcing)\n",
      "630000  77% ( 68m 20s)   0.139   |   0.00: January 4, 2010 -> january fourth twenty ten (✓) \n",
      "640000  80% (  71m 7s)   0.161   |   0.00: 2004 -> two thousand four (✓) \n",
      "650000  83% ( 73m 57s)   0.084   |   0.01: 47 -> forty seven (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 93.18% (    9318/   10000)\n",
      "660000  87% ( 77m 42s)   0.109   |   0.00: July 8, 2010 -> july eighth twenty ten (✓) \n",
      "670000  90% ( 80m 31s)   0.149   |   0.01: 2002 -> two thousand two (✓) \n",
      "680000  93% ( 83m 20s)   0.114   |   0.00: 2012 -> twenty twelve (✓) \n",
      "690000  97% (  86m 9s)   0.137   |   0.00: 8 -> eight (✓) \n",
      "700000 100% ( 88m 58s)   0.095   |   0.00: 588 -> five hundred eighty eight (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/700000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.75% (    9275/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19%          => zero point one seven percent || zero point one nine percent \n",
      "                  It got 8393 votes ( <SAMPLE> of the statewide vote ) , but was not close to winning any seat .\n",
      "1912/3         => nineteen thousand two hundred hours || one thousand nine hundred twelve thirds \n",
      "                  This was mainly intended for goods , but a passenger service did run on the branch until <SAMPLE> .\n",
      "$49,808        => forty nine thousand six hundred eight dollars || forty nine thousand eight hundred eight dollars \n",
      "                  The median income for a household in the village was <SAMPLE> , and the median income for a family was $56,375 .\n",
      "64,000,000 m3  => sixty four thousand square miles || sixty four million cubic meters \n",
      "                  The project diverts and delivers an average of 52,000 acre feet ( <SAMPLE> ) of water a year .\n",
      "343/344        => three hundred forty three thousand four hectares || three hundred forty three three hundred forty fourths \n",
      "                  <SAMPLE> Bangkok Kaeng Khoi Junction Bangkok ( weekends only ) Local no\n",
      "3-05-002363-5  => three sil five o o sil o o six three three sil five || three sil o five sil o o two three six three sil five \n",
      "                  Berlin : Akademie Verlag 1992 ISBN <SAMPLE> ( Planetary Politics after the Cold War ) .\n",
      "206 BC         => o six six b c  || two hundred six b c \n",
      "                  During the Han period ( <SAMPLE> to 220 AD ) the use of glass diversified .\n",
      "27 BC,         => twenty seven b b || twenty seven b c \n",
      "                  In <SAMPLE> , Octavian offered to transfer control of the state back to the Senate .\n",
      ".146           => point one three six || point one four six \n",
      "                  Serindia Publications , inc Source : ( accessed : Sunday April 12, 2009 ) , p <SAMPLE> Wallace , B. Alan with Goleman , Daniel ( 2006 ) .\n",
      "August 30, 1990 => june thirtieth nineteen ninety || august thirtieth nineteen ninety \n",
      "                  <SAMPLE> .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710000   3% (  2m 48s)   0.094   |   0.00: 41st -> forty first (✓) \n",
      "720000   7% (  5m 38s)   0.102   |   0.00: 168 -> one hundred sixty eight (✓) (forcing)\n",
      "730000  10% (  8m 27s)   0.073   |   0.00: 11 -> eleven (✓) (forcing)\n",
      "740000  13% ( 11m 18s)   0.068   |   0.01: 35 -> thirty five (✓) \n",
      "750000  17% (  14m 7s)   0.056   |   0.00: 300,000 -> three hundred thousand (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/750000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.76% (    9476/   10000)\n",
      "760000  20% ( 17m 51s)   0.063   |   0.00: 69 -> sixty nine (✓) (forcing)\n",
      "770000  23% ( 20m 40s)   0.073   |   0.00: 1982 -> nineteen eighty two (✓) \n",
      "780000  27% ( 23m 30s)   0.079   |   0.00: 1st -> first (✓) (forcing)\n",
      "790000  30% ( 26m 20s)   0.078   |   0.00: 4 April 2011 -> the fourth of april twenty eleven (✓) (forcing)\n",
      "800000  33% (  29m 9s)   0.072   |   0.16: 2011 -> twenty eleven (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/800000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.69% (    9469/   10000)\n",
      "810000  37% ( 32m 53s)   0.074   |   0.00: 2 -> two (✓) \n",
      "820000  40% ( 35m 45s)   0.066   |   0.01: 50 -> fifty (✓) (forcing)\n",
      "830000  43% ( 38m 36s)   0.055   |   0.00: 2012 -> twenty twelve (✓) (forcing)\n",
      "840000  47% ( 41m 26s)   0.087   |   0.00: 62 -> sixty two (✓) (forcing)\n",
      "850000  50% ( 44m 15s)   0.046   |   0.14: 2013-01-17 -> the seventeenth of january twenty thirteen (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/850000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.68% (    9468/   10000)\n",
      "860000  53% ( 47m 59s)   0.066   |   0.00: 15.0 -> fifteen point zero (✓) (forcing)\n",
      "870000  57% ( 50m 50s)   0.073   |   0.00: January 28, 2014 -> january twenty eighth twenty fourteen (✓) \n",
      "880000  60% ( 53m 39s)   0.053   |   0.00: 2000s -> two thousands (✓) \n",
      "890000  63% ( 56m 29s)   0.051   |   0.00: 2013 -> twenty thirteen (✓) \n",
      "900000  67% ( 59m 19s)   0.062   |   0.00: 3 -> three (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/900000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.86% (    9486/   10000)\n",
      "910000  70% (  63m 4s)   0.088   |   0.00: 21 June 2002 -> the twenty first of june two thousand two (✓) \n",
      "920000  73% ( 65m 54s)   0.052   |   0.00: 60 ft -> sixty feet (✓) (forcing)\n",
      "930000  77% ( 68m 45s)   0.056   |   0.00: 22 -> twenty two (✓) (forcing)\n",
      "940000  80% ( 71m 35s)   0.066   |   0.00: 74 -> seventy four (✓) (forcing)\n",
      "950000  83% ( 74m 25s)   0.075   |   0.00: 2005 -> two thousand five (✓) (forcing)\n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/950000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.45% (    9545/   10000)\n",
      "960000  87% ( 78m 10s)   0.064   |   0.18: 2009-07-24 -> the twenty fourth of march two thousand nine (✗: the twenty fourth of july two thousand nine) \n",
      "970000  90% (  81m 1s)   0.057   |   0.00: 80 -> eighty (✓) (forcing)\n",
      "980000  93% ( 83m 51s)   0.053   |   0.00: 10 -> ten (✓) \n",
      "990000  97% ( 86m 43s)   0.059   |   0.00: December 10, 1993 -> december tenth nineteen ninety three (✓) \n",
      "1000000 100% ( 89m 32s)   0.056   |   0.00: 1969 -> nineteen sixty nine (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1000000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.78% (    9578/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0.5, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US$14.5        => fourteen dollars || fourteen dollars and fifty cents \n",
      "                  Early estimates placed insured losses from the earthquake alone at <SAMPLE> to $34.6 billion .\n",
      "0-06-438580-9  => o sil o six sil o o o o five o sil nine || o sil o six sil four three eight five eight o sil nine \n",
      "                  ISBN <SAMPLE> Vitruvius , Translation : Morris Hicky Morgan ( 1960 ) .\n",
      "6256-6259      => six two five five sil two two five nine || six two five six sil six two five nine \n",
      "                  Score , Edition Peters <SAMPLE> .\n",
      "2001-10-28     => the twenty eighth of december two thousand one || the twenty eighth of october two thousand one \n",
      "                  Patrick Marber Biography ( 1964 - ) Marowitz , Charles ( <SAMPLE> ) .\n",
      "1391           => thirteen ninety one || one three nine one \n",
      "                  It meets a roundabout for Swineshead and passes through Drayton , meets the <SAMPLE> as Abbey Lane , then Drayton Road .\n",
      "95.45%         => ninety six point two three percent || ninety five point four five percent \n",
      "                  1901 : 440 inhabitants of which 420 ( <SAMPLE> ) where Protestant and 20 where Roman Catholic ( 4.55% ) .\n",
      "2006.12        => two thousand six point and forty two || two thousand six point one two \n",
      "                  Proceedings of the Water Environment Federation <SAMPLE> ( 2006 ) : 1460 - 1467 .\n",
      "0-86196-653-8  => o sil eight six five six three sil three six five sil eight || o sil eight six one nine six sil six five three sil eight \n",
      "                  Koszarski , Richard ( 2004 ) , Fort Lee : The Film Town , Rome , Italy : John Libbey Publishing - CIC srl , ISBN <SAMPLE> \" Studios and Films \" .\n",
      "800            => eight hundred  || eight o o \n",
      "                  \" iBall Slide 3 G 7345 Q- <SAMPLE> Voice Calling Tablet Available Online at Rs .\n",
      "2010-04-12     => the twelfth of july twenty ten || the twelfth of april twenty ten \n",
      "                  Spikes Magazine ( <SAMPLE> ) .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010000  10% (  2m 50s)   0.133   |   0.00: 11 March 2016 -> the eleventh of march twenty sixteen (✓) \n",
      "1020000  20% (  5m 38s)   0.127   |   0.00: 1986 -> nineteen eighty six (✓) \n",
      "1030000  30% (  8m 28s)   0.106   |   0.03: $399 -> three hundred ninety nine dollars (✓) \n",
      "1040000  40% ( 11m 17s)   0.107   |   0.00: 2001 -> two thousand one (✓) \n",
      "1050000  50% (  14m 9s)   0.111   |   0.00: 720 -> seven hundred twenty (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.81% (    9281/   10000)\n",
      "1060000  60% ( 17m 53s)   0.132   |   0.00: 7.49 -> seven point four nine (✓) \n",
      "1070000  70% ( 20m 44s)   0.118   |   0.00: December 6, 2010 -> december sixth twenty ten (✓) \n",
      "1080000  80% ( 23m 34s)   0.152   |   0.01: 2015 -> twenty fifteen (✓) \n",
      "1090000  90% ( 26m 23s)   0.117   |   0.00: 50,000 -> fifty thousand (✓) \n",
      "1100000 100% ( 29m 14s)   0.131   |   0.00: 1987 -> nineteen eighty seven (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 92.51% (    9251/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000, print_every=10000, teacher_forcing_ratio=0, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-07-15     => the fifteenth of march twenty twelve || the fifteenth of july twenty twelve \n",
      "                  Retrieved on <SAMPLE> .\n",
      "169/4522       => one hundred sixty nine thousand two hundred || one hundred sixty nine four thousand five hundred twenty seconds \n",
      "                  45 RTR War Diary , Oct 1942 , The National Archives , file WO <SAMPLE> .\n",
      "2012-03-04     => the fourth of july twenty twelve || the fourth of march twenty twelve \n",
      "                  Archived from the original on <SAMPLE> .\n",
      "19278          => nineteen thousand two hundred eighty eight || nineteen thousand two hundred seventy eight \n",
      "                  5 ; Issue <SAMPLE> ; col EThe Times , Thursday, May 06, 1847 ; pg .\n",
      "2007-04-12     => the twelfth of july two thousand seven || the twelfth of april two thousand seven \n",
      "                  Ross , Barbara ; Tracy Connor ( <SAMPLE> ) .\n",
      "05-2010        => the fifth of march twenty ten || o five sil two o one o \n",
      "                  There have been 6 rounds of surveys to date ( 1989-1992-1996-2000-2004 / <SAMPLE> ) .\n",
      "0-09-183551-8  => o sil o nine sil o sil o o one one sil eight || o sil o nine sil one eight three five five one sil eight \n",
      "                  ISBN <SAMPLE> The Broken Book , Allan & Unwin , 2005 .\n",
      "85.24%         => eighty five thousand two one percent || eighty five point two four percent \n",
      "                  Dehradun has a sex ratio of 902 females for every 1000 males , and a literacy rate of <SAMPLE> .\n",
      "16             => one six        || sixteen \n",
      "                  In 11 games , he collected two hits in <SAMPLE> at bats for a .125 batting average .\n",
      "11393          => eleven thousand one hundred ninety three || eleven thousand three hundred ninety three \n",
      "                  Katarina , urn : sbl : <SAMPLE> , svensk t biografiskt lexikon ( art av Ake Kromnow ) , hamtad 2014-07-01 .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110000   3% (  2m 48s)   0.087   |   0.00: 57 -> fifty seven (✓) \n",
      "1120000   7% (  5m 37s)   0.089   |   0.01: 2007 -> two thousand seven (✓) \n",
      "1130000  10% (  8m 25s)   0.088   |   0.00: 42 -> forty two (✓) \n",
      "1140000  13% ( 11m 15s)   0.072   |   0.00: 5 April 2008 -> the fifth of april two thousand eight (✓) \n",
      "1150000  17% (  14m 4s)   0.076   |   0.00: 21 -> twenty one (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 94.96% (    9496/   10000)\n",
      "1160000  20% ( 17m 48s)   0.077   |   0.01: 2012 -> twenty twelve (✓) \n",
      "1170000  23% ( 20m 39s)   0.094   |   0.00: 2000 -> two thousand (✓) \n",
      "1180000  27% ( 23m 29s)   0.084   |   0.00: 67 -> sixty seven (✓) \n",
      "1190000  30% ( 26m 18s)   0.079   |   0.00: 14 February 2015 -> the fourteenth of february twenty fifteen (✓) \n",
      "1200000  33% (  29m 7s)   0.083   |   0.00: 1 -> one (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1200000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.02% (    9502/   10000)\n",
      "1210000  37% ( 32m 51s)   0.093   |   0.06: 300 -> three hundred (✓) \n",
      "1220000  40% ( 35m 41s)   0.069   |   1.58: II -> two (✗: the second) \n",
      "1230000  43% ( 38m 30s)   0.065   |   0.00: March 11 -> march eleventh (✓) \n",
      "1240000  47% ( 41m 19s)   0.078   |   0.00: 1979 -> nineteen seventy nine (✓) \n",
      "1250000  50% (  44m 7s)   0.053   |   0.11: 1065 -> ten sixty five (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1250000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.42% (    9542/   10000)\n",
      "1260000  53% ( 47m 52s)   0.060   |   0.00: 1934 -> nineteen thirty four (✓) \n",
      "1270000  57% ( 51m 12s)   0.063   |   0.00: 9,736 -> nine thousand seven hundred thirty six (✓) \n",
      "1280000  60% (  55m 9s)   0.060   |   0.00: September 30, 1996 -> september thirtieth nineteen ninety six (✓) \n",
      "1290000  63% ( 59m 23s)   0.066   |   0.00: 16 -> sixteen (✓) \n",
      "1300000  67% ( 63m 25s)   0.071   |   0.00: May 26, 2008 -> may twenty sixth two thousand eight (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1300000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.59% (    9559/   10000)\n",
      "1310000  70% ( 68m 35s)   0.066   |   0.00: 14 September 2014 -> the fourteenth of september twenty fourteen (✓) \n",
      "1320000  73% ( 72m 22s)   0.046   |   0.00: 1st -> first (✓) \n",
      "1330000  77% ( 76m 13s)   0.065   |   0.00: 150 -> one hundred fifty (✓) \n",
      "1340000  80% (  80m 3s)   0.081   |   0.00: 2 -> two (✓) \n",
      "1350000  83% ( 83m 54s)   0.062   |   0.00: 16 -> sixteen (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1350000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 95.83% (    9583/   10000)\n",
      "1360000  87% ( 88m 55s)   0.056   |   0.00: 10 -> ten (✓) \n",
      "1370000  90% ( 92m 48s)   0.045   |   0.01: 1877 -> eighteen seventy seven (✓) \n",
      "1380000  93% ( 96m 38s)   0.055   |   0.00: 15 -> fifteen (✓) \n",
      "1390000  97% (100m 30s)   0.069   |   2.16: 2065 MW -> two thousand six hundred fifty (✗: two thousand sixty five megawatts) \n",
      "1400000 100% (104m 19s)   0.062   |   0.00: December 2013 -> december twenty thirteen (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1400000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.26% (    9626/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=300000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,724.7        => five thousand seven hundred twenty nine point seven || five thousand seven hundred twenty four point seven \n",
      "                  The population density was <SAMPLE> people per square mile ( 2,214.2/km2 ) .\n",
      "(2009) 103     => two o o nine sil o o o three || two o o nine sil one o three \n",
      "                  Heredity <SAMPLE> , pp .\n",
      "0 906 245 06 0 => o sil nine o o o o o o o o sil o sil o || o sil nine o six sil two four five sil o six sil o \n",
      "                  ISBN <SAMPLE> .\n",
      "0-8020-8191-62011 => o sil eight o o o sil o o one one sil || o sil eight o two o sil eight one nine one sil six two o one one \n",
      "                  Toronto : University of Toronto Press , ISBN <SAMPLE> : Directions Home : Approaches to African Canadian Literature .\n",
      "Tuesday, October 5, 2004 => tuesday november fifth two thousand four || tuesday october fifth two thousand four \n",
      "                  John Scheinman , \" ABC Will Broadcast Belmont Stakes Starting in 2006 , \" The Washington Post , <SAMPLE> .\n",
      "10/03/2008     => the third of march two thousand eight || the third of october two thousand eight \n",
      "                  \" Cipro Labeling Revision <SAMPLE> Supplement 068 \" ( PDF ) .\n",
      "Sat 15 Dec 2001 => saturday five five of march two thousand one || saturday the fifteenth of december two thousand one \n",
      "                  \" Official Results <SAMPLE> \" ( PDF ) .\n",
      "150            => one hundred fifty || one five o \n",
      "                  The previous version of CRF <SAMPLE> F was the old CBZ classic .\n",
      "113            => one hundred thirteen || one one three \n",
      "                  FMC continued development as a private venture , resulting in the product improved ( PI ) M <SAMPLE> A 1 in 1970 .\n",
      "20033          => twenty thousand o three three || two o o three three \n",
      "                  Documentary Film by Sahitya Academy , New Delhi indiana , <SAMPLE> .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410000   5% (  3m 49s)   0.080   |   0.00: 1981 -> nineteen eighty one (✓) \n",
      "1420000  10% (  7m 40s)   0.071   |   0.00: 8 Oct 2011 -> the eighth of october twenty eleven (✓) \n",
      "1430000  15% ( 11m 33s)   0.058   |   0.00: 550 -> five hundred fifty (✓) \n",
      "1440000  20% ( 15m 24s)   0.065   |   0.00: 0.8% -> zero point eight percent (✓) \n",
      "1450000  25% (  19m 8s)   0.066   |   0.00: 1863 -> eighteen sixty three (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1450000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.16% (    9616/   10000)\n",
      "1460000  30% ( 24m 15s)   0.049   |   0.00: 56 -> fifty six (✓) \n",
      "1470000  35% (  28m 5s)   0.057   |   0.00: 100 -> one hundred (✓) \n",
      "1480000  40% ( 31m 54s)   0.080   |   0.01: 7/20 -> seven twentieths (✓) \n",
      "1490000  45% ( 35m 40s)   0.066   |   0.00: 1966 -> nineteen sixty six (✓) \n",
      "1500000  50% ( 39m 29s)   0.058   |   0.00: 24 -> twenty four (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1500000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.35% (    9635/   10000)\n",
      "1510000  55% ( 44m 35s)   0.088   |   0.00: 16 April 2013 -> the sixteenth of april twenty thirteen (✓) \n",
      "1520000  60% ( 48m 26s)   0.060   |   0.00: 1 Sep 2014 -> the first of september twenty fourteen (✓) \n",
      "1530000  65% ( 52m 16s)   0.056   |   0.00: 1869 -> eighteen sixty nine (✓) \n",
      "1540000  70% (  56m 2s)   0.081   |   0.00: 10 -> ten (✓) \n",
      "1550000  75% ( 59m 53s)   0.076   |   0.00: 2006 -> two thousand six (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1550000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.10% (    9610/   10000)\n",
      "1560000  80% (  65m 1s)   0.069   |   0.01: 1994 -> nineteen ninety four (✓) \n",
      "1570000  85% ( 72m 32s)   0.045   |   0.00: May 20, 2010 -> may twentieth twenty ten (✓) \n",
      "1580000  90% ( 76m 18s)   0.037   |   0.00: 2007 -> two thousand seven (✓) \n",
      "1590000  95% (  80m 8s)   0.048   |   0.00: May 1995 -> may nineteen ninety five (✓) \n",
      "1600000 100% ( 83m 58s)   0.062   |   0.00: 480 m -> four hundred eighty meters (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1600000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.24% (    9624/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992           => nineteen ninety two || one thousand nine hundred ninety two \n",
      "                  131 Idaho Blue Book , 1991 - <SAMPLE> Edition , pg .\n",
      "3,378.4/km²    => three thousand three hundred seventy eight point seven per square kilometers || three thousand three hundred seventy eight point four per square kilometers \n",
      "                  The population density was 8,996.8 people per square mile ( <SAMPLE> ) .\n",
      "XXXV           => twenty five    || thirty five \n",
      "                  The Book Review <SAMPLE> ( 11 ) .\n",
      "121968         => one hundred twenty thousand nine hundred sixty eight || one hundred twenty one thousand nine hundred sixty eight \n",
      "                  Congo Premier League : <SAMPLE> , 1978 , 1979 , 1980 , 1985 , 1987 , 1989 , 1993 , 1994 , 2000 , 2001 , 2006 .\n",
      "01             => o one          || one \n",
      "                  The 2000 - <SAMPLE> season was again tough for Berg .\n",
      "0-374-93239-5  => o sil three seven seven sil four nine one nine nine sil five || o sil three seven four sil nine three two three nine sil five \n",
      "                  ISBN <SAMPLE> , ISBN 978-0-374-93239-8 Culhwch ac Olwen , translated by Lady Charlotte Guest and sub edited by Mary Jones .\n",
      "I              => one            || the first \n",
      "                  An idol of Semargl was present in the pantheon of Great Prince Vladimir <SAMPLE> of Kiev .\n",
      "6/1980         => six thousand one nine hundred || six one thousand nine hundred eightieths \n",
      "                  Erzgebirgische Heimatblatter <SAMPLE> , pp .\n",
      "VI             => the sixth      || six \n",
      "                  There was no Mark <SAMPLE> at Disneyland , as Mark VI are used on the Walt Disney World Monorail System .\n",
      "8108           => eight thousand one hundred eight || eight one o eight \n",
      "                  FA <SAMPLE> - 09- D- 0010 ) VSE Corporation , alexandria virginia ( Contract no\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610000   5% (  3m 49s)   0.066   |   0.00: 1907 -> nineteen o seven (✓) \n",
      "1620000  10% (  7m 35s)   0.059   |   0.00: January 2009 -> january two thousand nine (✓) \n",
      "1630000  15% ( 11m 23s)   0.057   |   0.00: 52 -> fifty two (✓) \n",
      "1640000  20% ( 15m 15s)   0.046   |   0.00: 2015 -> twenty fifteen (✓) \n",
      "1650000  25% (  19m 7s)   0.070   |   0.00: 1:54 -> one fifty four (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/1650000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.38% (    9638/   10000)\n",
      "1660000  30% ( 24m 13s)   0.054   |   0.00: March 20, 2009 -> march twentieth two thousand nine (✓) \n",
      "1670000  35% ( 27m 58s)   0.070   |   0.00: 1980 -> nineteen eighty (✓) \n",
      "1680000  40% ( 32m 21s)   0.065   |   0.00: December 16, 2013 -> december sixteenth twenty thirteen (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34's           => thirty fives   || thirty fours \n",
      "                  West Philadelphia Streetcar Suburb Historic District \" Studio <SAMPLE> Eponymous Trolley , or , A Short History of Route 34 \" .\n",
      "III            => three          || the third \n",
      "                  George <SAMPLE> , however , did not share the same view .\n",
      "140            => one hundred forty || one four o \n",
      "                  Enneasartorite is a very rare mineral with formula Tl 6Pb 32 As 70 S <SAMPLE> .\n",
      "2012           => twenty twelve  || two thousand twelve \n",
      "                  The Orange Drive Miami Beach Music Festival 2011 - <SAMPLE> will be held December 30, 31 , January 1 .\n",
      "411's          => four one       || four elevens \n",
      "                  \" <SAMPLE> WWE Raw Report 11.16 .09 \" .\n",
      "4463           => four thousand four hundred sixty three || four four six three \n",
      "                  Accident description for CCCP- L <SAMPLE> at the Aviation Safety Network .\n",
      "2490           => two thousand four hundred ninety || two four nine o \n",
      "                  Finney has stated since the defeat of both HB 2924 and SB <SAMPLE> within the Tennessee General Assembly that he \" .\n",
      "8 sq           => eight square half || eight square \n",
      "                  Alderney at <SAMPLE> km is now one of the most fortified places in the world .\n",
      "14520          => fourteen thousand two hundred twenty || fourteen thousand five hundred twenty \n",
      "                  ST <SAMPLE> / 05 \" ( PDF ) .\n",
      "1934           => nineteen thirty four || one nine three four \n",
      "                  Botevgrad ( Bulgarian : Б о т е в г р а д ; PRE <SAMPLE> Orhanie ( О р х а н и е ) ; PRE 1866 : Samundzhievo ( С а м у н д ж и е в о ) ) , is a town in western Bulgaria .\n"
     ]
    }
   ],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010000   2% (  3m 23s)   0.061   |   0.00: 1991-10-16 -> the sixteenth of october nineteen ninety one (✓) \n",
      "2020000   5% (  6m 43s)   0.061   |   0.00: 6 -> six (✓) \n",
      "2030000   8% (  10m 0s)   0.072   |   0.03: 149.5/km² -> one hundred forty nine point five per square kilometers (✓) \n",
      "2040000  10% (  13m 7s)   0.066   |   0.00: 1958 -> nineteen fifty eight (✓) \n",
      "2050000  12% ( 16m 18s)   0.046   |   0.00: January 2012 -> january twenty twelve (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/2050000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.21% (    9621/   10000)\n",
      "2060000  15% ( 20m 21s)   0.048   |   0.00: 23 -> twenty three (✓) \n",
      "2070000  18% ( 23m 20s)   0.075   |   0.00: February 1506 -> february fifteen o six (✓) \n",
      "2080000  20% ( 26m 18s)   0.045   |   0.00: 1954 -> nineteen fifty four (✓) \n",
      "2090000  22% ( 29m 12s)   0.073   |   0.00: January 2001 -> january two thousand one (✓) \n",
      "2100000  25% ( 32m 11s)   0.063   |   0.00: 1 -> one (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/2100000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.40% (    9640/   10000)\n",
      "2110000  28% (  36m 5s)   0.055   |   0.10: 96078 -> ninety six thousand seventy eight (✓) \n",
      "2120000  30% (  39m 4s)   0.045   |   0.81: 8/13 -> eight thirteen (✗: eight thirteenths) \n",
      "2130000  32% ( 42m 13s)   0.069   |   0.00: 2009 -> two thousand nine (✓) \n",
      "2140000  35% ( 45m 28s)   0.053   |   0.00: August 1942 -> august nineteen forty two (✓) \n",
      "2150000  38% ( 48m 49s)   0.051   |   0.00: 11 -> eleven (✓) \n",
      "Saved model to data/models/numbers_gen_6_2_layer_output_lstm/2150000_(EncoderRNN/DecoderRNN)\n",
      "Accuracy: 96.60% (    9660/   10000)\n",
      "2160000  40% (  53m 0s)   0.046   |   0.01: 2014 -> twenty fourteen (✓) \n",
      "2170000  42% (  56m 5s)   0.075   |   2.52: 1891350 -> one million eight one ninety one thousand hundred (✗: one eight nine one three five o) \n",
      "2180000  45% (  59m 4s)   0.066   |   0.00: October 2, 1977 -> october second nineteen seventy seven (✓) \n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=400000, print_every=10000, teacher_forcing_ratio=0, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_local_wrong_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
