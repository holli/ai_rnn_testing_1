{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import bcolz\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "print(\"Pytorch: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_org = pd.read_csv('data/en_train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616107</th>\n",
       "      <td>49226</td>\n",
       "      <td>17</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684691</th>\n",
       "      <td>54634</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965529</th>\n",
       "      <td>76612</td>\n",
       "      <td>7</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id    class before after\n",
       "616107        49226        17  LETTERS    NaN   n a\n",
       "684691        54634         1    PLAIN    NaN   NaN\n",
       "965529        76612         7    PLAIN    NaN   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_org[pd.isnull(all_data_org['before'])][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 9918390,  (dropped none rows: 51)\n",
      "Data rows: 9840282,  (dropped rows: 78159)\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data_org.dropna()\n",
    "print(\"Data rows: {},  (dropped none rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data[all_data['class'] != 'VERBATIM']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we dropped VERBATIM class. Thats because it had so many weird characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_sentence_index = all_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS           522\n",
       "CARDINAL       133744\n",
       "DATE           258348\n",
       "DECIMAL          9821\n",
       "DIGIT            5442\n",
       "ELECTRONIC       5162\n",
       "FRACTION         1196\n",
       "LETTERS        152790\n",
       "MEASURE         14783\n",
       "MONEY            6128\n",
       "ORDINAL         12703\n",
       "PLAIN         7353647\n",
       "PUNCT         1880507\n",
       "TELEPHONE        4024\n",
       "TIME             1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_classes = list(all_data.groupby('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_balance_randomize_classes(max_len=10000):\n",
    "    global data_balanced_classes\n",
    "    data_balanced_classes = pd.concat([v.sample(min(max_len, len(v))) for k, v in all_data_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "data_balance_randomize_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS         522\n",
       "CARDINAL      10000\n",
       "DATE          10000\n",
       "DECIMAL        9821\n",
       "DIGIT          5442\n",
       "ELECTRONIC     5162\n",
       "FRACTION       1196\n",
       "LETTERS       10000\n",
       "MEASURE       10000\n",
       "MONEY          6128\n",
       "ORDINAL       10000\n",
       "PLAIN         10000\n",
       "PUNCT         10000\n",
       "TELEPHONE      4024\n",
       "TIME           1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>952359</th>\n",
       "      <td>76198</td>\n",
       "      <td>4</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>Soaps.com</td>\n",
       "      <td>s o a p s dot c o m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910701</th>\n",
       "      <td>302950</td>\n",
       "      <td>0</td>\n",
       "      <td>MEASURE</td>\n",
       "      <td>20%</td>\n",
       "      <td>twenty percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272976</th>\n",
       "      <td>22142</td>\n",
       "      <td>19</td>\n",
       "      <td>MEASURE</td>\n",
       "      <td>8.9 km2</td>\n",
       "      <td>eight point nine square kilometers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944877</th>\n",
       "      <td>75620</td>\n",
       "      <td>22</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526545</th>\n",
       "      <td>500200</td>\n",
       "      <td>5</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909802</th>\n",
       "      <td>226855</td>\n",
       "      <td>6</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>104th</td>\n",
       "      <td>one hundred fourth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926732</th>\n",
       "      <td>228123</td>\n",
       "      <td>9</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004613</th>\n",
       "      <td>535973</td>\n",
       "      <td>5</td>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>U.S. 35</td>\n",
       "      <td>u s thirty five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069200</th>\n",
       "      <td>238958</td>\n",
       "      <td>8</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1998</td>\n",
       "      <td>nineteen ninety eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532463</th>\n",
       "      <td>198031</td>\n",
       "      <td>6</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>74,139</td>\n",
       "      <td>seventy four thousand one hundred thirty nine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id       class     before  \\\n",
       "952359         76198         4  ELECTRONIC  Soaps.com   \n",
       "3910701       302950         0     MEASURE        20%   \n",
       "272976         22142        19     MEASURE    8.9 km2   \n",
       "944877         75620        22       PUNCT          .   \n",
       "6526545       500200         5       PLAIN        and   \n",
       "2909802       226855         6     ORDINAL      104th   \n",
       "2926732       228123         9     LETTERS          U   \n",
       "7004613       535973         5     ADDRESS    U.S. 35   \n",
       "3069200       238958         8        DATE       1998   \n",
       "2532463       198031         6    CARDINAL     74,139   \n",
       "\n",
       "                                                 after  \n",
       "952359                             s o a p s dot c o m  \n",
       "3910701                                 twenty percent  \n",
       "272976              eight point nine square kilometers  \n",
       "944877                                               .  \n",
       "6526545                                            and  \n",
       "2909802                             one hundred fourth  \n",
       "2926732                                              U  \n",
       "7004613                                u s thirty five  \n",
       "3069200                          nineteen ninety eight  \n",
       "2532463  seventy four thousand one hundred thirty nine  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(name):\n",
    "    with open(name, 'r') as f: lines = [line.split() for line in f]\n",
    "    words = [d[0] for d in lines]\n",
    "    vecs = np.stack(np.array(d[1:], dtype=np.float32) for d in lines)\n",
    "    wordidx = {o:i for i,o in enumerate(words)}\n",
    "    return vecs, words, wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vecs, wv_words, wv_idx = load_glove('/home/ohu/koodi/data/glove_wordvec/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', \"'s\", 'asdf', '-', 'testaaa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_tokeniser(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' - ')\n",
    "    sent = re_punc.sub(r\" \\1 \", sent)\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()\n",
    "simple_tokeniser(\"asdf's   asdf   -testaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arr = [simple_tokeniser(s_)[0] for s_ in list(all_data.sample(1000)['before'])]\n",
    "[s in wv_idx for s in arr].count(True) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECTRONIC : :: -> :: : (1, 23, 50) <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "def get_random_sample():\n",
    "    sample_row = data_balanced_classes.iloc[random.randint(1, len(data_balanced_classes)-1)]\n",
    "    sentence_id = sample_row['class']\n",
    "\n",
    "    #rows = all_data[all_data['sentence_id']==sample_row['sentence_id']]\n",
    "    rows = all_data_sentence_index.loc[sample_row['sentence_id']]\n",
    "    befores = rows.before.values\n",
    "        \n",
    "    token_id_idx = list(rows['token_id']).index(sample_row['token_id'])\n",
    "    befores[token_id_idx] = '*****'\n",
    "    str_list = simple_tokeniser(' '.join(befores))\n",
    "    \n",
    "    word_vect = np.zeros((1, len(str_list), wv_vecs.shape[1]), dtype=np.float32)\n",
    "    # var = np.zeros((1, len(str_list), wv_vecs.shape[1]+1))\n",
    "    for i, w in enumerate(str_list):\n",
    "        if w=='*****':\n",
    "            word_vect[0][i] = np.zeros((1, wv_vecs.shape[1]))\n",
    "        else:\n",
    "            try:\n",
    "                word_vect[0][i] = wv_vecs[wv_idx[w]]\n",
    "            except KeyError:\n",
    "                word_vect[0][i] = np.random.rand(1, wv_vecs.shape[1])\n",
    "    return sample_row['before'], sample_row['after'], sample_row['class'], word_vect\n",
    "            \n",
    "# get_random_sample()\n",
    "s_bef, s_aft, s_class, s_word_v = get_random_sample()\n",
    "print(s_class, ':', s_bef, '->', s_aft, ':', s_word_v.shape, type(s_word_v[0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 µs ± 5.97 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories and Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAIN' 'PUNCT' 'DATE' 'LETTERS' 'CARDINAL' 'DECIMAL' 'MEASURE' 'MONEY'\n",
      " 'ORDINAL' 'TIME' 'ELECTRONIC' 'DIGIT' 'FRACTION' 'TELEPHONE' 'ADDRESS']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "categories_all = all_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "print(len(categories_all))\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~¡£¥ª«²³µº»¼½¾¿éɒʻˈΩμ—€⅓⅔⅛⅝⅞\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "letters_all = sorted(list(set(''.join(all_data['before']))))\n",
    "letters_index = dict((c, i) for i, c in enumerate(letters_all))\n",
    "letters_n = len(letters_all)\n",
    "print(''.join(letters_all))\n",
    "print(len(letters_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 112])\n"
     ]
    }
   ],
   "source": [
    "def string_to_tensor(line):\n",
    "    tensor = torch.zeros(1, len(line), letters_n)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[0, li, letters_index[letter]] = 1\n",
    "    return tensor\n",
    "print(string_to_tensor('wordup').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vect size: (1, 32, 50) . String vector size: torch.Size([1, 2, 112])\n",
      "Output: torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "class RNN_WORDS_CHARS_CLASS(nn.Module):\n",
    "    def __init__(self, wordvect_size, letters_size, hidden_size, output_size,\n",
    "                 words_layers=1, chars_layers=1, words_dropout=0, chars_dropout=0):\n",
    "        super(RNN_WORDS_CHARS_CLASS, self).__init__()\n",
    "\n",
    "        self.train_iterations = 0\n",
    "        self.train_history = []\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(wordvect_size, hidden_size // 2, words_layers,\n",
    "                                 dropout=words_dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(letters_size, hidden_size // 2, chars_layers,\n",
    "                                 dropout=chars_dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        #self.lin_1 = nn.Linear(hidden_size*2, 1024)\n",
    "        self.lin_output = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #output = self.lin_1(output)\n",
    "        output = self.lin_output(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.hidden_size // 2))\n",
    "        if use_cuda:\n",
    "            var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "            var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))\n",
    "\n",
    "use_cuda = False\n",
    "s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "s_string = string_to_tensor(s_bef)\n",
    "model_tmp = RNN_WORDS_CHARS_CLASS(wordvect_size=s_word_vs.shape[-1], letters_size=letters_n,\n",
    "                                  hidden_size=128, output_size=len(categories_all),\n",
    "                                  words_layers=2, chars_layers=2)\n",
    "print('Word vect size:', s_word_vs.shape, '. String vector size:', s_string.size())\n",
    "output = model_tmp(Variable(torch.from_numpy(s_word_vs)), Variable(s_string))\n",
    "print('Output:', output.size())\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp = use_cuda\n",
    "use_cuda = True\n",
    "model_tmp.cuda()\n",
    "output = model_tmp(Variable(torch.from_numpy(s_word_vs)).cuda(), Variable(s_string).cuda())\n",
    "use_cuda = tmp\n",
    "type(output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FRACTION', 12)\n"
     ]
    }
   ],
   "source": [
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1)\n",
    "    category_i = top_i[0][0]\n",
    "    return categories_all[category_i], category_i\n",
    "\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_accuracy(model, n_sample=10000):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    for iteration in range(n_sample):\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "        s_string = Variable(string_to_tensor(s_bef))\n",
    "        s_word_vs = Variable(torch.from_numpy(s_word_vs))\n",
    "        if use_cuda:\n",
    "            s_word_vs = s_word_vs.cuda()\n",
    "            s_string = s_string.cuda()\n",
    "        output = model(s_word_vs, s_string)\n",
    "        if s_class == category_from_output(output)[0]:\n",
    "            n_correct += 1\n",
    "\n",
    "    print(\"Accuracy: {:>4.2%} ({:>8d}/{:>8d})\".format(\n",
    "            n_correct/n_sample, n_correct, n_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "test_model_accuracy(model_tmp.cuda(), n_sample=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, category, word_vectors, string, loss_function, optimizer):\n",
    "    category_tensor = Variable(torch.LongTensor([categories_index[category]]))\n",
    "    word_vectors = Variable(torch.from_numpy(word_vectors))\n",
    "    string = Variable(string_to_tensor(string))\n",
    "    if use_cuda:\n",
    "        category_tensor = category_tensor.cuda()\n",
    "        word_vectors = word_vectors.cuda()\n",
    "        string = string.cuda()\n",
    "\n",
    "    output = model(word_vectors, string)\n",
    "    loss = loss_function(output, category_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "train(model, s_class, s_word_vs, s_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, lr=0.001, print_every=20000, plot_every=1000):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        model.train_iterations += 1\n",
    "        s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "\n",
    "        output, loss = train(model, s_class, s_word_vs, s_bef, loss_function, optimizer)\n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            guess, guess_i = category_from_output(output)\n",
    "            correct = '✓' if guess == s_class else \"✗ ({})\".format(s_class)\n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>7.3f}   | {:>6.2f}: {} -> {} {}\".format(\n",
    "                model.train_iterations, iteration/n_iters, timeSince(start),\n",
    "                current_loss/current_loss_iter, loss,\n",
    "                s_bef, guess, correct ))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            model.train_history.append((current_loss / plot_every, lr))\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "        if model.train_iterations % 50000 == 0:\n",
    "            data_balance_randomize_classes()\n",
    "    \n",
    "    test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN_WORDS_CHARS_CLASS(wordvect_size=s_word_vs.shape[-1], letters_size=letters_n,\n",
    "                              hidden_size=128, output_size=len(categories_all),\n",
    "                              words_layers=2, chars_layers=2, words_dropout=0.2, chars_dropout=0.2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000  20% (   3m 5s)   0.182   |   0.03: 16 -> CARDINAL ✓\n",
      " 40000  40% (   6m 5s)   0.157   |   0.00: ) -> PUNCT ✓\n",
      " 60000  60% (   9m 7s)   0.147   |   0.20: 15 -> DIGIT ✓\n",
      " 80000  80% (  12m 9s)   0.111   |   0.00: DT -> LETTERS ✓\n",
      "100000 100% (  15m 9s)   0.092   |   0.00: , -> PUNCT ✓\n",
      "Accuracy: 96.44% (    9644/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000  10% (   3m 1s)   0.100   |   0.00: 24 February 2016 -> DATE ✓\n",
      "140000  20% (   6m 4s)   0.086   |   0.00: June 13 -> DATE ✓\n",
      "160000  30% (   9m 6s)   0.081   |   0.03: A29091 -> ADDRESS ✓\n",
      "180000  40% (  12m 8s)   0.069   |   0.16: 1  -> CARDINAL ✓\n",
      "200000  50% (  15m 9s)   0.060   |   0.00: 2012 -> DATE ✓\n",
      "220000  60% (  18m 9s)   0.069   |   0.00: 897.8/km2 -> MEASURE ✓\n",
      "240000  70% (  21m 8s)   0.091   |   0.07: 4 -> CARDINAL ✓\n",
      "260000  80% ( 24m 12s)   0.071   |   0.02: 9 -> CARDINAL ✓\n",
      "280000  90% ( 27m 17s)   0.081   |   0.01: 2004 -> DATE ✓\n",
      "300000 100% ( 30m 20s)   0.078   |   0.00: 4th -> ORDINAL ✓\n",
      "Accuracy: 97.98% (    9798/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000  10% (  2m 56s)   0.086   |   0.00: EIHL -> LETTERS ✓\n",
      "340000  20% (  5m 54s)   0.053   |   0.00: HPL -> LETTERS ✓\n",
      "360000  30% (  9m 10s)   0.068   |   0.00: B.A. -> LETTERS ✓\n",
      "380000  40% (  12m 7s)   0.080   |   0.00: PDF -> LETTERS ✓\n",
      "400000  50% ( 14m 59s)   0.048   |   0.00: 17th -> ORDINAL ✓\n",
      "420000  60% ( 17m 51s)   0.060   |   0.00: 9.24 -> DECIMAL ✓\n",
      "440000  70% ( 20m 47s)   0.058   |   0.00: D. -> LETTERS ✓\n",
      "460000  80% (  24m 0s)   0.031   |   0.00: 17:00 -> TIME ✓\n",
      "480000  90% ( 27m 19s)   0.070   |   0.00: .950 -> DECIMAL ✓\n",
      "500000 100% ( 30m 23s)   0.051   |   0.00: 20.153 -> DECIMAL ✓\n",
      "Accuracy: 97.91% (    9791/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000  10% (  2m 59s)   0.064   |   0.01: FCT -> LETTERS ✓\n",
      "740000  20% (  5m 59s)   0.047   |   0.00: 102 -> CARDINAL ✓\n",
      "760000  30% (  8m 59s)   0.056   |   0.00: tightly -> PLAIN ✓\n",
      "780000  40% (  12m 3s)   0.027   |   0.00: 0-553-48638-1 -> TELEPHONE ✓\n",
      "800000  50% (  15m 3s)   0.082   |   0.00: 09 -> DIGIT ✓\n",
      "820000  60% (  18m 5s)   0.043   |   0.00: 34th -> ORDINAL ✓\n",
      "840000  70% (  21m 5s)   0.041   |   0.00: 128.5/km² -> MEASURE ✓\n",
      "860000  80% (  24m 7s)   0.071   |   0.00: 36th -> ORDINAL ✓\n",
      "880000  90% (  27m 7s)   0.046   |   0.00: . -> PUNCT ✓\n",
      "900000 100% ( 30m 15s)   0.036   |   0.00: 56.875 -> DECIMAL ✓\n",
      "Accuracy: 98.22% (    9822/   10000)\n"
     ]
    }
   ],
   "source": [
    "train_iterations(n_iters=200000, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(n_iters=400000, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.93% (    9793/   10000)\n"
     ]
    }
   ],
   "source": [
    "data_balance_randomize_classes(50000)\n",
    "test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.10% (    9710/   10000)\n"
     ]
    }
   ],
   "source": [
    "data_balance_randomize_classes(50000)\n",
    "test_model_accuracy(model, n_sample=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(len(categories_all), len(categories_all))\n",
    "n_confusion = 100000\n",
    "data_balance_randomize_classes(50000)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    s_bef, s_aft, s_class, s_word_vs = get_random_sample()\n",
    "    word_vectors = Variable(torch.from_numpy(s_word_vs))\n",
    "    string = Variable(string_to_tensor(s_bef))\n",
    "    if use_cuda:\n",
    "        word_vectors = word_vectors.cuda()\n",
    "        string = string.cuda()\n",
    "    output = model(word_vectors, string)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = categories_index[s_class]\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(len(categories_all)):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + list(categories_all), rotation=90)\n",
    "ax.set_yticklabels([''] + list(categories_all))\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_2 = confusion.clone().numpy()\n",
    "for i in range(len(confusion_2)):\n",
    "    confusion_2[i,i]=0\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion_2)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + list(categories_all), rotation=90)\n",
    "ax.set_yticklabels([''] + list(categories_all))\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = [arr[0] for arr in model.train_history[10:]]\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/models/category_rnn_bi_2_layer_dropout_700000'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = 'data/models/category_rnn_bi_2_layer_dropout_' + str(model.train_iterations)\n",
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
