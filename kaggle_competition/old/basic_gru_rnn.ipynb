{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "print(\"Pytorch: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if False:\n",
    "    import keras\n",
    "    from keras import backend as K\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.utils.data_utils import get_file\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "    from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "    from keras.layers.pooling import GlobalAveragePooling2D\n",
    "    from keras.optimizers import SGD, RMSprop, Adam\n",
    "    from keras.preprocessing import image\n",
    "    print(\"Keras {}, TensorFlow {}\".format(keras.__version__, keras.backend.tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data to train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_org = pd.read_csv('data/en_train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616107</th>\n",
       "      <td>49226</td>\n",
       "      <td>17</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684691</th>\n",
       "      <td>54634</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965529</th>\n",
       "      <td>76612</td>\n",
       "      <td>7</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id    class before after\n",
       "616107        49226        17  LETTERS    NaN   n a\n",
       "684691        54634         1    PLAIN    NaN   NaN\n",
       "965529        76612         7    PLAIN    NaN   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_org[pd.isnull(all_data_org['before'])][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 9918390,  (dropped none rows: 51)\n",
      "Data rows: 9840282,  (dropped rows: 78159)\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data_org.dropna()\n",
    "print(\"Data rows: {},  (dropped none rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data[all_data['class'] != 'VERBATIM']\n",
    "print(\"Data rows: {},  (dropped rows: {})\".format(len(all_data), len(all_data_org)-len(all_data)))\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note we dropped VERBATIM class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sentences = all_data.groupby('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_indexes_grouped = random.sample(list(grouped_sentences.indices.values()), int(len(grouped_sentences)*0.3))\n",
    "validation_indexes = [item for sublist in validation_indexes_grouped for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = all_data.loc[validation_indexes]\n",
    "train_data = all_data.loc[~all_data.index.isin(validation_indexes)]\n",
    "train_data = train_data.sort_values(['sentence_id','token_id'])\n",
    "validation_data = validation_data.sort_values(['sentence_id','token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes: all_data 9840282, train_data 6887505 (0.6999), validation_data 2952777 (0.3001)\n",
      "Match : True\n"
     ]
    }
   ],
   "source": [
    "print(\"sizes: all_data {}, train_data {} ({:.4f}), validation_data {} ({:.4f})\".format(\n",
    "    len(all_data), len(train_data), len(train_data)/len(all_data),\n",
    "    len(validation_data), len(validation_data)/len(all_data)))\n",
    "print(\"Match : {}\".format(len(all_data)==(len(validation_data)+len(train_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Brillantaisia</td>\n",
       "      <td>Brillantaisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>genus</td>\n",
       "      <td>genus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id  class         before          after\n",
       "0            0         0  PLAIN  Brillantaisia  Brillantaisia\n",
       "1            0         1  PLAIN             is             is\n",
       "2            0         2  PLAIN              a              a\n",
       "3            0         3  PLAIN          genus          genus\n",
       "4            0         4  PLAIN             of             of"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DATE</td>\n",
       "      <td>2006</td>\n",
       "      <td>two thousand six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>IUCN</td>\n",
       "      <td>i u c n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Red</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>List</td>\n",
       "      <td>List</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id  token_id    class before             after\n",
       "10          1.0       0.0     DATE   2006  two thousand six\n",
       "11          1.0       1.0  LETTERS   IUCN           i u c n\n",
       "12          1.0       2.0    PLAIN    Red               Red\n",
       "13          1.0       3.0    PLAIN   List              List\n",
       "14          1.0       4.0    PLAIN     of                of"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = testing_data = validation_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = testing_data.groupby(['before', 'after']).size()\n",
    "d = d.reset_index().sort_values(0, ascending=False)\n",
    "d = d.loc[d['before'].drop_duplicates(keep='first').index]\n",
    "d = d.loc[d['before'] != d['after']]\n",
    "d = d.set_index('before')['after'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(list(all_data['before']) + list(all_data['after'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words_indices = dict((c, i) for i, c in enumerate(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_x = np.zeros(len(all_data['before']), dtype=np.int)\n",
    "for idx, word in enumerate(list(all_data['after'])):\n",
    "    train_x[idx] = all_words_indices[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.zeros(len(all_data['after']), dtype=np.int)\n",
    "for idx, word in enumerate(list(all_data['after'])):\n",
    "    train_y[idx] = all_words_indices[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More balanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS           522\n",
       "CARDINAL       133744\n",
       "DATE           258348\n",
       "DECIMAL          9821\n",
       "DIGIT            5442\n",
       "ELECTRONIC       5162\n",
       "FRACTION         1196\n",
       "LETTERS        152790\n",
       "MEASURE         14783\n",
       "MONEY            6128\n",
       "ORDINAL         12703\n",
       "PLAIN         7353647\n",
       "PUNCT         1880507\n",
       "TELEPHONE        4024\n",
       "TIME             1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_classes = list(all_data.groupby('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9145012</th>\n",
       "      <td>696045</td>\n",
       "      <td>8</td>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>C2</td>\n",
       "      <td>c two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902879</th>\n",
       "      <td>302356</td>\n",
       "      <td>3</td>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>SR-263</td>\n",
       "      <td>s r two sixty three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018111</th>\n",
       "      <td>311149</td>\n",
       "      <td>9</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>96</td>\n",
       "      <td>ninety six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497885</th>\n",
       "      <td>722622</td>\n",
       "      <td>11</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>2977</td>\n",
       "      <td>two thousand nine hundred seventy seven</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id     class  before  \\\n",
       "9145012       696045         8   ADDRESS     C2    \n",
       "3902879       302356         3   ADDRESS  SR-263   \n",
       "4018111       311149         9  CARDINAL      96   \n",
       "9497885       722622        11  CARDINAL    2977   \n",
       "\n",
       "                                           after  \n",
       "9145012                                    c two  \n",
       "3902879                      s r two sixty three  \n",
       "4018111                               ninety six  \n",
       "9497885  two thousand nine hundred seventy seven  "
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_classes[0][1].sample(2).append(all_data_classes[1][1].sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced_classes = pd.concat([v.sample(min(50000, len(v))) for k, v in all_data_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ADDRESS         522\n",
       "CARDINAL      50000\n",
       "DATE          50000\n",
       "DECIMAL        9821\n",
       "DIGIT          5442\n",
       "ELECTRONIC     5162\n",
       "FRACTION       1196\n",
       "LETTERS       50000\n",
       "MEASURE       14783\n",
       "MONEY          6128\n",
       "ORDINAL       12703\n",
       "PLAIN         50000\n",
       "PUNCT         50000\n",
       "TELEPHONE      4024\n",
       "TIME           1465\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.groupby(\"class\")[\"class\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7509636</th>\n",
       "      <td>573952</td>\n",
       "      <td>10</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7475877</th>\n",
       "      <td>571397</td>\n",
       "      <td>4</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9801958</th>\n",
       "      <td>745193</td>\n",
       "      <td>13</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1921</td>\n",
       "      <td>nineteen twenty one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668815</th>\n",
       "      <td>435882</td>\n",
       "      <td>0</td>\n",
       "      <td>DIGIT</td>\n",
       "      <td>08</td>\n",
       "      <td>o eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518674</th>\n",
       "      <td>41887</td>\n",
       "      <td>1</td>\n",
       "      <td>TELEPHONE</td>\n",
       "      <td>0-520-01660-2</td>\n",
       "      <td>o sil five two o sil o one six six o sil two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547876</th>\n",
       "      <td>275387</td>\n",
       "      <td>7</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1999</td>\n",
       "      <td>nineteen ninety nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588436</th>\n",
       "      <td>125430</td>\n",
       "      <td>3</td>\n",
       "      <td>DIGIT</td>\n",
       "      <td>2948</td>\n",
       "      <td>two nine four eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571317</th>\n",
       "      <td>578631</td>\n",
       "      <td>8</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464662</th>\n",
       "      <td>645479</td>\n",
       "      <td>4</td>\n",
       "      <td>DATE</td>\n",
       "      <td>1948</td>\n",
       "      <td>nineteen forty eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620085</th>\n",
       "      <td>127854</td>\n",
       "      <td>3</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>Rs 1,432 cr</td>\n",
       "      <td>one thousand four hundred thirty two crore rupees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id      class         before  \\\n",
       "7509636       573952        10      PLAIN            the   \n",
       "7475877       571397         4      PLAIN             of   \n",
       "9801958       745193        13       DATE           1921   \n",
       "5668815       435882         0      DIGIT             08   \n",
       "518674         41887         1  TELEPHONE  0-520-01660-2   \n",
       "3547876       275387         7       DATE           1999   \n",
       "1588436       125430         3      DIGIT           2948   \n",
       "7571317       578631         8      PUNCT              .   \n",
       "8464662       645479         4       DATE           1948   \n",
       "1620085       127854         3      MONEY    Rs 1,432 cr   \n",
       "\n",
       "                                                     after  \n",
       "7509636                                                the  \n",
       "7475877                                                 of  \n",
       "9801958                                nineteen twenty one  \n",
       "5668815                                            o eight  \n",
       "518674        o sil five two o sil o one six six o sil two  \n",
       "3547876                               nineteen ninety nine  \n",
       "1588436                                two nine four eight  \n",
       "7571317                                                  .  \n",
       "8464662                               nineteen forty eight  \n",
       "1620085  one thousand four hundred thirty two crore rupees  "
      ]
     },
     "execution_count": 993,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_classes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting string class from characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAIN' 'PUNCT' 'DATE' 'LETTERS' 'CARDINAL' 'MEASURE' 'ORDINAL' 'DECIMAL'\n",
      " 'TIME' 'DIGIT' 'MONEY' 'ELECTRONIC' 'TELEPHONE' 'FRACTION' 'ADDRESS']\n"
     ]
    }
   ],
   "source": [
    "categories_all = train_data[\"class\"].unique()\n",
    "print(categories_all)\n",
    "len(categories_all)\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%&'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|~¡£¥ª«²³µº»¼½¾¿éɒʻˈΩμ—€⅓⅔⅛⅝⅞\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "letters_all = sorted(list(set(''.join(all_data['before']))))\n",
    "letters_index = dict((c, i) for i, c in enumerate(letters_all))\n",
    "letters_n = len(letters_all)\n",
    "print(''.join(letters_all))\n",
    "print(len(letters_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 112])\n"
     ]
    }
   ],
   "source": [
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, letters_n)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letters_index[letter]] = 1\n",
    "    return tensor\n",
    "print(lineToTensor('wordup').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: True\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers)\n",
    "        #self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.lin_1 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word, hidden):\n",
    "        all_outputs, hidden = self.gru(word, hidden)\n",
    "        #output = hidden\n",
    "        #output = output.view(1, -1)\n",
    "        output = all_outputs[-1]\n",
    "        output = self.lin_1(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        var = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        #var = Variable(torch.zeros(1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            var = var.cuda()\n",
    "        return var\n",
    "\n",
    "\n",
    "rnn = RNN(input_size=letters_n, hidden_size=128, output_size=len(categories_all))\n",
    "print(\"Using cuda: {}\".format(use_cuda))\n",
    "if use_cuda:\n",
    "    rnn = rnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 112])"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineToTensor('wordup').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(4, 3, num_layers=2, batch_first=True)  # Input dim is 3, output dim is 3\n",
    "inputs = [Variable(torch.randn((1, 4))) for _ in range(5)]  # make a sequence of length 5\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "#hidden = Variable(torch.randn(2, 1, 3))\n",
    "arr = [0.0489, -0.3077, -0.0098]\n",
    "hidden = Variable(torch.from_numpy(np.array([arr, arr])).float()).view(2, 1, -1)\n",
    "res, hidden = rnn(inputs, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#rnn(Variable(torch.zeros(4, 1, 112)), Variable(torch.zeros(1, 1, 128)))\n",
    "rnn(Variable(torch.zeros(4, 1, 112)), rnn.initHidden())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-2.6633 -2.6343 -2.6845 -2.6679 -2.6163 -2.7538 -2.6775 -2.6830 -2.7373 -2.7680\n",
      "\n",
      "Columns 10 to 14 \n",
      "-2.8636 -2.7344 -2.8343 -2.7743 -2.5727\n",
      "[torch.cuda.FloatTensor of size 1x15 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_info = Variable(lineToTensor('wordup'))\n",
    "# hidden = Variable(torch.zeros(1, n_hidden))\n",
    "\n",
    "if use_cuda:\n",
    "    output = rnn(input_info.cuda(), rnn.initHidden())\n",
    "else:\n",
    "    output = rnn(input_info, rnn.initHidden())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TELEPHONE', 12)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return categories_all[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_balanced_classes.sample(frac=1)\n",
    "train_x = list(data['before'])\n",
    "train_y = list(data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = PUNCT / line = .      ( torch.Size([1])\n",
      "category = LETTERS / line = Ba'ath      ( torch.Size([1])\n",
      "category = PLAIN / line = also      ( torch.Size([1])\n",
      "category = DATE / line = 2012      ( torch.Size([1])\n",
      "category = PLAIN / line = Lithuanian      ( torch.Size([1])\n",
      "category = CARDINAL / line = 202      ( torch.Size([1])\n",
      "category = DATE / line = April 30th, 1918      ( torch.Size([1])\n",
      "category = PLAIN / line = which      ( torch.Size([1])\n",
      "category = DATE / line = 2003      ( torch.Size([1])\n",
      "category = DATE / line = October 6, 2014      ( torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "def randomTrainingExample():\n",
    "    #sample = data_balanced_classes.sample(1)\n",
    "    #category = sample['class'].item()\n",
    "    #line = sample['before'].item()\n",
    "    i = random.randint(0, len(train_x)-1)\n",
    "    category = train_y[i]\n",
    "    line = train_x[i]\n",
    "    category_tensor = Variable(torch.LongTensor([categories_index[category]]))\n",
    "    line_tensor = Variable(lineToTensor(line))\n",
    "    if use_cuda:\n",
    "        category_tensor = category_tensor.cuda()\n",
    "        line_tensor = line_tensor.cuda()\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line,\n",
    "          '     (', category_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTrainingExample()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 µs ± 66.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(category_tensor, line_tensor, learning_rate=0.001):\n",
    "    #hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    #for i in range(line_tensor.size()[0]):\n",
    "    #    output, hidden = rnn(line_tensor[i], hidden)\n",
    "    output = rnn(line_tensor, rnn.initHidden())\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(n_iters=100000, start_iter=0, print_every=5000, plot_every=1000):\n",
    "    rrn.train()\n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "\n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "        output, loss = train(category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "        current_loss_iter += 1\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iteration % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else \"✗ ({})\".format(category)\n",
    "            print(\"{:>6d} {:>4.0%} ({:>8}) {:>6.2f}   | {:>6.2f}: {} -> {} {}\".format(\n",
    "                (iteration+start_iter), iteration/n_iters, timeSince(start),\n",
    "                current_loss/current_loss_iter, loss,\n",
    "                line, guess, correct ))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iteration % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "            current_loss_iter = 0\n",
    "            \n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5000   5% (  0m 23s)   2.00   |   2.10: : -> DATE ✗ (PUNCT)\n",
      " 10000  10% (  0m 47s)   1.70   |   1.93: 2, -> PLAIN ✗ (CARDINAL)\n",
      " 15000  15% (  1m 11s)   1.53   |   4.61: 10th -> DATE ✗ (ORDINAL)\n",
      " 20000  20% (  1m 35s)   1.37   |   1.31: UTA -> LETTERS ✓\n",
      " 25000  25% (  1m 54s)   1.17   |   1.11: HDP -> LETTERS ✓\n",
      " 30000  30% (  2m 14s)   1.03   |   2.54: 80,000 -> DATE ✗ (CARDINAL)\n",
      " 35000  35% (  2m 33s)   0.88   |   4.68: 08 -> CARDINAL ✗ (DIGIT)\n",
      " 40000  40% (  2m 52s)   0.79   |   0.69: EZH -> LETTERS ✓\n",
      " 45000  45% (  3m 12s)   0.71   |   1.96: x -> PUNCT ✗ (LETTERS)\n",
      " 50000  50% (  3m 31s)   0.71   |   0.05: firefighter -> PLAIN ✓\n",
      " 55000  55% (  3m 50s)   0.61   |   0.25: the -> PLAIN ✓\n",
      " 60000  60% (  4m 10s)   0.56   |   0.42: 1 -> CARDINAL ✓\n",
      " 65000  65% (  4m 30s)   0.54   |   0.22: with -> PLAIN ✓\n",
      " 70000  70% (  4m 52s)   0.63   |   0.08: , -> PUNCT ✓\n",
      " 75000  75% (  5m 13s)   0.45   |   0.33: : -> PUNCT ✓\n",
      " 80000  80% (  5m 31s)   0.52   |   5.98: Onet.pl -> PLAIN ✗ (ELECTRONIC)\n",
      " 85000  85% (  5m 51s)   0.47   |   2.92: X -> PUNCT ✗ (PLAIN)\n",
      " 90000  90% (  6m 10s)   0.45   |   0.22: of -> PLAIN ✓\n",
      " 95000  95% (  6m 30s)   0.41   |   0.06: . -> PUNCT ✓\n",
      "100000 100% (  6m 52s)   0.40   |   2.91: 120,460 -> DATE ✗ (CARDINAL)\n"
     ]
    }
   ],
   "source": [
    "all_losses = train_iterations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "  5000   2% (   0m 8s)   2.05   |   1.42: 1901 -> DATE ✓\n",
      " 10000   3% (  0m 16s)   1.71   |   1.08: declared -> DATE ✗ (PLAIN)\n",
      " 15000   5% (  0m 24s)   1.61   |   4.56: 55th -> PLAIN ✗ (ORDINAL)\n",
      " 20000   7% (  0m 33s)   1.35   |   0.44: June 9, 2014 -> DATE ✓\n",
      " 25000   8% (  0m 41s)   1.26   |   1.44: 124 -> CARDINAL ✓\n",
      " 30000  10% (  0m 49s)   1.05   |   0.41: . -> PUNCT ✓\n",
      " 35000  12% (  0m 57s)   0.95   |   3.80: 1st -> LETTERS ✗ (ORDINAL)\n",
      " 40000  13% (   1m 6s)   0.80   |   0.57: ( -> PUNCT ✓\n",
      " 45000  15% (  1m 14s)   0.78   |   0.68: SB -> LETTERS ✓\n",
      " 50000  17% (  1m 23s)   0.70   |   0.69: 55 -> CARDINAL ✓\n",
      " 55000  18% (  1m 31s)   0.62   |   0.27: 1997 -> DATE ✓\n",
      " 60000  20% (  1m 39s)   0.63   |   0.06: August 3, 2014 -> DATE ✓\n",
      " 65000  22% (  1m 47s)   0.53   |   0.11: . -> PUNCT ✓\n",
      " 70000  23% (  1m 56s)   0.52   |   0.42: 615 -> CARDINAL ✓\n",
      " 75000  25% (   2m 5s)   0.47   |   0.02: L.A. -> LETTERS ✓\n",
      " 80000  27% (  2m 14s)   0.45   |   0.28: was -> PLAIN ✓\n",
      " 85000  28% (  2m 23s)   0.51   |   0.11: 99 -> CARDINAL ✓\n",
      " 90000  30% (  2m 32s)   0.41   |   0.07: . -> PUNCT ✓\n",
      " 95000  32% (  2m 40s)   0.46   |   0.20: ( -> PUNCT ✓\n",
      "100000  33% (  2m 49s)   0.38   |   1.50: 37th -> LETTERS ✗ (ORDINAL)\n",
      "105000  35% (  2m 57s)   0.33   |   0.01: Action -> PLAIN ✓\n",
      "110000  37% (   3m 6s)   0.40   |   0.03: , -> PUNCT ✓\n",
      "115000  38% (  3m 14s)   0.37   |   0.36: 3 -> CARDINAL ✓\n",
      "120000  40% (  3m 23s)   0.38   |   0.05: KMAJ -> LETTERS ✓\n",
      "125000  42% (  3m 31s)   0.34   |   0.01: inuii -> PLAIN ✓\n",
      "130000  43% (  3m 40s)   0.30   |   0.19: MGM's -> LETTERS ✓\n",
      "135000  45% (  3m 48s)   0.34   |   0.01: that -> PLAIN ✓\n",
      "140000  47% (  3m 57s)   0.31   |   0.08: CIO -> LETTERS ✓\n",
      "145000  48% (   4m 6s)   0.35   |   0.02: the -> PLAIN ✓\n",
      "150000  50% (  4m 14s)   0.31   |   0.54: ! -> PUNCT ✓\n",
      "155000  52% (  4m 22s)   0.31   |   0.14: 1990s -> DATE ✓\n",
      "160000  53% (  4m 31s)   0.30   |   0.07: 130 -> CARDINAL ✓\n",
      "165000  55% (  4m 39s)   0.34   |   0.82: 1st -> ORDINAL ✓\n",
      "170000  57% (  4m 48s)   0.31   |   0.00: California -> PLAIN ✓\n",
      "175000  58% (  4m 56s)   0.31   |   0.36: 107 -> CARDINAL ✓\n",
      "180000  60% (   5m 5s)   0.32   |   0.21: 6 -> CARDINAL ✓\n",
      "185000  62% (  5m 14s)   0.27   |   0.04: FC -> LETTERS ✓\n",
      "190000  63% (  5m 22s)   0.25   |   0.04: 1919 -> DATE ✓\n",
      "195000  65% (  5m 31s)   0.31   |   0.02: F. -> LETTERS ✓\n",
      "200000  67% (  5m 39s)   0.33   |   0.06: influenced -> PLAIN ✓\n",
      "205000  68% (  5m 48s)   0.25   |   0.02: . -> PUNCT ✓\n",
      "210000  70% (  5m 56s)   0.26   |   0.03: 1917 -> DATE ✓\n",
      "215000  72% (   6m 5s)   0.23   |   0.02: . -> PUNCT ✓\n",
      "220000  73% (  6m 13s)   0.26   |   0.08: in -> PLAIN ✓\n",
      "225000  75% (  6m 22s)   0.23   |   0.00: SANDF -> LETTERS ✓\n",
      "230000  77% (  6m 31s)   0.24   |   0.08: IOC -> LETTERS ✓\n",
      "235000  78% (  6m 39s)   0.28   |   0.05: size -> PLAIN ✓\n",
      "240000  80% (  6m 48s)   0.25   |   0.02: 1994 -> DATE ✓\n",
      "245000  82% (  6m 56s)   0.23   |   0.01: , -> PUNCT ✓\n",
      "250000  83% (   7m 5s)   0.24   |   0.08: 2011 -> DATE ✓\n",
      "255000  85% (  7m 14s)   0.21   |   0.04: ( -> PUNCT ✓\n",
      "260000  87% (  7m 23s)   0.21   |   0.01: 21 April 2015 -> DATE ✓\n",
      "265000  88% (  7m 31s)   0.24   |   0.24: 14th -> ORDINAL ✓\n",
      "270000  90% (  7m 40s)   0.17   |   1.50: Q -> PUNCT ✗ (LETTERS)\n",
      "275000  92% (  7m 48s)   0.22   |   0.01: 12 -> CARDINAL ✓\n",
      "280000  93% (  7m 57s)   0.19   |   0.19: a -> PLAIN ✓\n",
      "285000  95% (   8m 6s)   0.21   |   0.01: July 31, 2011 -> DATE ✓\n",
      "290000  97% (  8m 14s)   0.25   |   0.01: ISBN -> LETTERS ✓\n",
      "295000  98% (  8m 23s)   0.25   |   0.01: ware -> PLAIN ✓\n",
      "300000 100% (  8m 33s)   0.22   |   0.05: of -> PLAIN ✓\n"
     ]
    }
   ],
   "source": [
    "print(use_cuda)\n",
    "all_losses = train_iterations(n_iters=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5000   2% (  0m 11s)   1.89   |   2.11: . -> PLAIN ✗ (PUNCT)\n",
      " 10000   3% (  0m 22s)   1.70   |   1.78: Zod -> PLAIN ✗ (LETTERS)\n",
      " 15000   5% (  0m 32s)   1.59   |   1.21: 1870 -> DATE ✓\n",
      " 20000   7% (  0m 43s)   1.47   |   0.92: . -> PUNCT ✓\n",
      " 25000   8% (  0m 54s)   1.18   |   0.85: Clay -> PLAIN ✓\n",
      " 30000  10% (   1m 5s)   1.05   |   0.65: ( -> PUNCT ✓\n",
      " 35000  12% (  1m 15s)   0.91   |   1.26: Berx -> PLAIN ✗ (LETTERS)\n",
      " 40000  13% (  1m 26s)   0.82   |   0.88: Ny -> LETTERS ✓\n",
      " 45000  15% (  1m 36s)   0.73   |   0.02: February 1, 2011 -> DATE ✓\n",
      " 50000  17% (  1m 47s)   0.65   |   0.30: the -> PLAIN ✓\n",
      " 55000  18% (  1m 57s)   0.56   |   0.27: 1979 -> DATE ✓\n",
      " 60000  20% (   2m 8s)   0.53   |   0.19: Cyclone -> PLAIN ✓\n",
      " 65000  22% (  2m 18s)   0.54   |   4.40: http://www.harvardilj.org/print/124 -> DATE ✗ (ELECTRONIC)\n",
      " 70000  23% (  2m 29s)   0.47   |   0.39: 2000 -> DATE ✓\n",
      " 75000  25% (  2m 40s)   0.45   |   7.30: 210/9 -> DATE ✗ (FRACTION)\n",
      " 80000  27% (  2m 51s)   0.46   |   0.01: commenting -> PLAIN ✓\n",
      " 85000  28% (   3m 2s)   0.48   |   0.01: endangered -> PLAIN ✓\n",
      " 90000  30% (  3m 13s)   0.39   |   0.19: ( -> PUNCT ✓\n",
      " 95000  32% (  3m 24s)   0.38   |   0.07: 1 -> CARDINAL ✓\n",
      "100000  33% (  3m 35s)   0.42   |   0.10: The -> PLAIN ✓\n",
      "105000  35% (  3m 45s)   0.38   |   0.04: . -> PUNCT ✓\n",
      "110000  37% (  3m 56s)   0.37   |   1.48: 3rd -> LETTERS ✗ (ORDINAL)\n",
      "115000  38% (   4m 7s)   0.32   |   0.12: CD -> LETTERS ✓\n",
      "120000  40% (  4m 19s)   0.36   |   1.68: (2002) 74 -> DATE ✗ (TELEPHONE)\n",
      "125000  42% (  4m 30s)   0.37   |   0.33: — -> PUNCT ✓\n",
      "130000  43% (  4m 41s)   0.29   |   0.12: ( -> PUNCT ✓\n",
      "135000  45% (  4m 52s)   0.36   |   0.03: January 27, 2016 -> DATE ✓\n",
      "140000  47% (   5m 3s)   0.32   |   0.04: E. -> LETTERS ✓\n",
      "145000  48% (  5m 13s)   0.29   |   0.08: 1997 -> DATE ✓\n",
      "150000  50% (  5m 25s)   0.31   |   0.03: Brown -> PLAIN ✓\n",
      "155000  52% (  5m 35s)   0.28   |   0.03: . -> PUNCT ✓\n",
      "160000  53% (  5m 47s)   0.32   |   0.02: P. -> LETTERS ✓\n",
      "165000  55% (  5m 58s)   0.28   |   0.02: 10 April 2014 -> DATE ✓\n",
      "170000  57% (   6m 9s)   0.26   |   0.15: : -> PUNCT ✓\n",
      "175000  58% (  6m 19s)   0.28   |   0.97: http://www.iisg.nl/archives/en/files/i/10886062.phpBaraheni -> ELECTRONIC ✓\n",
      "180000  60% (  6m 30s)   0.27   |   0.02: part -> PLAIN ✓\n",
      "185000  62% (  6m 41s)   0.26   |   0.02: . -> PUNCT ✓\n",
      "190000  63% (  6m 51s)   0.26   |   0.13: 1908 -> DATE ✓\n",
      "195000  65% (   7m 2s)   0.24   |   0.02: 17 -> CARDINAL ✓\n",
      "200000  67% (  7m 13s)   0.23   |   0.07: 2006 -> DATE ✓\n",
      "205000  68% (  7m 24s)   0.27   |   0.01: 48 -> CARDINAL ✓\n",
      "210000  70% (  7m 34s)   0.23   |   0.01: S. -> LETTERS ✓\n",
      "215000  72% (  7m 45s)   0.26   |   0.00: , -> PUNCT ✓\n",
      "220000  73% (  7m 56s)   0.24   |   0.02: \" -> PUNCT ✓\n",
      "225000  75% (   8m 6s)   0.24   |   0.01: B. -> LETTERS ✓\n",
      "230000  77% (  8m 17s)   0.21   |   0.00: , -> PUNCT ✓\n",
      "235000  78% (  8m 27s)   0.21   |   0.00: Australian -> PLAIN ✓\n",
      "240000  80% (  8m 38s)   0.23   |   0.01: involves -> PLAIN ✓\n",
      "245000  82% (  8m 49s)   0.20   |   0.01: April 12, 2008 -> DATE ✓\n",
      "250000  83% (  8m 59s)   0.23   |   0.00: January 2003 -> DATE ✓\n",
      "255000  85% (  9m 10s)   0.21   |   0.04: SG -> LETTERS ✓\n",
      "260000  87% (  9m 21s)   0.22   |   0.01: 15 June 2012 -> DATE ✓\n",
      "265000  88% (  9m 31s)   0.22   |   0.07: 5 -> CARDINAL ✓\n",
      "270000  90% (  9m 41s)   0.20   |   0.06: 1983 -> DATE ✓\n",
      "275000  92% (  9m 53s)   0.18   |   0.02: 1998 -> DATE ✓\n",
      "280000  93% (  10m 3s)   0.20   |   0.57: March 7 -> DATE ✓\n",
      "285000  95% ( 10m 14s)   0.16   |   0.10: HM -> LETTERS ✓\n",
      "290000  97% ( 10m 25s)   0.22   |   0.07: 7 -> CARDINAL ✓\n",
      "295000  98% ( 10m 36s)   0.18   |   0.52: 3rd -> ORDINAL ✓\n",
      "300000 100% ( 10m 47s)   0.23   |   0.02: 300 -> CARDINAL ✓\n"
     ]
    }
   ],
   "source": [
    "# with 2 layers \n",
    "rnn = RNN(input_size=letters_n, hidden_size=256, output_size=len(categories_all), n_layers=2).cuda()\n",
    "all_losses = train_iterations(n_iters=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305000   2% (  0m 10s)   0.22   |   0.03: was -> PLAIN ✓\n",
      "310000   3% (  0m 21s)   0.23   |   0.05: 1212 -> DATE ✓\n",
      "315000   5% (  0m 32s)   0.20   |   0.01: . -> PUNCT ✓\n",
      "320000   7% (  0m 43s)   0.22   |   0.02: UK -> LETTERS ✓\n",
      "325000   8% (  0m 53s)   0.21   |   0.09: The -> PLAIN ✓\n",
      "330000  10% (   1m 4s)   0.23   |   0.07: 1 -> CARDINAL ✓\n",
      "335000  12% (  1m 14s)   0.18   |   0.00: father -> PLAIN ✓\n",
      "340000  13% (  1m 25s)   0.19   |   0.03: 400 -> CARDINAL ✓\n",
      "345000  15% (  1m 36s)   0.18   |   0.00: M. -> LETTERS ✓\n",
      "350000  17% (  1m 46s)   0.19   |   0.00: Houghton -> PLAIN ✓\n",
      "355000  18% (  1m 57s)   0.20   |   0.00: PDF -> LETTERS ✓\n",
      "360000  20% (   2m 7s)   0.20   |   0.00: , -> PUNCT ✓\n",
      "365000  22% (  2m 17s)   0.18   |   0.01: feet -> PLAIN ✓\n",
      "370000  23% (  2m 28s)   0.21   |   0.04: 341 -> CARDINAL ✓\n",
      "375000  25% (  2m 38s)   0.20   |   0.11: 116 -> CARDINAL ✓\n",
      "380000  27% (  2m 49s)   0.15   |   0.05: 1878 -> DATE ✓\n",
      "385000  28% (  2m 59s)   0.19   |   0.05: : -> PUNCT ✓\n",
      "390000  30% (  3m 10s)   0.18   |   0.01: CEO -> LETTERS ✓\n",
      "395000  32% (  3m 20s)   0.13   |   0.05: 1980s -> DATE ✓\n",
      "400000  33% (  3m 31s)   0.14   |   0.03: 566 -> CARDINAL ✓\n",
      "405000  35% (  3m 41s)   0.20   |   0.00: 28 December 1989 -> DATE ✓\n",
      "410000  37% (  3m 52s)   0.20   |   0.05: 100 -> CARDINAL ✓\n",
      "415000  38% (   4m 2s)   0.17   |   0.00: , -> PUNCT ✓\n",
      "420000  40% (  4m 13s)   0.15   |   0.00: . -> PUNCT ✓\n",
      "425000  42% (  4m 24s)   0.19   |   0.01: structures -> PLAIN ✓\n",
      "430000  43% (  4m 34s)   0.20   |   0.18: II -> CARDINAL ✓\n",
      "435000  45% (  4m 44s)   0.15   |   0.00: . -> PUNCT ✓\n",
      "440000  47% (  4m 55s)   0.15   |   0.00: . -> PUNCT ✓\n",
      "445000  48% (   5m 5s)   0.20   |   0.26: up -> PLAIN ✓\n",
      "450000  50% (  5m 16s)   0.14   |   0.02: 74 -> CARDINAL ✓\n",
      "455000  52% (  5m 26s)   0.15   |   0.00: \" -> PUNCT ✓\n",
      "460000  53% (  5m 37s)   0.13   |   0.04: 2007 -> DATE ✓\n",
      "465000  55% (  5m 47s)   0.16   |   0.01: ) -> PUNCT ✓\n",
      "470000  57% (  5m 58s)   0.15   |   0.00: 16 March 2006 -> DATE ✓\n",
      "475000  58% (   6m 8s)   0.15   |   0.03: 1967 -> DATE ✓\n",
      "480000  60% (  6m 19s)   0.16   |   0.01: 1958 -> DATE ✓\n",
      "485000  62% (  6m 29s)   0.16   |   0.00: the -> PLAIN ✓\n",
      "490000  63% (  6m 40s)   0.15   |   2.10: 4\" -> CARDINAL ✗ (MEASURE)\n",
      "495000  65% (  6m 50s)   0.16   |   0.01: W. -> LETTERS ✓\n",
      "500000  67% (   7m 1s)   0.15   |   0.05: 1581 -> DATE ✓\n",
      "505000  68% (  7m 11s)   0.19   |   0.01: Freguesia -> PLAIN ✓\n",
      "510000  70% (  7m 22s)   0.17   |   0.00: March 26, 2013 -> DATE ✓\n",
      "515000  72% (  7m 32s)   0.12   |   0.02: LP -> LETTERS ✓\n",
      "520000  73% (  7m 43s)   0.18   |   0.02: of -> PLAIN ✓\n",
      "525000  75% (  7m 54s)   0.17   |   0.00: . -> PUNCT ✓\n",
      "530000  77% (   8m 4s)   0.14   |   0.00: that -> PLAIN ✓\n",
      "535000  78% (  8m 15s)   0.16   |   0.01: 13 -> CARDINAL ✓\n",
      "540000  80% (  8m 25s)   0.17   |   0.01: way -> PLAIN ✓\n",
      "545000  82% (  8m 36s)   0.16   |   0.02: 1994 -> DATE ✓\n",
      "550000  83% (  8m 46s)   0.13   |   0.02: 1979 -> DATE ✓\n",
      "555000  85% (  8m 57s)   0.15   |   0.02: ISR -> LETTERS ✓\n",
      "560000  87% (   9m 7s)   0.16   |   0.00: . -> PUNCT ✓\n",
      "565000  88% (  9m 18s)   0.15   |   0.03: his -> PLAIN ✓\n",
      "570000  90% (  9m 29s)   0.14   |   0.05: 1 -> CARDINAL ✓\n",
      "575000  92% (  9m 39s)   0.13   |   0.01: ire -> PLAIN ✓\n",
      "580000  93% (  9m 50s)   0.16   |   0.02: 40 -> CARDINAL ✓\n",
      "585000  95% (  10m 1s)   0.14   |   0.00: ) -> PUNCT ✓\n",
      "590000  97% ( 10m 11s)   0.13   |   0.01: the -> PLAIN ✓\n",
      "595000  98% ( 10m 21s)   0.17   |   0.02: French -> PLAIN ✓\n",
      "600000 100% ( 10m 32s)   0.12   |   0.01: Producers -> PLAIN ✓\n"
     ]
    }
   ],
   "source": [
    "all_losses = train_iterations(start_iter=300000, n_iters=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605000   2% (  0m 10s)   0.13   |   0.00: , -> PUNCT ✓\n",
      "610000   3% (  0m 21s)   0.14   |   0.08: OS -> LETTERS ✓\n",
      "615000   5% (  0m 31s)   0.11   |   0.03: for -> PLAIN ✓\n",
      "620000   7% (  0m 42s)   0.18   |   0.14: $8,500,000 -> MONEY ✓\n",
      "625000   8% (  0m 52s)   0.14   |   0.01: 10 -> CARDINAL ✓\n",
      "630000  10% (   1m 3s)   0.14   |   0.01: it -> PLAIN ✓\n",
      "635000  12% (  1m 13s)   0.12   |   0.01: 30 -> CARDINAL ✓\n",
      "640000  13% (  1m 24s)   0.16   |   0.00: series -> PLAIN ✓\n",
      "645000  15% (  1m 34s)   0.16   |   0.00: ( -> PUNCT ✓\n",
      "650000  17% (  1m 45s)   0.13   |   0.01: 48 -> CARDINAL ✓\n",
      "655000  18% (  1m 56s)   0.16   |   0.03: 1957 -> DATE ✓\n",
      "660000  20% (   2m 6s)   0.17   |   0.00: , -> PUNCT ✓\n",
      "665000  22% (  2m 17s)   0.14   |   0.00: . -> PUNCT ✓\n",
      "670000  23% (  2m 28s)   0.19   |   0.00: , -> PUNCT ✓\n",
      "675000  25% (  2m 39s)   0.17   |   0.00: , -> PUNCT ✓\n",
      "680000  27% (  2m 50s)   0.14   |   0.01: the -> PLAIN ✓\n",
      "685000  28% (   3m 0s)   0.13   |   0.00: struggles -> PLAIN ✓\n",
      "690000  30% (  3m 11s)   0.14   |   0.00: often -> PLAIN ✓\n",
      "695000  32% (  3m 21s)   0.17   |   0.06: 8 -> CARDINAL ✓\n",
      "700000  33% (  3m 32s)   0.15   |   0.03: XAA- -> LETTERS ✓\n",
      "705000  35% (  3m 43s)   0.15   |   0.00: ) -> PUNCT ✓\n",
      "710000  37% (  3m 54s)   0.12   |   0.00: D.A. -> LETTERS ✓\n",
      "715000  38% (   4m 5s)   0.16   |   0.05: .11% -> MEASURE ✓\n",
      "720000  40% (  4m 16s)   0.12   |   0.00: ( -> PUNCT ✓\n",
      "725000  42% (  4m 27s)   0.15   |   0.17: ? -> PUNCT ✓\n",
      "730000  43% (  4m 37s)   0.15   |   0.20: $20 million -> MONEY ✓\n",
      "735000  45% (  4m 48s)   0.14   |   0.01: in -> PLAIN ✓\n",
      "740000  47% (  4m 59s)   0.13   |   0.32: $858 million -> MONEY ✓\n",
      "745000  48% (  5m 10s)   0.16   |   0.01: 20 -> CARDINAL ✓\n",
      "750000  50% (  5m 21s)   0.13   |   0.05: 4 -> CARDINAL ✓\n",
      "755000  52% (  5m 31s)   0.16   |   0.01: Records -> PLAIN ✓\n",
      "760000  53% (  5m 42s)   0.16   |   0.19: $60 million -> MONEY ✓\n",
      "765000  55% (  5m 53s)   0.15   |   0.05: 2005 -> DATE ✓\n",
      "770000  57% (   6m 3s)   0.18   |   0.00: , -> PUNCT ✓\n",
      "775000  58% (  6m 14s)   0.13   |   0.01: 24 -> CARDINAL ✓\n",
      "780000  60% (  6m 25s)   0.13   |   0.11: by -> PLAIN ✓\n",
      "785000  62% (  6m 35s)   0.10   |   0.00: \" -> PUNCT ✓\n",
      "790000  63% (  6m 46s)   0.18   |   0.00: E. -> LETTERS ✓\n",
      "795000  65% (  6m 57s)   0.14   |   0.02: 14 -> CARDINAL ✓\n",
      "800000  67% (   7m 7s)   0.13   |   0.07: 2001 -> DATE ✓\n",
      "805000  68% (  7m 18s)   0.13   |   0.03: 2007-10-01 -> DATE ✓\n",
      "810000  70% (  7m 28s)   0.15   |   2.31: ICHNEUMONIDAE -> ELECTRONIC ✗ (LETTERS)\n",
      "815000  72% (  7m 39s)   0.10   |   0.07: ' -> PUNCT ✓\n",
      "820000  73% (  7m 50s)   0.13   |   0.02: 1919 -> DATE ✓\n",
      "825000  75% (   8m 1s)   0.12   |   0.00: , -> PUNCT ✓\n",
      "830000  77% (  8m 11s)   0.12   |   3.74: 566 -> CARDINAL ✗ (DIGIT)\n",
      "835000  78% (  8m 22s)   0.15   |   0.00: . -> PUNCT ✓\n",
      "840000  80% (  8m 34s)   0.14   |   0.05: 2001 -> DATE ✓\n",
      "845000  82% (  8m 44s)   0.13   |   0.00: . -> PUNCT ✓\n",
      "850000  83% (  8m 56s)   0.13   |   0.00: additional -> PLAIN ✓\n",
      "855000  85% (   9m 7s)   0.11   |   0.14: 2002 -> DATE ✓\n",
      "860000  87% (  9m 18s)   0.13   |   0.02: 12 -> CARDINAL ✓\n",
      "865000  88% (  9m 29s)   0.11   |   0.00: . -> PUNCT ✓\n",
      "870000  90% (  9m 40s)   0.12   |   0.00: June 2011 -> DATE ✓\n",
      "875000  92% (  9m 51s)   0.13   |   0.29: 9985-3-0537 -> TELEPHONE ✓\n",
      "880000  93% (  10m 2s)   0.12   |   0.01: of -> PLAIN ✓\n",
      "885000  95% ( 10m 13s)   0.15   |   0.01: the -> PLAIN ✓\n",
      "890000  97% ( 10m 24s)   0.14   |   0.05: 3 -> CARDINAL ✓\n",
      "895000  98% ( 10m 35s)   0.11   |   0.02: 98 -> CARDINAL ✓\n",
      "900000 100% ( 10m 46s)   0.14   |   0.22: Mike -> PLAIN ✓\n"
     ]
    }
   ],
   "source": [
    "all_losses = train_iterations(start_iter=600000, n_iters=300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_accuracy(model, n_sample=10000):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    all_losses = []\n",
    "    current_loss = 0\n",
    "    current_loss_iter = 0\n",
    "    \n",
    "    n_correct = 0\n",
    "\n",
    "    for iteration in range(n_sample):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "        output = model(line_tensor, model.initHidden())\n",
    "        if category == categoryFromOutput(output)[0]:\n",
    "            n_correct += 1\n",
    "        \n",
    "        \n",
    "    print(\"{:>4.2%} ({:>8d}/{:>8d})\".format(\n",
    "            n_correct/n_sample, n_correct, n_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_accuracy(rnn, n_sample=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    #hidden = rnn.initHidden()\n",
    "\n",
    "    #for i in range(line_tensor.size()[0]):\n",
    "    #    output, hidden = rnn(line_tensor[i], hidden)\n",
    "    output = rnn(line_tensor, rnn.initHidden())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEzCAYAAAAPe9kVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86189e7320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(len(categories_all), len(categories_all))\n",
    "n_confusion = 100000\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    # category_i = categories_all.index(category)\n",
    "    category_i = categories_index[category]\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(len(categories)):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + list(categories_all), rotation=90)\n",
    "ax.set_yticklabels([''] + list(categories_all))\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(lineToTensor(input_line)))\n",
    "\n",
    "    # Get top N categories\n",
    "    #topv, topi = output.data.topk(n_predictions, 1, True)\n",
    "    topv, topi = output.data.topk(n_predictions, 2, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][0][i]\n",
    "        category_index = topi[0][0][i]\n",
    "        print('(%.2f) %s' % (value, categories_all[category_index]))\n",
    "        predictions.append([value, categories_all[category_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Normal\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.FloatTensor, torch.cuda.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:\n * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.cuda.FloatTensor\u001b[0m)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.cuda.FloatTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-981-30e2edc83b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2017-12-12'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-774-c1cd57c97bfb>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(input_line, n_predictions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_predictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n> %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0minput_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlineToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get top N categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-854-e96b18806d94>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(line_tensor)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#for i in range(line_tensor.size()[0]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#    output, hidden = rnn(line_tensor[i], hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-971-0cbd20399d28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word, hidden)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mall_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#output = hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#output = output.view(1, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb_ih\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3_pytorch_2/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.FloatTensor, torch.cuda.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:\n * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.cuda.FloatTensor\u001b[0m)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.cuda.FloatTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "predict('Normal')\n",
    "predict('2017-12-12')\n",
    "predict('~')\n",
    "predict('20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
