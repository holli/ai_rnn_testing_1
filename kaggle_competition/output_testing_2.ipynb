{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "from pytorch_utils_oh_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch utils oh: pytorch_utils_oh_2.py\n",
      "Pytorch: 0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import pytorch_utils_oh_2; importlib.reload(pytorch_utils_oh_2); from pytorch_utils_oh_2 import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_normal, chars_normal_index = load_characters_pkl('data/en_features/chars_normal.pkl')\n",
    "common_words, common_words_index = load_common_words_10k()\n",
    "chars_with_changes = pickle.load(open('data/en_features/chars_with_changes.pkl', \"rb\"))\n",
    "chars_with_no_changes_re = re.compile(\"[^{}]\".format(''.join(chars_with_changes)))\n",
    "chars_with_single_output_dict = pickle.load(open('data/en_features/chars_with_single_output_dict.pkl', \"rb\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088564"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_org = pd.read_csv('data/en_test.csv', keep_default_na=False)\n",
    "len(test_data_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data_org.copy()\n",
    "test_data_sentence_index = test_data.set_index('sentence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53612</th>\n",
       "      <td>3434</td>\n",
       "      <td>3</td>\n",
       "      <td>SchoolWaconda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881019</th>\n",
       "      <td>56651</td>\n",
       "      <td>19</td>\n",
       "      <td>release</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id         before\n",
       "53612          3434         3  SchoolWaconda\n",
       "881019        56651        19        release"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+': 'plus w',\n",
       " '=': 'equals',\n",
       " '>': 'greater than',\n",
       " '×': 'times',\n",
       " 'Α': 'alpha',\n",
       " 'Β': 'beta',\n",
       " 'Γ': 'gamma',\n",
       " 'Δ': 'delta',\n",
       " 'Ε': 'epsilon',\n",
       " 'Ζ': 'zeta',\n",
       " 'Η': 'eta',\n",
       " 'Θ': 'theta',\n",
       " 'Ι': 'iota',\n",
       " 'Κ': 'kappa',\n",
       " 'Λ': 'lambda',\n",
       " 'Μ': 'mu',\n",
       " 'Ν': 'nu',\n",
       " 'Ξ': 'xi',\n",
       " 'Ο': 'omicron',\n",
       " 'Π': 'pi',\n",
       " 'Ρ': 'rho',\n",
       " 'Σ': 'sigma',\n",
       " 'Τ': 'tau',\n",
       " 'Υ': 'upsilon',\n",
       " 'Φ': 'phi',\n",
       " 'Χ': 'chi',\n",
       " 'Ψ': 'psi',\n",
       " 'α': 'alpha',\n",
       " 'β': 'beta',\n",
       " 'γ': 'gamma',\n",
       " 'δ': 'delta',\n",
       " 'ε': 'epsilon',\n",
       " 'ζ': 'zeta',\n",
       " 'η': 'eta',\n",
       " 'θ': 'theta',\n",
       " 'ι': 'iota',\n",
       " 'κ': 'kappa',\n",
       " 'λ': 'lambda',\n",
       " 'ν': 'nu',\n",
       " 'ξ': 'xi',\n",
       " 'ο': 'omicron',\n",
       " 'π': 'pi',\n",
       " 'ρ': 'rho',\n",
       " 'ς': 'sigma',\n",
       " 'σ': 'sigma',\n",
       " 'τ': 'tau',\n",
       " 'υ': 'upsilon',\n",
       " 'φ': 'phi',\n",
       " 'χ': 'chi',\n",
       " 'ψ': 'psi',\n",
       " 'ω': 'omega',\n",
       " '⅝': 'five eighths',\n",
       " '⅞': 'seven eighths'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_with_single_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manual_pre_checks(before):\n",
    "    if chars_with_no_changes_re.search(before): #contains chars that were never changed\n",
    "        return before \n",
    "    if before in chars_with_single_output_dict:\n",
    "        return chars_with_single_output_dict[before]\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'epsilon'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'利'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_pre_checks('yes')\n",
    "manual_pre_checks('Ε')\n",
    "manual_pre_checks('利')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sigma'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_pre_checks('ς')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_all = ['NOT_CHANGED', 'NUMBERS', 'LETTERS', 'PLAIN', 'VERBATIM', 'ELECTRONIC']\n",
    "categories_index = dict((c, i) for i, c in enumerate(categories_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategorizeRNN(nn.Module):\n",
    "    def __init__(self, output_size, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_dropout=0, chars_dropout=0, words_layers=1, chars_layers=1):\n",
    "        super(CategorizeRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 dropout=words_dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                 dropout=chars_dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.lin_output = nn.Linear(words_hidden_size+chars_hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, words_tensor, string_tensor, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(words_tensor, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        #output = self.lin_1(output)\n",
    "        output = self.lin_output(output)\n",
    "        output = F.log_softmax(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategorizeRNN (\n",
       "  (rnn_words): LSTM(8192, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (lin_output): Linear (256 -> 6)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_categorize = CategorizeRNN(len(categories_all), \n",
    "                              words_input_size=len(common_words), chars_input_size=len(chars_normal),\n",
    "                              words_hidden_size=128, chars_hidden_size=128,\n",
    "                              words_layers=2, chars_layers=2,\n",
    "                              words_dropout=0.2, chars_dropout=0.2)\n",
    "m_categorize = m_categorize.cuda()\n",
    "\n",
    "m_categorize\n",
    "\n",
    "m_categorize.load_state_dict(torch.load('data/models/category_6_mod_data_common_words/400000_CategorizeRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NOT_CHANGED', 0.9998573064804077)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('NOT_CHANGED', 0.9991723299026489)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize(s_bef, s_sentence):\n",
    "    model = m_categorize\n",
    "    #words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = words_to_tensor(sentence_arr_tokenize(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    output = model(words_t, string_t)\n",
    "    guess = category_from_output(output, categories_all)\n",
    "    confidence = torch.nn.functional.softmax(output).topk(1)[0].data[0][0]\n",
    "    return guess[0], confidence\n",
    "\n",
    "categorize('hello', ['<SAMPLE> it\\'s me'])\n",
    "categorize('hello', 'Hello welcome to <SAMPLE>'.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words = pickle.load(open('data/models/numbers_gen_4_mod_data_1/number_words.pkl', 'rb'))\n",
    "number_words_index = dict((c, i) for i, c in enumerate(number_words))\n",
    "len(number_words_index)\n",
    "def number_words_to_tensor(words, include_eos=True):\n",
    "    return words_to_tensor(words, words_lookup_index=number_words_index, include_eos=include_eos)\n",
    "number_words_onehot_sos = number_words_to_tensor([SOS_TOKEN], include_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumEncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(NumEncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumEncoderRNN (\n",
       "  (rnn_words): LSTM(8192, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_rnn = NumEncoderRNN(words_input_size=len(common_words), chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=128,\n",
    "                         words_layers=2, chars_layers=2).cuda()\n",
    "num_encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumDecoderRNN (\n",
       "  (rnn): GRU(511, 256, batch_first=True)\n",
       "  (lin_out): Linear (256 -> 511)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NumDecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(NumDecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "                         # LSTM would require own hidden included\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "num_decoder_rnn = NumDecoderRNN(input_size=len(number_words), hidden_size=128*2, n_layers=1)\n",
    "num_decoder_rnn = num_decoder_rnn.cuda()\n",
    "num_decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encoder_rnn.load_state_dict(torch.load('data/models/numbers_gen_4_mod_data_1/2250000_EncoderRNN'))\n",
    "num_decoder_rnn.load_state_dict(torch.load('data/models/numbers_gen_4_mod_data_1/2250000_DecoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('four thousand two hundred', -0.065897941589355469)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('twenty five thousand two o', -2.3529701232910156)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('twenty five thousand two hundred two', -2.8845462799072266)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_model_eval(s_bef, s_sentence):\n",
    "    #words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = words_to_tensor(sentence_arr_tokenize(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = num_encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = encoder_output\n",
    "    decoder_input = Variable(number_words_onehot_sos).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    max_length = 20\n",
    "    \n",
    "    decoder_confidences = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = num_decoder_rnn(decoder_input, decoder_hidden)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        decoder_confidences.append(topv[0][0])\n",
    "        word_index = topi[0][0]\n",
    "        word = number_words[word_index] # Use own prediction as next input\n",
    "                \n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(word)\n",
    "        \n",
    "        decoder_input = number_words_to_tensor([word], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ' '.join(decoded_output)\n",
    "    return output, np.sum(decoder_confidences)\n",
    "    \n",
    "num_model_eval('4200', ['he', 'was', '<SAMPLE>', 'years', 'old'])\n",
    "num_model_eval('25200€', ['the', 'price', 'is', '<SAMPLE>'])\n",
    "num_model_eval('25200€', ['my', 'phone', 'is', '<SAMPLE>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numbers(s_bef, sentence_words):\n",
    "    return num_model_eval(s_bef, sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_after = pickle.load(open('data/en_features/chars_after_1.pkl', 'rb'))\n",
    "chars_after_index = dict((c, i) for i, c in enumerate(chars_after))\n",
    "def after_string_to_tensor(word, include_eos=True):\n",
    "    return string_to_tensor(word, chars_index=chars_after_index, include_eos=include_eos)\n",
    "after_string_to_tensor('abcé').shape\n",
    "after_string_to_tensor('abcé')[0, -2, -1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_onehot_sos = after_string_to_tensor([SOS_TOKEN], include_eos=False)\n",
    "whole_onehot_sos.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WholeEncoderRNN(nn.Module):\n",
    "    def __init__(self, words_input_size, chars_input_size, words_hidden_size, chars_hidden_size,\n",
    "                 words_layers=1, chars_layers=1):\n",
    "        super(WholeEncoderRNN, self).__init__()\n",
    "        \n",
    "        self.words_layers = words_layers\n",
    "        self.chars_layers = chars_layers\n",
    "        self.words_hidden_size = words_hidden_size\n",
    "        self.chars_hidden_size = chars_hidden_size\n",
    "\n",
    "        self.rnn_words = nn.LSTM(words_input_size, words_hidden_size // 2, words_layers,\n",
    "                                 batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.rnn_chars = nn.LSTM(chars_input_size, chars_hidden_size // 2, chars_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_vectors, string_tensor, hidden = None, init_hidden = True):\n",
    "        if init_hidden:\n",
    "            hidden_words, hidden_chars = self.init_hidden()\n",
    "        \n",
    "        all_outputs_words, hidden_words = self.rnn_words(word_vectors, hidden_words)\n",
    "        output_words = all_outputs_words[:, -1]\n",
    "        \n",
    "        all_outputs_chars, hidden_chars = self.rnn_chars(string_tensor, hidden_chars)\n",
    "        output_chars = all_outputs_chars[:, -1]\n",
    "        \n",
    "        output = torch.cat((output_words, output_chars), 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        var1_1 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var1_2 = Variable(torch.zeros(2 * self.words_layers, 1, self.words_hidden_size // 2))\n",
    "        var2_1 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        var2_2 = Variable(torch.zeros(2 * self.chars_layers, 1, self.chars_hidden_size // 2))\n",
    "        \n",
    "        var1_1 = var1_1.cuda(); var1_2 = var1_2.cuda()\n",
    "        var2_1 = var2_1.cuda(); var2_2 = var2_2.cuda()\n",
    "        return ((var1_1, var1_2), (var2_1, var2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WholeEncoderRNN (\n",
       "  (rnn_words): LSTM(8192, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (rnn_chars): LSTM(104, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_encoder_rnn = WholeEncoderRNN(words_input_size=len(common_words), chars_input_size=len(chars_normal),\n",
    "                         words_hidden_size=128, chars_hidden_size=128,\n",
    "                         words_layers=2, chars_layers=2).cuda()\n",
    "whole_encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WholeDecoderRNN (\n",
       "  (rnn): GRU(32, 256, num_layers=2, batch_first=True)\n",
       "  (lin_out): Linear (256 -> 32)\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WholeDecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(WholeDecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.lin_out = nn.Linear(hidden_size, input_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, char, hidden):\n",
    "        #char = char.view(1,1,-1)\n",
    "        #hidden = hidden.view(1,1,-1)\n",
    "        output, hidden = self.rnn(char, hidden)\n",
    "        output = output[:, -1] # view(1,-1)\n",
    "        output = self.lin_out(output)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_rest_hidden(self, input_var):\n",
    "        hid_var = Variable(torch.zeros(self.n_layers - 1, 1, self.hidden_size)).cuda()\n",
    "        res = torch.cat((input_var, hid_var), 0)\n",
    "        return res\n",
    "        \n",
    "\n",
    "whole_decoder_rnn = WholeDecoderRNN(input_size=len(chars_after), hidden_size=256, n_layers=2).cuda()\n",
    "whole_decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_encoder_rnn.load_state_dict(torch.load('data/models/whole_gen_2_chars/200000_EncoderRNN'))\n",
    "whole_decoder_rnn.load_state_dict(torch.load('data/models/whole_gen_2_chars/200000_DecoderRNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('four hundred thousan', -6.1360671520233154)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('and', -0.0025339126586914062)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def whole_model_eval(s_bef, s_sentence):\n",
    "    #words_t = words_to_tensor(list(s_sentence), common_words_index)\n",
    "    words_t = words_to_tensor(sentence_arr_tokenize(s_sentence), common_words_index)\n",
    "    words_t = Variable(words_t).cuda()\n",
    "    \n",
    "    string_t = string_to_tensor(s_bef, chars_normal_index)\n",
    "    string_t = Variable(string_t).cuda()\n",
    "    \n",
    "    encoder_output = whole_encoder_rnn(words_t, string_t)\n",
    "    \n",
    "    encoder_output = encoder_output.view(1,1,-1)\n",
    "    \n",
    "    decoder_hidden = whole_decoder_rnn.init_rest_hidden(encoder_output)\n",
    "    decoder_input = Variable(whole_onehot_sos).cuda()\n",
    "\n",
    "    decoded_output = []\n",
    "    max_length = 20\n",
    "    \n",
    "    decoder_confidences = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = whole_decoder_rnn(decoder_input, decoder_hidden)\n",
    "        #return decoder_output\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        char_index = topi[0][0]\n",
    "        char = chars_after[char_index] # Use own prediction as next input\n",
    "        \n",
    "        decoder_confidences.append(topv[0][0])\n",
    "                \n",
    "        if char == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_output.append(char)\n",
    "        \n",
    "        decoder_input = after_string_to_tensor([char], include_eos=False)\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    output = ''.join(decoded_output)\n",
    "    return output, np.sum(decoder_confidences)\n",
    "    \n",
    "whole_model_eval('4200', ['he', 'was', '<SAMPLE>', 'years', 'old'])\n",
    "whole_model_eval('&', ['<SAMPLE>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def letters(x):\n",
    "    try:\n",
    "        x = re.sub('[^a-zA-Z]', '', x)\n",
    "        x = x.lower()\n",
    "        result_string = ''\n",
    "        for i in range(len(x)):\n",
    "            result_string = result_string + x[i] + ' '\n",
    "        return(result_string.strip())  \n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters('X.D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plain(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbatim(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def electronic(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088564"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rows_category(pd_rows):\n",
    "    global current_row\n",
    "    iter_len = len(pd_rows)\n",
    "    rows_iter = pd_rows.itertuples()\n",
    "    iteration_idx = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    #for sample_row in test_data[0:100].itertuples():\n",
    "    for sample_row in log_progress(rows_iter, every=10, size=iter_len):\n",
    "        before = sample_row.before\n",
    "        current_row = sample_row\n",
    "\n",
    "        sentence_rows = test_data_sentence_index.loc[sample_row.sentence_id]\n",
    "        sentence_words = list(sentence_rows.before)\n",
    "        token_id_idx = list(sentence_rows['token_id']).index(sample_row.token_id)\n",
    "        sentence_words[token_id_idx] = SAMPLE_WORD_TOKEN\n",
    "\n",
    "        manual_pre_check_after = manual_pre_checks(before)\n",
    "        if manual_pre_check_after:\n",
    "            test_data.at[sample_row.Index, 'after'] = manual_pre_check_after\n",
    "            test_data.at[sample_row.Index, 'pred_class'] = 'MANUAL'\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            category, category_conf = categorize(before, sentence_words)\n",
    "            test_data.at[sample_row.Index, 'pred_class'] = category\n",
    "            test_data.at[sample_row.Index, 'pred_c_conf'] = category_conf\n",
    "            if category == 'NOT_CHANGED':\n",
    "                test_data.at[sample_row.Index, 'after'] = result\n",
    "        except: # Exception as inst:\n",
    "            test_data.at[sample_row.Index, 'pred_class'] = 'PROBLEM'\n",
    "            continue\n",
    "\n",
    "        iteration_idx += 1\n",
    "        if iteration_idx%10000 == 0:\n",
    "            print(\"{:>7d} {:>2.2%} ({:>8})\".format(iteration_idx, iteration_idx/iter_len, time_since(start)))\n",
    "\n",
    "        test_data.at[sample_row.Index, 'after'] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        elif category == 'NUMBERS':\n",
    "            result = numbers(before)\n",
    "        elif category == 'LETTERS':\n",
    "            result = letters(before)\n",
    "        elif category == 'PLAIN':\n",
    "            result = plain(before)\n",
    "        elif category == 'VERBATIM':\n",
    "            result = verbatim(before)\n",
    "        elif category == 'ELECTRONIC':\n",
    "            result = electronic(before)\n",
    "        else:\n",
    "            print(\"PROBLEM WITH:\", sample_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5adb6509eaf4cb390a82a656019430f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_rows(test_data[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43640dc2e9fa4349bbec8dce9e1f31e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000 0.92% (  1m 19s)\n",
      "  20000 1.84% (  2m 32s)\n",
      "  30000 2.76% (  3m 41s)\n",
      "  40000 3.67% (  4m 51s)\n",
      "  50000 4.59% (  6m 10s)\n",
      "  60000 5.51% (  7m 29s)\n",
      "  70000 6.43% (  8m 50s)\n",
      "  80000 7.35% (  10m 1s)\n",
      "  90000 8.27% ( 10m 50s)\n",
      " 100000 9.19% (  12m 5s)\n",
      " 110000 10.11% ( 13m 25s)\n",
      " 120000 11.02% ( 14m 47s)\n",
      " 130000 11.94% ( 16m 11s)\n",
      " 140000 12.86% ( 17m 37s)\n",
      " 150000 13.78% (  19m 4s)\n",
      " 160000 14.70% ( 20m 27s)\n",
      " 170000 15.62% ( 21m 51s)\n",
      " 180000 16.54% ( 23m 15s)\n",
      " 190000 17.45% ( 24m 43s)\n",
      " 200000 18.37% (  26m 9s)\n",
      " 210000 19.29% ( 27m 35s)\n",
      " 220000 20.21% (  29m 3s)\n",
      " 230000 21.13% ( 30m 23s)\n",
      " 240000 22.05% ( 31m 51s)\n",
      " 250000 22.97% ( 33m 16s)\n",
      " 260000 23.88% ( 34m 42s)\n",
      " 270000 24.80% (  36m 6s)\n",
      " 280000 25.72% ( 37m 25s)\n",
      " 290000 26.64% ( 38m 43s)\n",
      " 300000 27.56% ( 39m 59s)\n",
      " 310000 28.48% ( 41m 14s)\n",
      " 320000 29.40% ( 42m 36s)\n",
      " 330000 30.32% ( 43m 55s)\n",
      " 340000 31.23% ( 45m 11s)\n",
      " 350000 32.15% ( 46m 33s)\n",
      " 360000 33.07% ( 47m 49s)\n",
      " 370000 33.99% ( 49m 13s)\n",
      " 380000 34.91% ( 50m 32s)\n",
      " 390000 35.83% ( 51m 51s)\n",
      " 400000 36.75% (  53m 8s)\n",
      " 410000 37.66% ( 54m 22s)\n",
      " 420000 38.58% ( 55m 34s)\n",
      " 430000 39.50% ( 56m 57s)\n",
      " 440000 40.42% ( 58m 13s)\n",
      " 450000 41.34% ( 59m 28s)\n",
      " 460000 42.26% ( 60m 44s)\n",
      " 470000 43.18% (  62m 4s)\n",
      " 480000 44.09% ( 63m 17s)\n",
      " 490000 45.01% ( 64m 28s)\n",
      " 500000 45.93% ( 65m 45s)\n",
      " 510000 46.85% ( 66m 58s)\n",
      " 520000 47.77% ( 68m 16s)\n",
      " 530000 48.69% ( 69m 31s)\n",
      " 540000 49.61% ( 70m 51s)\n",
      " 550000 50.53% ( 72m 11s)\n",
      " 560000 51.44% ( 73m 28s)\n",
      " 570000 52.36% ( 74m 47s)\n",
      " 580000 53.28% (  76m 4s)\n",
      " 590000 54.20% ( 77m 21s)\n",
      " 600000 55.12% ( 78m 43s)\n",
      " 610000 56.04% ( 79m 59s)\n",
      " 620000 56.96% ( 81m 15s)\n",
      " 630000 57.87% ( 82m 30s)\n",
      " 640000 58.79% ( 83m 44s)\n",
      " 650000 59.71% (  85m 5s)\n",
      " 660000 60.63% ( 86m 21s)\n",
      " 670000 61.55% ( 87m 35s)\n",
      " 680000 62.47% ( 88m 51s)\n",
      " 690000 63.39% (  90m 8s)\n",
      " 700000 64.30% ( 91m 25s)\n",
      " 710000 65.22% ( 92m 39s)\n",
      " 720000 66.14% ( 93m 50s)\n",
      " 730000 67.06% ( 94m 42s)\n",
      " 740000 67.98% ( 95m 34s)\n",
      " 750000 68.90% ( 96m 25s)\n",
      " 760000 69.82% ( 97m 19s)\n",
      " 770000 70.74% (  98m 9s)\n",
      " 780000 71.65% (  99m 0s)\n",
      " 790000 72.57% ( 99m 52s)\n",
      " 800000 73.49% (100m 44s)\n",
      " 810000 74.41% (101m 35s)\n",
      " 820000 75.33% (102m 26s)\n",
      " 830000 76.25% (103m 17s)\n",
      " 840000 77.17% ( 104m 8s)\n",
      " 850000 78.08% ( 105m 0s)\n",
      " 860000 79.00% (105m 52s)\n",
      " 870000 79.92% (106m 43s)\n",
      " 880000 80.84% (107m 34s)\n",
      " 890000 81.76% (108m 26s)\n",
      " 900000 82.68% (109m 16s)\n",
      " 910000 83.60% ( 110m 8s)\n",
      " 920000 84.52% ( 111m 0s)\n",
      " 930000 85.43% (111m 51s)\n",
      " 940000 86.35% (112m 42s)\n",
      " 950000 87.27% (113m 34s)\n",
      " 960000 88.19% (114m 26s)\n",
      " 970000 89.11% (115m 17s)\n",
      " 980000 90.03% ( 116m 8s)\n",
      " 990000 90.95% (116m 59s)\n",
      "1000000 91.86% (117m 49s)\n",
      "1010000 92.78% (118m 42s)\n",
      "1020000 93.70% (119m 32s)\n",
      "1030000 94.62% (120m 25s)\n",
      "1040000 95.54% (121m 17s)\n",
      "1050000 96.46% ( 122m 9s)\n",
      "1060000 97.38% ( 123m 1s)\n",
      "1070000 98.29% (123m 53s)\n",
      "1080000 99.21% (124m 44s)\n"
     ]
    }
   ],
   "source": [
    "run_rows(test_data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(test_data, open('data/output_testing_2_1_first_run.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence_id, token_id, before, pred_class, pred_c_conf, after]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data['pred_class']=='PROBLEM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_class\n",
       "ELECTRONIC     0.000906\n",
       "LETTERS        0.027239\n",
       "MANUAL         0.003980\n",
       "NOT_CHANGED    0.900857\n",
       "NUMBERS        0.053430\n",
       "PLAIN          0.012385\n",
       "VERBATIM       0.001203\n",
       "Name: pred_class, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.groupby('pred_class')['pred_class'].count()/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218253</th>\n",
       "      <td>14025</td>\n",
       "      <td>8</td>\n",
       "      <td>times</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.999523</td>\n",
       "      <td>times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889067</th>\n",
       "      <td>57163</td>\n",
       "      <td>11</td>\n",
       "      <td>pursue</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>pursue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id  before   pred_class  pred_c_conf   after\n",
       "218253        14025         8   times  NOT_CHANGED     0.999523   times\n",
       "889067        57163        11  pursue  NOT_CHANGED     0.999051  pursue"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171576</th>\n",
       "      <td>11047</td>\n",
       "      <td>10</td>\n",
       "      <td>Don</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.353442</td>\n",
       "      <td>Don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270842</th>\n",
       "      <td>17405</td>\n",
       "      <td>6</td>\n",
       "      <td>v</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>0.392677</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036163</th>\n",
       "      <td>66637</td>\n",
       "      <td>7</td>\n",
       "      <td>Page</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.386125</td>\n",
       "      <td>Page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845127</th>\n",
       "      <td>54367</td>\n",
       "      <td>0</td>\n",
       "      <td>Don</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.385927</td>\n",
       "      <td>Don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659862</th>\n",
       "      <td>42489</td>\n",
       "      <td>2</td>\n",
       "      <td>v</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.383359</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183033</th>\n",
       "      <td>11762</td>\n",
       "      <td>13</td>\n",
       "      <td>or</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.385437</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523532</th>\n",
       "      <td>33676</td>\n",
       "      <td>9</td>\n",
       "      <td>v</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.384640</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044552</th>\n",
       "      <td>67181</td>\n",
       "      <td>9</td>\n",
       "      <td>years</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.377610</td>\n",
       "      <td>y e a r s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807594</th>\n",
       "      <td>51960</td>\n",
       "      <td>12</td>\n",
       "      <td>v</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.345380</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760125</th>\n",
       "      <td>48900</td>\n",
       "      <td>0</td>\n",
       "      <td>The</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.393858</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id before   pred_class  pred_c_conf      after\n",
       "171576         11047        10    Don        PLAIN     0.353442        Don\n",
       "270842         17405         6      v      NUMBERS     0.392677          v\n",
       "1036163        66637         7   Page        PLAIN     0.386125       Page\n",
       "845127         54367         0    Don  NOT_CHANGED     0.385927        Don\n",
       "659862         42489         2      v      LETTERS     0.383359          v\n",
       "183033         11762        13     or  NOT_CHANGED     0.385437         or\n",
       "523532         33676         9      v      LETTERS     0.384640          v\n",
       "1044552        67181         9  years      LETTERS     0.377610  y e a r s\n",
       "807594         51960        12      v  NOT_CHANGED     0.345380          v\n",
       "760125         48900         0    The  NOT_CHANGED     0.393858        The"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data['pred_c_conf'] < 0.4].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rows_after_categorizing(pd_rows):\n",
    "    global current_row\n",
    "    iter_len = len(pd_rows)\n",
    "    rows_iter = pd_rows.itertuples()\n",
    "    iteration_idx = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    #for sample_row in test_data[0:100].itertuples():\n",
    "    for sample_row in log_progress(rows_iter, every=10, size=iter_len):\n",
    "        before = sample_row.before\n",
    "        current_row = sample_row\n",
    "\n",
    "        sentence_rows = test_data_sentence_index.loc[sample_row.sentence_id]\n",
    "        sentence_words = list(sentence_rows.before)\n",
    "        token_id_idx = list(sentence_rows['token_id']).index(sample_row.token_id)\n",
    "        sentence_words[token_id_idx] = SAMPLE_WORD_TOKEN\n",
    "        \n",
    "        category = sample_row.pred_class\n",
    "        if category == 'NOT_CHANGED':\n",
    "            result = (before, 0)\n",
    "        elif category == 'NUMBERS':\n",
    "            result = num_model_eval(before, sentence_words)\n",
    "        elif category == 'LETTERS':\n",
    "            result = whole_model_eval(before, sentence_words)\n",
    "        elif category == 'PLAIN':\n",
    "            result = whole_model_eval(before, sentence_words)\n",
    "        elif category == 'VERBATIM':\n",
    "            result = whole_model_eval(before, sentence_words)\n",
    "        elif category == 'ELECTRONIC':\n",
    "            result = whole_model_eval(before, sentence_words)\n",
    "        else:\n",
    "            print(\"PROBLEM WITH:\", sample_row)\n",
    "\n",
    "        iteration_idx += 1\n",
    "        if iteration_idx%10000 == 0:\n",
    "            print(\"{:>7d} {:>2.2%} ({:>8})\".format(iteration_idx, iteration_idx/iter_len, time_since(start)))\n",
    "\n",
    "        test_data.at[sample_row.Index, 'after'] = result[0]\n",
    "        test_data.at[sample_row.Index, 'after_conf'] = result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80160ebb783a4c438cc11aacae8ea591"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000 17.19% (  0m 58s)\n",
      "  20000 34.39% (  1m 58s)\n",
      "  30000 51.58% (  2m 59s)\n",
      "  40000 68.77% (  3m 58s)\n",
      "  50000 85.97% (  4m 59s)\n"
     ]
    }
   ],
   "source": [
    "run_rows_after_categorizing(test_data[test_data['pred_class']=='NUMBERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "      <th>after_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>922635</th>\n",
       "      <td>59326</td>\n",
       "      <td>8</td>\n",
       "      <td>Dec 1965</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>december nineteen sixty five</td>\n",
       "      <td>-0.005705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959176</th>\n",
       "      <td>61695</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>one</td>\n",
       "      <td>-0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817151</th>\n",
       "      <td>52565</td>\n",
       "      <td>6</td>\n",
       "      <td>146,000</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>one hundred forty six thousand</td>\n",
       "      <td>-0.078651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472613</th>\n",
       "      <td>30385</td>\n",
       "      <td>15</td>\n",
       "      <td>2004-04-06</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>the sixth of april two thousand four</td>\n",
       "      <td>-0.008052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821847</th>\n",
       "      <td>52866</td>\n",
       "      <td>9</td>\n",
       "      <td>7th</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>seventh</td>\n",
       "      <td>-0.000122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id      before pred_class  pred_c_conf  \\\n",
       "922635        59326         8    Dec 1965    NUMBERS     1.000000   \n",
       "959176        61695        14           1    NUMBERS     0.999999   \n",
       "817151        52565         6     146,000    NUMBERS     1.000000   \n",
       "472613        30385        15  2004-04-06    NUMBERS     0.999995   \n",
       "821847        52866         9         7th    NUMBERS     0.999815   \n",
       "\n",
       "                                       after  after_conf  \n",
       "922635          december nineteen sixty five   -0.005705  \n",
       "959176                                   one   -0.000053  \n",
       "817151        one hundred forty six thousand   -0.078651  \n",
       "472613  the sixth of april two thousand four   -0.008052  \n",
       "821847                               seventh   -0.000122  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data['pred_class']=='NUMBERS'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98ecedbb6c3425fb275f643ebbbb530"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000 22.01% (  1m 21s)\n",
      "  20000 44.02% (  2m 43s)\n",
      "  30000 66.04% (   4m 2s)\n",
      "  40000 88.05% (  5m 23s)\n"
     ]
    }
   ],
   "source": [
    "run_rows_after_categorizing(test_data[\n",
    "    (test_data['pred_class']!='NUMBERS') &\n",
    "    (test_data['pred_class']!='NOT_CHANGED') &\n",
    "    (test_data['pred_class']!='MANUAL') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "      <th>after_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>744989</th>\n",
       "      <td>47934</td>\n",
       "      <td>4</td>\n",
       "      <td>A.</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>a</td>\n",
       "      <td>-0.004233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51274</th>\n",
       "      <td>3286</td>\n",
       "      <td>0</td>\n",
       "      <td>WZRX</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>w z r r</td>\n",
       "      <td>-3.142048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826725</th>\n",
       "      <td>53185</td>\n",
       "      <td>0</td>\n",
       "      <td>GamePro</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.524487</td>\n",
       "      <td>g a b l e</td>\n",
       "      <td>-5.681352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65591</th>\n",
       "      <td>4210</td>\n",
       "      <td>14</td>\n",
       "      <td>Oh</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.573894</td>\n",
       "      <td>o d</td>\n",
       "      <td>-1.613022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278287</th>\n",
       "      <td>17878</td>\n",
       "      <td>6</td>\n",
       "      <td>WHO</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.993327</td>\n",
       "      <td>w h o</td>\n",
       "      <td>-0.744287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id   before pred_class  pred_c_conf      after  \\\n",
       "744989        47934         4       A.    LETTERS     0.999840          a   \n",
       "51274          3286         0     WZRX    LETTERS     0.999607    w z r r   \n",
       "826725        53185         0  GamePro    LETTERS     0.524487  g a b l e   \n",
       "65591          4210        14       Oh    LETTERS     0.573894        o d   \n",
       "278287        17878         6      WHO    LETTERS     0.993327      w h o   \n",
       "\n",
       "        after_conf  \n",
       "744989   -0.004233  \n",
       "51274    -3.142048  \n",
       "826725   -5.681352  \n",
       "65591    -1.613022  \n",
       "278287   -0.744287  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data['pred_class']=='LETTERS'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "      <th>after_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151421</th>\n",
       "      <td>9745</td>\n",
       "      <td>14</td>\n",
       "      <td>exit</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.782171</td>\n",
       "      <td>e x i i</td>\n",
       "      <td>-2.752991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285043</th>\n",
       "      <td>18314</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>to</td>\n",
       "      <td>-0.028054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186468</th>\n",
       "      <td>11992</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.918404</td>\n",
       "      <td>number</td>\n",
       "      <td>-0.099773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008827</th>\n",
       "      <td>64892</td>\n",
       "      <td>4</td>\n",
       "      <td>feet</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.644730</td>\n",
       "      <td>f e e e</td>\n",
       "      <td>-3.764598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483956</th>\n",
       "      <td>31133</td>\n",
       "      <td>14</td>\n",
       "      <td>-</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>to</td>\n",
       "      <td>-0.028176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id  token_id before pred_class  pred_c_conf    after  \\\n",
       "151421          9745        14   exit      PLAIN     0.782171  e x i i   \n",
       "285043         18314         4      -      PLAIN     0.999987       to   \n",
       "186468         11992         6     no      PLAIN     0.918404   number   \n",
       "1008827        64892         4   feet      PLAIN     0.644730  f e e e   \n",
       "483956         31133        14      -      PLAIN     0.999996       to   \n",
       "\n",
       "         after_conf  \n",
       "151421    -2.752991  \n",
       "285043    -0.028054  \n",
       "186468    -0.099773  \n",
       "1008827   -3.764598  \n",
       "483956    -0.028176  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data['pred_class']=='PLAIN'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id          24102\n",
       "token_id                 6\n",
       "before                  is\n",
       "pred_class     NOT_CHANGED\n",
       "pred_c_conf       0.999976\n",
       "after                   is\n",
       "Name: 375381, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.loc[375381]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_data = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tmp = list(result_data.columns)\n",
    "tmp[tmp.index('before')] = 'after'\n",
    "result_data.columns = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_data['id'] = result_data.apply(lambda row: \"{}_{}\".format(row['sentence_id'], row['token_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_c_conf</th>\n",
       "      <th>after</th>\n",
       "      <th>after_conf</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>746860</th>\n",
       "      <td>48054</td>\n",
       "      <td>21</td>\n",
       "      <td>Galvan</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.973958</td>\n",
       "      <td>Galvan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48054_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863245</th>\n",
       "      <td>55515</td>\n",
       "      <td>8</td>\n",
       "      <td>Need</td>\n",
       "      <td>NOT_CHANGED</td>\n",
       "      <td>0.988249</td>\n",
       "      <td>Need</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55515_8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id  before   pred_class  pred_c_conf   after  \\\n",
       "746860        48054        21  Galvan  NOT_CHANGED     0.973958  Galvan   \n",
       "863245        55515         8    Need  NOT_CHANGED     0.988249    Need   \n",
       "\n",
       "        after_conf        id  \n",
       "746860         NaN  48054_21  \n",
       "863245         NaN   55515_8  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result_data.loc[10, 'after'] = '\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_file_path = 'data/en_submission_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_data.to_csv(result_file_path, index=False, columns=['id', 'after'], quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_pytorch_2]",
   "language": "python",
   "name": "conda-env-py3_pytorch_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
